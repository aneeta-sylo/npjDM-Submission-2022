{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create HiRID Temporal External Validation Dataset (Experiment 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all necessary modules\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to HiRID database\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2 import Error\n",
    "\n",
    "# Connect to HiRID\n",
    "conn = psycopg2.connect(user=\"mimicuser\",\n",
    "                                  password=\"knowlabMIMIC\",\n",
    "                                  host=\"172.17.0.1\",\n",
    "                                  port=\"5433\",\n",
    "                                  database=\"HiRID\")\n",
    "\n",
    "# Cursor \n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Filtering for all instances with drug administration (Adrenaline/Noradrenaline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HiRID documentation contains the variableids for all drug variables:<br>\n",
    "https://hirid.intensivecare.ai/Structure-of-the-published-data-de050e11529140178dea1a1b925d558f\n",
    "\n",
    "- **Adrenaline** (continuously administered): 1000649, 1000650\n",
    "- **Noradrenaline** (continuously administered): 1000656, 1000657"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to pharma_records table\n",
    "##filtering for only continuous Adrenaline/ Noradrenaline fields\n",
    "##pharmaids for continuous Adrenaline/Noradrenaline fields found in HiRID documentation - https://hirid.intensivecare.ai/Structure-of-the-published-data-de050e11529140178dea1a1b925d558f\n",
    "\n",
    "#import all records from HiRID Pharma table containing continuous Adrenaline/Noradrenaline\n",
    "drugs_df = pd.read_sql_query(\"SELECT * FROM hirid.pharma_records \\\n",
    "                         WHERE pharmaid IN (1000649, 1000650, 1000656, 1000657) \\\n",
    "                         ORDER BY patientid, pharmaid, givenat\", conn)\n",
    "\n",
    "\n",
    "#categorise drugs_df 'givenat' column to closest 15min timepoint (needed for later analysis)\n",
    "drugs_df['timepoint'] = drugs_df['givenat'].dt.round('15min') \n",
    "\n",
    "print(drugs_df.shape)\n",
    "drugs_df.head()\n",
    "\n",
    "##1,458,539 records\n",
    "##NOTE: includes rows where given_dose=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter for records containing Adrenaline/Noradrenaline readings with non-zero values \n",
    "\n",
    "drugs_nonzero = drugs_df[drugs_df.givendose != 0] #drop all rows where givendose=0.0\n",
    "\n",
    "print(drugs_nonzero.shape)\n",
    "drugs_nonzero\n",
    "\n",
    "##1,313,057 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if necessary - convert givenat to datetime format\n",
    "drugs_df['givenat'] =  pd.to_datetime(drugs_df['givenat'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "drugs_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show count of each drug field\n",
    "\n",
    "drugs_df['pharmaid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show all route values\n",
    "\n",
    "drugs_df['route'].unique()\n",
    "\n",
    "##only continuous administration - as required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of unique ICU admissions\n",
    "\n",
    "pat_drugs = list(drugs_df['patientid'].unique())\n",
    "len(pat_drugs)\n",
    "\n",
    "#10,665 unique ICU admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check number of unique instances (patientid + givenat)\n",
    "\n",
    "agg = drugs_df.groupby(['patientid','givenat'])['pharmaid'].nunique()\n",
    "agg = pd.DataFrame(data=agg, index=None)\n",
    "agg = agg.reset_index()\n",
    "print(agg.shape)\n",
    "agg.head(10)\n",
    "\n",
    "##1,430,607 unique instances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter for 15min time intervals (per patientid) containing (non-zero) readings for Adrenaline/Noradrenaline\n",
    "\n",
    "agg_drugs = drugs_nonzero.groupby([pd.Grouper(key='givenat', freq='15Min'), 'patientid']).pharmaid.unique()\n",
    "agg_drugs = pd.DataFrame(data=agg_drugs)\n",
    "agg_drugs = agg_drugs.reset_index()\n",
    "agg_drugs = agg_drugs.sort_values(by=['patientid','givenat'], ascending=[True,True])\n",
    "\n",
    "print(agg_drugs.shape)\n",
    "agg_drugs.head(15)\n",
    "\n",
    "#818,425 unique instances (with non-zero values for Adrenaline/Noradrenaline) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check datatypes - givenat should be in datetime format\n",
    "\n",
    "agg_drugs.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check unique ICU admissions\n",
    "\n",
    "agg_drugs['patientid'].nunique()\n",
    "\n",
    "##10,634 unique ICU admissions that have non-zero values for Adrenaline/Noradrenaline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Calculate drug rate for instances with Adrenaline/Noradrenaline drug readings (mg/hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Order drugs_df table by patientid, pharmaid, givenat\n",
    "\n",
    "drugs_df.sort_values(by=['patientid','pharmaid','givenat'])\n",
    "\n",
    "print(drugs_df.shape)\n",
    "drugs_df.head()\n",
    "\n",
    "##1,458,539 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm givenat is in datetime format\n",
    "\n",
    "drugs_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract relevant columns from drugs_df\n",
    "\n",
    "drugs_mod = drugs_df.copy(deep=True)\n",
    "drugs_mod = drugs_mod[['patientid', 'pharmaid', 'givenat', 'timepoint', 'givendose','cumulativedose','doseunit']]\n",
    "\n",
    "print(drugs_mod.shape)\n",
    "drugs_mod.head()\n",
    "\n",
    "##1,458,539 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check drug units\n",
    "\n",
    "drugs_mod['doseunit'].unique()\n",
    "\n",
    "##unit = micrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get min_time for each (patientid, pharmaid, timepoint)\n",
    "##min_time = minimum (earliest) drug administration time within given timepoint\n",
    "\n",
    "temp = drugs_mod.groupby(['patientid','pharmaid', 'timepoint'])\n",
    "min_time = temp.agg(min_time=('givenat', np.min))\n",
    "min_time = min_time.reset_index()\n",
    "min_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get cumulative dose for min_time per patientid in drugs_mod  \n",
    "\n",
    "merge = pd.merge(min_time, drugs_mod, left_on=['patientid','pharmaid','timepoint','min_time'], right_on=['patientid','pharmaid','timepoint','givenat'])\n",
    "min_cumval = merge[['patientid', 'pharmaid','timepoint','min_time','cumulativedose']]\n",
    "min_cumval.rename({'cumulativedose': 'cumulativedose(min)'}, axis=1, inplace=True)\n",
    "min_cumval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get max_time for each (patientid, pharmaid, timepoint)\n",
    "##max_time = maximum (latest) drug administration time within given timepoint\n",
    "\n",
    "temp = drugs_mod.groupby(['patientid','pharmaid','timepoint'])\n",
    "max_time = temp.agg(max_time=('givenat', np.max))\n",
    "max_time = max_time.reset_index()\n",
    "max_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get cumulative dose for max_time per patientid in drugs_mod  \n",
    "\n",
    "merge = pd.merge(max_time, drugs_mod, left_on=['patientid','pharmaid','timepoint','max_time'], right_on=['patientid','pharmaid','timepoint','givenat'])\n",
    "max_cumval = merge[['patientid', 'pharmaid','timepoint','max_time','cumulativedose']]\n",
    "max_cumval.rename({'cumulativedose': 'cumulativedose(max)'}, axis=1, inplace=True)\n",
    "max_cumval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate drugs rate\n",
    "\n",
    "drugs_rate = pd.merge(min_cumval, max_cumval, left_on=['patientid','pharmaid','timepoint'], right_on=['patientid','pharmaid','timepoint'])\n",
    "\n",
    "drugs_rate['time_diff'] = drugs_rate['max_time']-drugs_rate['min_time']\n",
    "drugs_rate['time_diff(mins)'] = (drugs_rate['time_diff'] / np.timedelta64(1, 'h'))*60\n",
    "drugs_rate['cumulativedose_diff'] = drugs_rate['cumulativedose(max)']-drugs_rate['cumulativedose(min)']\n",
    "\n",
    "drugs_rate['rate(microg/min)']= drugs_rate['cumulativedose_diff']/drugs_rate['time_diff(mins)']\n",
    "drugs_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Covert rate units from microgrames/min to milligrams/hr\n",
    "\n",
    "conversion = 0.060000071999942\n",
    "\n",
    "drugs_rate['rate(millig/hr)']=drugs_rate['rate(microg/min)']*conversion\n",
    "drugs_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter for acceptable rate values \n",
    "\n",
    "drugs_rate = drugs_rate[drugs_rate['rate(millig/hr)']<10]\n",
    "drugs_rate = drugs_rate[drugs_rate['rate(millig/hr)']>0]\n",
    "\n",
    "print(drugs_rate.shape)\n",
    "drugs_rate\n",
    "\n",
    "##272,932 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check min and max drug rate\n",
    "\n",
    "max_rate = drugs_rate['rate(millig/hr)'].max()\n",
    "min_rate = drugs_rate['rate(millig/hr)'].min()\n",
    "print(\"Lowest drug rate:\", min_rate)\n",
    "print(\"Highest drug rate:\", max_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename pharmaid to drug name \n",
    "\n",
    "drugs_rate['pharmaid'] = drugs_rate['pharmaid'].replace({ 1000649 : \"Adrenaline\", 1000650 : \"Adrenaline\", \n",
    "                                                         1000656: \"Noradrenaline\", 1000657 : \"Noradrenaline\"})\n",
    "drugs_rate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract relevant columns from drugs_rate\n",
    "\n",
    "drugs_rate = drugs_rate[['patientid','pharmaid','timepoint','rate(millig/hr)']]\n",
    "\n",
    "print(drugs_rate.shape)\n",
    "drugs_rate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reformat drugs_rate DF to structure similar to training data \n",
    "\n",
    "drugs_rate = drugs_rate.groupby(['patientid', 'timepoint', 'pharmaid'])['rate(millig/hr)'].sum().unstack('pharmaid')\n",
    "\n",
    "drugs_rate = drugs_rate.reset_index()\n",
    "drugs_rate = drugs_rate.rename_axis(None, axis=1)\n",
    "\n",
    "drugs_rate['Adrenaline'] = round(drugs_rate['Adrenaline'],2)\n",
    "drugs_rate['Noradrenaline'] = round(drugs_rate['Noradrenaline'],2)\n",
    "\n",
    "drugs_rate.to_csv('drugs_rate.csv')\n",
    "\n",
    "print(drugs_rate.shape)\n",
    "drugs_rate\n",
    "\n",
    "#268,461 records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Find instances with physiological parameter readings (FiO2/SpO2/HR/MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HiRID documentation contains the variableids for all physiological parameters:<br>\n",
    "https://hirid.intensivecare.ai/Structure-of-the-published-data-de050e11529140178dea1a1b925d558f\n",
    "\n",
    "- **FiO2**: 2010\n",
    "- **SpO2**: 4000, 8280\n",
    "- **Heart Rate**: 200\n",
    "- **Mean Arterial Pressure**: 110, 610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to observations_table_1\n",
    "\n",
    "obs_tab1 = pd.read_sql_query(\" SELECT * FROM hirid.observations_table_1 \\\n",
    "                                    WHERE variableid IN (200, 110, 610, 4000, 8280, 2010) \\\n",
    "                                    ORDER BY patientid, datetime \", conn)\n",
    "\n",
    "obs_tab1['row_idx']=obs_tab1.index #create new index column\n",
    "\n",
    "obs_tab1.to_csv('obs_tab1.csv')\n",
    "\n",
    "print(obs_tab1.shape)\n",
    "obs_tab1.head()\n",
    "\n",
    "#number of records = 19,492,758"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to observations_table_2\n",
    "\n",
    "obs_tab2 = pd.read_sql_query(\" SELECT * FROM hirid.observations_table_2 \\\n",
    "                                    WHERE variableid IN (200, 110, 610, 4000, 8280, 2010) \\\n",
    "                                    ORDER BY patientid, datetime \", conn)\n",
    "\n",
    "obs_tab2['row_idx']=obs_tab2.index #create new index column\n",
    "\n",
    "obs_tab2.to_csv('obs_tab2.csv')\n",
    "\n",
    "print(obs_tab2.shape)\n",
    "obs_tab2.head()\n",
    "\n",
    "#number of records = 19,373,029"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to observations_table_3\n",
    "\n",
    "obs_tab3 = pd.read_sql_query(\" SELECT * FROM hirid.observations_table_3 \\\n",
    "                                    WHERE variableid IN (200, 110, 610, 4000, 8280, 2010) \\\n",
    "                                    ORDER BY patientid, datetime \", conn)\n",
    "\n",
    "obs_tab3['row_idx']=obs_tab3.index #create new index column\n",
    "\n",
    "obs_tab3.to_csv('obs_tab3.csv')\n",
    "\n",
    "print(obs_tab3.shape)\n",
    "obs_tab3.head()\n",
    "\n",
    "#number of records = 19,922,456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to observations_table_4\n",
    "\n",
    "obs_tab4 = pd.read_sql_query(\" SELECT * FROM hirid.observations_table_4 \\\n",
    "                                    WHERE variableid IN (200, 110, 610, 4000, 8280, 2010) \\\n",
    "                                    ORDER BY patientid, datetime \", conn)\n",
    "\n",
    "obs_tab4['row_idx']=obs_tab4.index #create new index column\n",
    "\n",
    "obs_tab4.to_csv('obs_tab4.csv')\n",
    "\n",
    "print(obs_tab4.shape)\n",
    "obs_tab4.head()\n",
    "\n",
    "#number of records = 19,576,004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to observations_table_5\n",
    "\n",
    "obs_tab5 = pd.read_sql_query(\" SELECT * FROM hirid.observations_table_5 \\\n",
    "                                    WHERE variableid IN (200, 110, 610, 4000, 8280, 2010) \\\n",
    "                                    ORDER BY patientid, datetime \", conn)\n",
    "\n",
    "obs_tab5['row_idx']=obs_tab5.index #create new index column\n",
    "\n",
    "obs_tab5.to_csv('obs_tab5.csv')\n",
    "\n",
    "print(obs_tab5.shape)\n",
    "obs_tab5.head()\n",
    "\n",
    "#number of records = 19,475,848"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to observations_table_6\n",
    "\n",
    "obs_tab6 = pd.read_sql_query(\" SELECT * FROM hirid.observations_table_6 \\\n",
    "                                    WHERE variableid IN (200, 110, 610, 4000, 8280, 2010) \\\n",
    "                                    ORDER BY patientid, datetime \", conn)\n",
    "\n",
    "obs_tab6['row_idx']=obs_tab6.index #create new index column\n",
    "\n",
    "obs_tab6.to_csv('obs_tab6.csv')\n",
    "\n",
    "print(obs_tab6.shape)\n",
    "obs_tab6.head()\n",
    "\n",
    "#number of records = 19,612,923"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to observations_table_7\n",
    "\n",
    "obs_tab7 = pd.read_sql_query(\" SELECT * FROM hirid.observations_table_7 \\\n",
    "                                    WHERE variableid IN (200, 110, 610, 4000, 8280, 2010) \\\n",
    "                                    ORDER BY patientid, datetime \", conn)\n",
    "\n",
    "obs_tab7['row_idx']=obs_tab7.index #create new index column\n",
    "\n",
    "obs_tab7.to_csv('obs_tab7.csv')\n",
    "\n",
    "print(obs_tab7.shape)\n",
    "obs_tab7.head()\n",
    "\n",
    "## number of records = 19,818,593"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to observations_table_8\n",
    "\n",
    "obs_tab8 = pd.read_sql_query(\" SELECT * FROM hirid.observations_table_8 \\\n",
    "                                    WHERE variableid IN (200, 110, 610, 4000, 8280, 2010) \\\n",
    "                                    ORDER BY patientid, datetime \", conn)\n",
    "\n",
    "obs_tab8['row_idx']=obs_tab8.index #create new index column\n",
    "\n",
    "obs_tab8.to_csv('obs_tab8.csv')\n",
    "\n",
    "print(obs_tab8.shape)\n",
    "obs_tab8.head()\n",
    "\n",
    "#number of records = 19,449,724"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to observations_table_9\n",
    "\n",
    "obs_tab9 = pd.read_sql_query(\" SELECT * FROM hirid.observations_table_9 \\\n",
    "                                    WHERE variableid IN (200, 110, 610, 4000, 8280, 2010) \\\n",
    "                                    ORDER BY patientid, datetime \", conn)\n",
    "\n",
    "obs_tab9['row_idx']=obs_tab9.index #create new index column\n",
    "\n",
    "obs_tab9.to_csv('obs_tab9.csv')\n",
    "\n",
    "print(obs_tab9.shape)\n",
    "obs_tab9.head()\n",
    "\n",
    "#number of records = 19,922,926"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to observations_table_10\n",
    "\n",
    "obs_tab10 = pd.read_sql_query(\" SELECT * FROM hirid.observations_table_10 \\\n",
    "                                    WHERE variableid IN (200, 110, 610, 4000, 8280, 2010) \\\n",
    "                                    ORDER BY patientid, datetime \", conn)\n",
    "\n",
    "obs_tab10['row_idx']=obs_tab10.index #create new index column\n",
    "\n",
    "obs_tab10.to_csv('obs_tab10.csv')\n",
    "\n",
    "print(obs_tab10.shape)\n",
    "obs_tab10.head()\n",
    "\n",
    "#number of records = 18,538,580"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of records observation_table_1:', len(obs_tab1))\n",
    "print('Number of records observation_table_2:', len(obs_tab2))\n",
    "print('Number of records observation_table_3:', len(obs_tab3))\n",
    "print('Number of records observation_table_4:', len(obs_tab4))\n",
    "print('Number of records observation_table_5:', len(obs_tab5))\n",
    "print('Number of records observation_table_6:', len(obs_tab6))\n",
    "print('Number of records observation_table_7:', len(obs_tab7))\n",
    "print('Number of records observation_table_8:', len(obs_tab8))\n",
    "print('Number of records observation_table_9:', len(obs_tab9))\n",
    "print('Number of records observation_table_10:', len(obs_tab10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import obs_tab1\n",
    "\n",
    "obs_tab1= pd.read_csv(\"obs_tab1.csv\")\n",
    "obs_tab1= obs_tab1.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "obs_tab1['datetime'] =  pd.to_datetime(obs_tab1['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "obs_tab1['timepoint'] = obs_tab1['datetime'].dt.round('15min') #timepoint col needed in later manipulationn\n",
    "print(obs_tab1.shape)\n",
    "obs_tab1\n",
    "\n",
    "##19,492,758 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import obs_tab2\n",
    "\n",
    "obs_tab2= pd.read_csv(\"obs_tab2.csv\")\n",
    "obs_tab2= obs_tab2.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "obs_tab2['datetime'] =  pd.to_datetime(obs_tab2['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "obs_tab2['timepoint'] = obs_tab2['datetime'].dt.round('15min') #timepoint col needed in later manipulationn\n",
    "print(obs_tab2.shape)\n",
    "obs_tab2\n",
    "\n",
    "##19,373,029 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import obs_tab3\n",
    "\n",
    "obs_tab3= pd.read_csv(\"obs_tab3.csv\")\n",
    "obs_tab3= obs_tab3.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "obs_tab3['datetime'] =  pd.to_datetime(obs_tab3['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "obs_tab3['timepoint'] = obs_tab3['datetime'].dt.round('15min') #timepoint col needed in later manipulationn\n",
    "print(obs_tab3.shape)\n",
    "obs_tab3\n",
    "\n",
    "##19,922,456 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import obs_tab4\n",
    "\n",
    "obs_tab4= pd.read_csv(\"obs_tab4.csv\")\n",
    "obs_tab4= obs_tab4.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "obs_tab4['datetime'] =  pd.to_datetime(obs_tab4['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "obs_tab4['timepoint'] = obs_tab4['datetime'].dt.round('15min') #timepoint col needed in later manipulationn\n",
    "print(obs_tab4.shape)\n",
    "obs_tab4\n",
    "\n",
    "##19,576,004 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import obs_tab5\n",
    "\n",
    "obs_tab5= pd.read_csv(\"obs_tab5.csv\")\n",
    "obs_tab5= obs_tab5.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "obs_tab5['datetime'] =  pd.to_datetime(obs_tab5['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "obs_tab5['timepoint'] = obs_tab5['datetime'].dt.round('15min') #timepoint col needed in later manipulationn\n",
    "print(obs_tab5.shape)\n",
    "obs_tab5\n",
    "\n",
    "##19,475,848 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import obs_tab6\n",
    "\n",
    "obs_tab6= pd.read_csv(\"obs_tab6.csv\")\n",
    "obs_tab6= obs_tab6.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "obs_tab6['datetime'] =  pd.to_datetime(obs_tab6['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "obs_tab6['timepoint'] = obs_tab6['datetime'].dt.round('15min') #timepoint col needed in later manipulationn\n",
    "print(obs_tab6.shape)\n",
    "obs_tab6\n",
    "\n",
    "##19,612,923 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import obs_tab7\n",
    "\n",
    "obs_tab7= pd.read_csv(\"obs_tab7.csv\")\n",
    "obs_tab7= obs_tab7.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "obs_tab7['datetime'] =  pd.to_datetime(obs_tab7['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "obs_tab7['timepoint'] = obs_tab7['datetime'].dt.round('15min') #timepoint col needed in later manipulationn\n",
    "print(obs_tab7.shape)\n",
    "obs_tab7\n",
    "\n",
    "##19,818,593 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import obs_tab8\n",
    "\n",
    "obs_tab8= pd.read_csv(\"obs_tab8.csv\")\n",
    "obs_tab8= obs_tab8.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "obs_tab8['datetime'] =  pd.to_datetime(obs_tab8['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "obs_tab8['timepoint'] = obs_tab8['datetime'].dt.round('15min') #timepoint col needed in later manipulationn\n",
    "print(obs_tab8.shape)\n",
    "obs_tab8\n",
    "\n",
    "##19,449,724 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import obs_tab9\n",
    "\n",
    "obs_tab9= pd.read_csv(\"obs_tab9.csv\")\n",
    "obs_tab9= obs_tab9.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "obs_tab9['datetime'] =  pd.to_datetime(obs_tab9['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "obs_tab9['timepoint'] = obs_tab9['datetime'].dt.round('15min') #timepoint col needed in later manipulationn\n",
    "print(obs_tab9.shape)\n",
    "obs_tab9\n",
    "\n",
    "##19,922,926 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import obs_tab10\n",
    "\n",
    "obs_tab10= pd.read_csv(\"obs_tab10.csv\")\n",
    "obs_tab10= obs_tab10.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "obs_tab10['datetime'] =  pd.to_datetime(obs_tab10['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "obs_tab10['timepoint'] = obs_tab10['datetime'].dt.round('15min') #timepoint col needed in later manipulationn\n",
    "print(obs_tab10.shape)\n",
    "obs_tab10\n",
    "\n",
    "##18,538,580 records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Find instances with 15min intervals containing readings for all 4 physiological parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using obs_tab1\n",
    "#List unique physiological variableids within 15min time intervals, per patientid \n",
    "##datetime needs to be in correct format for query to run\n",
    "\n",
    "agg_var_1 = obs_tab1.groupby([pd.Grouper(key='datetime', freq='15Min'), 'patientid']).variableid.unique()\n",
    "agg_var_1 = pd.DataFrame(data=agg_var_1)\n",
    "agg_var_1 = agg_var_1.reset_index()\n",
    "agg_var_1 = agg_var_1.sort_values(by=['patientid','datetime'], ascending=[True,True])\n",
    "\n",
    "agg_var_1.to_csv('agg_var_1.csv')\n",
    "\n",
    "print(agg_var_1.shape)\n",
    "agg_var_1.head(5)\n",
    "\n",
    "##715,533 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using obs_tab2\n",
    "#List unique physiological variableids within 15min time intervals, per patientid \n",
    "##datetime needs to be in correct format for query to run\n",
    "\n",
    "agg_var_2 = obs_tab2.groupby([pd.Grouper(key='datetime', freq='15Min'), 'patientid']).variableid.unique()\n",
    "agg_var_2 = pd.DataFrame(data=agg_var_2)\n",
    "agg_var_2 = agg_var_2.reset_index()\n",
    "agg_var_2 = agg_var_2.sort_values(by=['patientid','datetime'], ascending=[True,True])\n",
    "\n",
    "agg_var_2.to_csv('agg_var_2.csv')\n",
    "\n",
    "print(agg_var_2.shape)\n",
    "agg_var_2.head(5)\n",
    "\n",
    "##720,062 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using obs_tab3\n",
    "#List unique physiological variableids within 15min time intervals, per patientid \n",
    "#datetime needs to be in correct format for query to run\n",
    "\n",
    "agg_var_3 = obs_tab3.groupby([pd.Grouper(key='datetime', freq='15Min'), 'patientid']).variableid.unique()\n",
    "agg_var_3 = pd.DataFrame(data=agg_var_3)\n",
    "agg_var_3 = agg_var_3.reset_index()\n",
    "agg_var_3 = agg_var_3.sort_values(by=['patientid','datetime'], ascending=[True,True])\n",
    "\n",
    "agg_var_3.to_csv('agg_var_3.csv')\n",
    "\n",
    "print(agg_var_3.shape)\n",
    "agg_var_3.head(5)\n",
    "\n",
    "##737,329 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using obs_tab4\n",
    "#List unique physiological variableids within 15min time intervals, per patientid \n",
    "#datetime needs to be in correct format for query to run\n",
    "\n",
    "agg_var_4 = obs_tab4.groupby([pd.Grouper(key='datetime', freq='15Min'), 'patientid']).variableid.unique()\n",
    "agg_var_4 = pd.DataFrame(data=agg_var_4)\n",
    "agg_var_4 = agg_var_4.reset_index()\n",
    "agg_var_4 = agg_var_4.sort_values(by=['patientid','datetime'], ascending=[True,True])\n",
    "\n",
    "agg_var_4.to_csv('agg_var_4.csv')\n",
    "\n",
    "print(agg_var_4.shape)\n",
    "agg_var_4.head(5)\n",
    "\n",
    "##722,235 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using obs_tab5\n",
    "#List unique physiological variableids within 15min time intervals, per patientid \n",
    "#datetime needs to be in correct format for query to run\n",
    "\n",
    "agg_var_5 = obs_tab5.groupby([pd.Grouper(key='datetime', freq='15Min'), 'patientid']).variableid.unique()\n",
    "agg_var_5 = pd.DataFrame(data=agg_var_5)\n",
    "agg_var_5 = agg_var_5.reset_index()\n",
    "agg_var_5 = agg_var_5.sort_values(by=['patientid','datetime'], ascending=[True,True])\n",
    "\n",
    "agg_var_5.to_csv('agg_var_5.csv')\n",
    "\n",
    "print(agg_var_5.shape)\n",
    "agg_var_5.head(5)\n",
    "\n",
    "##712,740 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using obs_tab6\n",
    "#List unique physiological variableids within 16min time intervals, per patientid \n",
    "#datetime needs to be in correct format for query to run\n",
    "\n",
    "agg_var_6 = obs_tab6.groupby([pd.Grouper(key='datetime', freq='15Min'), 'patientid']).variableid.unique()\n",
    "agg_var_6 = pd.DataFrame(data=agg_var_6)\n",
    "agg_var_6 = agg_var_6.reset_index()\n",
    "agg_var_6 = agg_var_6.sort_values(by=['patientid','datetime'], ascending=[True,True])\n",
    "\n",
    "agg_var_6.to_csv('agg_var_6.csv')\n",
    "\n",
    "print(agg_var_6.shape)\n",
    "agg_var_6.head(6)\n",
    "\n",
    "##729,026 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using obs_tab7\n",
    "#List unique physiological variableids within 17min time intervals, per patientid \n",
    "#datetime needs to be in correct format for query to run\n",
    "\n",
    "agg_var_7 = obs_tab7.groupby([pd.Grouper(key='datetime', freq='15Min'), 'patientid']).variableid.unique()\n",
    "agg_var_7 = pd.DataFrame(data=agg_var_7)\n",
    "agg_var_7 = agg_var_7.reset_index()\n",
    "agg_var_7 = agg_var_7.sort_values(by=['patientid','datetime'], ascending=[True,True])\n",
    "\n",
    "agg_var_7.to_csv('agg_var_7.csv')\n",
    "\n",
    "print(agg_var_7.shape)\n",
    "agg_var_7.head(7)\n",
    "\n",
    "##734,652 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using obs_tab8\n",
    "#List unique physiological variableids within 18min time intervals, per patientid \n",
    "#datetime needs to be in correct format for query to run\n",
    "\n",
    "agg_var_8 = obs_tab8.groupby([pd.Grouper(key='datetime', freq='15Min'), 'patientid']).variableid.unique()\n",
    "agg_var_8 = pd.DataFrame(data=agg_var_8)\n",
    "agg_var_8 = agg_var_8.reset_index()\n",
    "agg_var_8 = agg_var_8.sort_values(by=['patientid','datetime'], ascending=[True,True])\n",
    "\n",
    "agg_var_8.to_csv('agg_var_8.csv')\n",
    "\n",
    "print(agg_var_8.shape)\n",
    "agg_var_8.head(8)\n",
    "\n",
    "##712,585 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using obs_tab9\n",
    "#List unique physiological variableids within 19min time intervals, per patientid \n",
    "#datetime needs to be in correct format for query to run\n",
    "\n",
    "agg_var_9 = obs_tab9.groupby([pd.Grouper(key='datetime', freq='15Min'), 'patientid']).variableid.unique()\n",
    "agg_var_9 = pd.DataFrame(data=agg_var_9)\n",
    "agg_var_9 = agg_var_9.reset_index()\n",
    "agg_var_9 = agg_var_9.sort_values(by=['patientid','datetime'], ascending=[True,True])\n",
    "\n",
    "agg_var_9.to_csv('agg_var_9.csv')\n",
    "\n",
    "print(agg_var_9.shape)\n",
    "agg_var_9.head(9)\n",
    "\n",
    "##734,800 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using obs_tab10\n",
    "#List unique physiological variableids within 110min time intervals, per patientid \n",
    "#datetime needs to be in correct format for query to run\n",
    "\n",
    "agg_var_10 = obs_tab10.groupby([pd.Grouper(key='datetime', freq='15Min'), 'patientid']).variableid.unique()\n",
    "agg_var_10 = pd.DataFrame(data=agg_var_10)\n",
    "agg_var_10 = agg_var_10.reset_index()\n",
    "agg_var_10 = agg_var_10.sort_values(by=['patientid','datetime'], ascending=[True,True])\n",
    "\n",
    "agg_var_10.to_csv('agg_var_10.csv')\n",
    "\n",
    "print(agg_var_10.shape)\n",
    "agg_var_10.head(10)\n",
    "\n",
    "##685,927 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "agg_var_1 = pd.read_csv(\"agg_var_1.csv\")\n",
    "agg_var_1 = agg_var_1.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "agg_var_2 = pd.read_csv(\"agg_var_2.csv\")\n",
    "agg_var_2 = agg_var_2.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "agg_var_3 = pd.read_csv(\"agg_var_3.csv\")\n",
    "agg_var_3 = agg_var_3.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "agg_var_4 = pd.read_csv(\"agg_var_4.csv\")\n",
    "agg_var_4 = agg_var_4.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "agg_var_5 = pd.read_csv(\"agg_var_5.csv\")\n",
    "agg_var_5 = agg_var_5.drop(['Unnamed: 0'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "agg_var_1 = pd.read_csv(\"agg_var_1.csv\")\n",
    "agg_var_1 = agg_var_1.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "agg_var_2 = pd.read_csv(\"agg_var_2.csv\")\n",
    "agg_var_2 = agg_var_2.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "agg_var_3 = pd.read_csv(\"agg_var_3.csv\")\n",
    "agg_var_3 = agg_var_3.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "agg_var_4 = pd.read_csv(\"agg_var_4.csv\")\n",
    "agg_var_4 = agg_var_4.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "agg_var_5 = pd.read_csv(\"agg_var_5.csv\")\n",
    "agg_var_5 = agg_var_5.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "agg_var_6 = pd.read_csv(\"agg_var_6.csv\")\n",
    "agg_var_6 = agg_var_6.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "agg_var_7 = pd.read_csv(\"agg_var_7.csv\")\n",
    "agg_var_7 = agg_var_7.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "agg_var_8 = pd.read_csv(\"agg_var_8.csv\")\n",
    "agg_var_8 = agg_var_8.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "agg_var_9 = pd.read_csv(\"agg_var_9.csv\")\n",
    "agg_var_9 = agg_var_9.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "agg_var_10 = pd.read_csv(\"agg_var_10.csv\")\n",
    "agg_var_10 = agg_var_10.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate \n",
    "#List unique physiological variableids within ALL 15min time intervals, per patientid \n",
    "\n",
    "frames = [agg_var_1, agg_var_2, agg_var_3, agg_var_4, agg_var_5, \n",
    "          agg_var_6, agg_var_7, agg_var_8, agg_var_9, agg_var_10]\n",
    "\n",
    "agg_var_full = pd.concat(frames)\n",
    "\n",
    "agg_var_full.to_csv('agg_var_full.csv')\n",
    "\n",
    "print(agg_var_full.shape)\n",
    "agg_var_full\n",
    "\n",
    "##7,204,889 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if necessary\n",
    "\n",
    "agg_var_full = pd.read_csv(\"agg_var_full.csv\")\n",
    "agg_var_full = agg_var_full.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "print(agg_var_full.shape)\n",
    "\n",
    "agg_var_full['datetime'] =  pd.to_datetime(agg_var_full['datetime'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "agg_var_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter agg_var_full for all instances that contain a reading for FiO2\n",
    "\n",
    "#FiO2\n",
    "df_mod = agg_var_full[agg_var_full.variableid.astype(str).str.contains('2010')]\n",
    "print(df_mod.shape)\n",
    "df_mod\n",
    "\n",
    "##3,040,811 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter df_mod for all instances that contain a reading for SpO2\n",
    "\n",
    "df_mod = df_mod[df_mod.variableid.astype(str).str.contains('4000|8280')]\n",
    "print(df_mod.shape)\n",
    "df_mod\n",
    "\n",
    "##3,021,057 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter df_mod for all instances that contain a reading for HR\n",
    "\n",
    "df_mod = df_mod[df_mod.variableid.astype(str).str.contains('200')]\n",
    "print(df_mod.shape)\n",
    "df_mod\n",
    "\n",
    "##3,020,405 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter df_mod for all instances that contain a reading for MAP\n",
    "\n",
    "df_mod = df_mod[df_mod.variableid.astype(str).str.contains('110|610')]\n",
    "\n",
    "df_mod.to_csv('df_mod.csv')\n",
    "\n",
    "print(df_mod.shape)\n",
    "df_mod\n",
    "\n",
    "##3,004,013 records\n",
    "##There are 3,004,013 unique instances which contain readings for all 4 physiological parameters\n",
    "##This is our pool of relevant (patientid + timepoint) combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if necessary - import\n",
    "df_mod = pd.read_csv(\"df_mod.csv\")\n",
    "df_mod = df_mod.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "print(df_mod.shape)\n",
    "\n",
    "df_mod['datetime'] =  pd.to_datetime(df_mod['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_mod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check unique ICU admissions\n",
    "\n",
    "df_mod['patientid'].nunique()\n",
    "\n",
    "##20,289 unique ICU admissions within above pool of relevant instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mod.groupby(['datetime', 'patientid']).ngroups\n",
    "\n",
    "#confirms there are 3,004,013 unique combinations of (15min time interval + patientid) that have readings for 4 phys params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Find instances with readings for Adrenaline/Noradrenaline AND all 4 physiological parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge df_mod and agg_drugs\n",
    "##Pool of relevant instances (unique timepoint + patientid)\n",
    "\n",
    "agg_pool = pd.merge(df_mod, agg_drugs,  how='left', left_on=['datetime','patientid'], right_on = ['givenat','patientid'])\n",
    "agg_pool = agg_pool.drop(['givenat'],axis=1)\n",
    "agg_pool.rename({'datetime': 'timepoint'}, axis=1, inplace=True)\n",
    "\n",
    "agg_pool['timepoint'] =  pd.to_datetime(agg_pool['timepoint'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "agg_pool.to_csv('agg_pool.csv')\n",
    "\n",
    "print(agg_pool.shape)\n",
    "agg_pool\n",
    "\n",
    "#3,004,013 instances that have 4+ readings for phyiological parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if necessary - convert timepoint to datetime format\n",
    "agg_pool['timepoint'] =  pd.to_datetime(agg_pool['timepoint'], format='%Y-%m-%d %H:%M:%S')\n",
    "agg_pool.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check how many instances (unique patientid + timepoint) are common between df_mod and agg_drugs \n",
    "\n",
    "agg_merge = pd.merge(agg_drugs, df_mod,  how='left', left_on=['givenat','patientid'], right_on = ['datetime','patientid'])\n",
    "agg_merge = agg_merge.drop(['datetime'],axis=1)\n",
    "agg_merge.rename({'givenat': 'timepoint'}, axis=1, inplace=True)\n",
    "\n",
    "agg_merge = agg_merge[agg_merge['variableid'].notna()]\n",
    "\n",
    "print(agg_merge.shape)\n",
    "agg_merge.head()\n",
    "\n",
    "#610,917 instances with non-zero readings for Adrenaline/Noradrenaline AND all 4 physiological parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) For (patientid + timepoint) present in agg_pool, find all instances of physiological parameter readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter obs_tab1 for only relevant (patientid + timepoint) combinations (ie. those present in agg_pool dataframe)\n",
    "\n",
    "obs1_mod = pd.merge(obs_tab1, agg_pool,  how='left', left_on=['timepoint','patientid'], right_on = ['timepoint','patientid'])\n",
    "obs1_mod = obs1_mod[obs1_mod['variableid_y'].notna()]\n",
    "obs1_mod.rename({'variableid_x': 'variable'}, axis=1, inplace=True)\n",
    "\n",
    "obs1 = obs1_mod[['patientid','timepoint', 'variable', 'value']]\n",
    "obs1 = obs1.sort_values(by=['patientid','timepoint','variable'], ascending=[True,True,True])\n",
    "\n",
    "#Replace variable numbers by names\n",
    "obs1['variable'] = obs1['variable'].replace({ 2010 : \"FiO2\", 4000 : \"SpO2\", 8280: \"SpO2\", 200 : \"HR\", \n",
    "                                                    110 : \"MAP\", 610 : \"MAP\"})\n",
    "\n",
    "#List all variables per unique (patientid + timepoint)\n",
    "##Primary Key = (patientid + timepoint + variable)\n",
    "obs1 = obs1.groupby(['patientid','timepoint', 'variable'])['value'].apply(list)\n",
    "\n",
    "obs1 = pd.DataFrame(data=obs1)\n",
    "obs1 = obs1.reset_index()\n",
    "obs1.to_csv('obs1.csv')\n",
    "print(obs1.shape)\n",
    "obs1\n",
    "\n",
    "##1,161,327 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter obs_tab2 for only relevant (patientid + timepoint) combinations (ie. those present in agg_pool dataframe)\n",
    "\n",
    "obs2_mod = pd.merge(obs_tab2, agg_pool,  how='left', left_on=['timepoint','patientid'], right_on = ['timepoint','patientid'])\n",
    "obs2_mod = obs2_mod[obs2_mod['variableid_y'].notna()]\n",
    "obs2_mod.rename({'variableid_x': 'variable'}, axis=1, inplace=True)\n",
    "\n",
    "obs2 = obs2_mod[['patientid','timepoint', 'variable', 'value']]\n",
    "obs2 = obs2.sort_values(by=['patientid','timepoint','variable'], ascending=[True,True,True])\n",
    "\n",
    "#Replace variable numbers by names\n",
    "obs2['variable'] = obs2['variable'].replace({ 2010 : \"FiO2\", 4000 : \"SpO2\", 8280: \"SpO2\", 200 : \"HR\", \n",
    "                                                    110 : \"MAP\", 610 : \"MAP\"})\n",
    "\n",
    "#List all variables per unique (patientid + timepoint)\n",
    "obs2 = obs2.groupby(['patientid','timepoint', 'variable'])['value'].apply(list)\n",
    "\n",
    "obs2 = pd.DataFrame(data=obs2)\n",
    "obs2 = obs2.reset_index()\n",
    "obs2.to_csv('obs2.csv')\n",
    "print(obs2.shape)\n",
    "obs2\n",
    "\n",
    "#1,218,664 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter obs_tab3 for only relevant (patientid + timepoint) combinations (ie. those present in agg_pool dataframe)\n",
    "\n",
    "obs3_mod = pd.merge(obs_tab3, agg_pool,  how='left', left_on=['timepoint','patientid'], right_on = ['timepoint','patientid'])\n",
    "obs3_mod = obs3_mod[obs3_mod['variableid_y'].notna()]\n",
    "obs3_mod.rename({'variableid_x': 'variable'}, axis=1, inplace=True)\n",
    "\n",
    "obs3 = obs3_mod[['patientid','timepoint', 'variable', 'value']]\n",
    "obs3 = obs3.sort_values(by=['patientid','timepoint','variable'], ascending=[True,True,True])\n",
    "\n",
    "#Replace variable numbers by names\n",
    "obs3['variable'] = obs3['variable'].replace({ 2010 : \"FiO2\", 4000 : \"SpO2\", 8280: \"SpO2\", 200 : \"HR\", \n",
    "                                                    110 : \"MAP\", 610 : \"MAP\"})\n",
    "\n",
    "#List all variables per unique (patientid + timepoint)\n",
    "obs3 = obs3.groupby(['patientid','timepoint', 'variable'])['value'].apply(list)\n",
    "\n",
    "obs3 = pd.DataFrame(data=obs3)\n",
    "obs3 = obs3.reset_index()\n",
    "obs3.to_csv('obs3.csv')\n",
    "print(obs3.shape)\n",
    "obs3\n",
    "\n",
    "#1,249,820 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter obs_tab4 for only relevant (patientid + timepoint) combinations (ie. those present in agg_pool dataframe)\n",
    "\n",
    "obs4_mod = pd.merge(obs_tab4, agg_pool,  how='left', left_on=['timepoint','patientid'], right_on = ['timepoint','patientid'])\n",
    "obs4_mod = obs4_mod[obs4_mod['variableid_y'].notna()]\n",
    "obs4_mod.rename({'variableid_x': 'variable'}, axis=1, inplace=True)\n",
    "\n",
    "obs4 = obs4_mod[['patientid','timepoint', 'variable', 'value']]\n",
    "obs4 = obs4.sort_values(by=['patientid','timepoint','variable'], ascending=[True,True,True])\n",
    "\n",
    "#Replace variable numbers by names\n",
    "obs4['variable'] = obs4['variable'].replace({ 2010 : \"FiO2\", 4000 : \"SpO2\", 8280: \"SpO2\", 200 : \"HR\", \n",
    "                                                    110 : \"MAP\", 610 : \"MAP\"})\n",
    "\n",
    "#List all variables per unique (patientid + timepoint)\n",
    "obs4 = obs4.groupby(['patientid','timepoint', 'variable'])['value'].apply(list)\n",
    "\n",
    "obs4 = pd.DataFrame(data=obs4)\n",
    "obs4 = obs4.reset_index()\n",
    "obs4.to_csv('obs4.csv')\n",
    "print(obs4.shape)\n",
    "obs4\n",
    "\n",
    "#1,169,874 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter obs_tab5 for only relevant (patientid + timepoint) combinations (ie. those present in agg_pool dataframe)\n",
    "\n",
    "obs5_mod = pd.merge(obs_tab5, agg_pool,  how='left', left_on=['timepoint','patientid'], right_on = ['timepoint','patientid'])\n",
    "obs5_mod = obs5_mod[obs5_mod['variableid_y'].notna()]\n",
    "obs5_mod.rename({'variableid_x': 'variable'}, axis=1, inplace=True)\n",
    "\n",
    "obs5 = obs5_mod[['patientid','timepoint', 'variable', 'value']]\n",
    "obs5 = obs5.sort_values(by=['patientid','timepoint','variable'], ascending=[True,True,True])\n",
    "\n",
    "#Replace variable numbers by names\n",
    "obs5['variable'] = obs5['variable'].replace({ 2010 : \"FiO2\", 4000 : \"SpO2\", 8280: \"SpO2\", 200 : \"HR\", \n",
    "                                                    110 : \"MAP\", 610 : \"MAP\"})\n",
    "\n",
    "#List all variables per unique (patientid + timepoint)\n",
    "obs5 = obs5.groupby(['patientid','timepoint', 'variable'])['value'].apply(list)\n",
    "\n",
    "obs5 = pd.DataFrame(data=obs5)\n",
    "obs5 = obs5.reset_index()\n",
    "obs5.to_csv('obs5.csv')\n",
    "print(obs5.shape)\n",
    "obs5\n",
    "\n",
    "##1,175,952 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter obs_tab6 for only relevant (patientid + timepoint) combinations (ie. those present in agg_pool dataframe)\n",
    "\n",
    "obs6_mod = pd.merge(obs_tab6, agg_pool,  how='left', left_on=['timepoint','patientid'], right_on = ['timepoint','patientid'])\n",
    "obs6_mod = obs6_mod[obs6_mod['variableid_y'].notna()]\n",
    "obs6_mod.rename({'variableid_x': 'variable'}, axis=1, inplace=True)\n",
    "\n",
    "obs6 = obs6_mod[['patientid','timepoint', 'variable', 'value']]\n",
    "obs6 = obs6.sort_values(by=['patientid','timepoint','variable'], ascending=[True,True,True])\n",
    "\n",
    "#Replace variable numbers by names\n",
    "obs6['variable'] = obs6['variable'].replace({ 2010 : \"FiO2\", 4000 : \"SpO2\", 8280: \"SpO2\", 200 : \"HR\", \n",
    "                                                    110 : \"MAP\", 610 : \"MAP\"})\n",
    "\n",
    "#List all variables per unique (patientid + timepoint)\n",
    "obs6 = obs6.groupby(['patientid','timepoint', 'variable'])['value'].apply(list)\n",
    "\n",
    "obs6 = pd.DataFrame(data=obs6)\n",
    "obs6 = obs6.reset_index()\n",
    "obs6.to_csv('obs6.csv')\n",
    "print(obs6.shape)\n",
    "obs6\n",
    "\n",
    "##1,181,113 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter obs_tab7 for only relevant (patientid + timepoint) combinations (ie. those present in agg_pool dataframe)\n",
    "\n",
    "obs7_mod = pd.merge(obs_tab7, agg_pool,  how='left', left_on=['timepoint','patientid'], right_on = ['timepoint','patientid'])\n",
    "obs7_mod = obs7_mod[obs7_mod['variableid_y'].notna()]\n",
    "obs7_mod.rename({'variableid_x': 'variable'}, axis=1, inplace=True)\n",
    "\n",
    "obs7 = obs7_mod[['patientid','timepoint', 'variable', 'value']]\n",
    "obs7 = obs7.sort_values(by=['patientid','timepoint','variable'], ascending=[True,True,True])\n",
    "\n",
    "#Replace variable numbers by names\n",
    "\n",
    "obs7['variable'] = obs7['variable'].replace({ 2010 : \"FiO2\", 4000 : \"SpO2\", 8280: \"SpO2\", 200 : \"HR\", \n",
    "                                                    110 : \"MAP\", 610 : \"MAP\"})\n",
    "\n",
    "#List all variables per unique (patientid + timepoint)\n",
    "##Primary Key = (patientid + timepoint + variable)\n",
    "obs7 = obs7.groupby(['patientid','timepoint', 'variable'])['value'].apply(list)\n",
    "\n",
    "obs7 = pd.DataFrame(data=obs7)\n",
    "obs7 = obs7.reset_index()\n",
    "obs7.to_csv('obs7.csv')\n",
    "print(obs7.shape)\n",
    "obs7\n",
    "\n",
    "##1,250,260 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter obs_tab8 for only relevant (patientid + timepoint) combinations (ie. those present in agg_pool dataframe)\n",
    "\n",
    "obs8_mod = pd.merge(obs_tab8, agg_pool,  how='left', left_on=['timepoint','patientid'], right_on = ['timepoint','patientid'])\n",
    "obs8_mod = obs8_mod[obs8_mod['variableid_y'].notna()]\n",
    "obs8_mod.rename({'variableid_x': 'variable'}, axis=1, inplace=True)\n",
    "\n",
    "obs8 = obs8_mod[['patientid','timepoint', 'variable', 'value']]\n",
    "obs8 = obs8.sort_values(by=['patientid','timepoint','variable'], ascending=[True,True,True])\n",
    "\n",
    "#Replace variable numbers by names\n",
    "\n",
    "obs8['variable'] = obs8['variable'].replace({ 2010 : \"FiO2\", 4000 : \"SpO2\", 8280: \"SpO2\", 200 : \"HR\", \n",
    "                                                    110 : \"MAP\", 610 : \"MAP\"})\n",
    "\n",
    "#List all variables per unique (patientid + timepoint)\n",
    "##Primary Key = (patientid + timepoint + variable)\n",
    "obs8 = obs8.groupby(['patientid','timepoint', 'variable'])['value'].apply(list)\n",
    "\n",
    "obs8 = pd.DataFrame(data=obs8)\n",
    "obs8 = obs8.reset_index()\n",
    "obs8.to_csv('obs8.csv')\n",
    "print(obs8.shape)\n",
    "obs8\n",
    "\n",
    "##1,215,066 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter obs_tab9 for only relevant (patientid + timepoint) combinations (ie. those present in agg_pool dataframe)\n",
    "\n",
    "obs9_mod = pd.merge(obs_tab9, agg_pool,  how='left', left_on=['timepoint','patientid'], right_on = ['timepoint','patientid'])\n",
    "obs9_mod = obs9_mod[obs9_mod['variableid_y'].notna()]\n",
    "obs9_mod.rename({'variableid_x': 'variable'}, axis=1, inplace=True)\n",
    "\n",
    "obs9 = obs9_mod[['patientid','timepoint', 'variable', 'value']]\n",
    "obs9 = obs9.sort_values(by=['patientid','timepoint','variable'], ascending=[True,True,True])\n",
    "\n",
    "#Replace variable numbers by names\n",
    "\n",
    "obs9['variable'] = obs9['variable'].replace({ 2010 : \"FiO2\", 4000 : \"SpO2\", 8280: \"SpO2\", 200 : \"HR\", \n",
    "                                                    110 : \"MAP\", 610 : \"MAP\"})\n",
    "\n",
    "#List all variables per unique (patientid + timepoint)\n",
    "##Primary Key = (patientid + timepoint + variable)\n",
    "obs9 = obs9.groupby(['patientid','timepoint', 'variable'])['value'].apply(list)\n",
    "\n",
    "obs9 = pd.DataFrame(data=obs9)\n",
    "obs9 = obs9.reset_index()\n",
    "obs9.to_csv('obs9.csv')\n",
    "print(obs9.shape)\n",
    "obs9\n",
    "\n",
    "##1,233,930 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter obs_tab10 for only relevant (patientid + timepoint) combinations (ie. those present in agg_pool dataframe)\n",
    "\n",
    "obs10_mod = pd.merge(obs_tab10, agg_pool,  how='left', left_on=['timepoint','patientid'], right_on = ['timepoint','patientid'])\n",
    "obs10_mod = obs10_mod[obs10_mod['variableid_y'].notna()]\n",
    "obs10_mod.rename({'variableid_x': 'variable'}, axis=1, inplace=True)\n",
    "\n",
    "obs10 = obs10_mod[['patientid','timepoint', 'variable', 'value']]\n",
    "obs10 = obs10.sort_values(by=['patientid','timepoint','variable'], ascending=[True,True,True])\n",
    "\n",
    "#Replace variable numbers by names\n",
    "\n",
    "obs10['variable'] = obs10['variable'].replace({ 2010 : \"FiO2\", 4000 : \"SpO2\", 8280: \"SpO2\", 200 : \"HR\", \n",
    "                                                    110 : \"MAP\", 610 : \"MAP\"})\n",
    "\n",
    "#List all variables per unique (patientid + timepoint)\n",
    "##Primary Key = (patientid + timepoint + variable)\n",
    "obs10 = obs10.groupby(['patientid','timepoint', 'variable'])['value'].apply(list)\n",
    "\n",
    "obs10 = pd.DataFrame(data=obs10)\n",
    "obs10 = obs10.reset_index()\n",
    "obs10.to_csv('obs10.csv')\n",
    "print(obs10.shape)\n",
    "obs10\n",
    "\n",
    "##1,099,819 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all obsx\n",
    "obs1 = pd.read_csv(\"obs1.csv\")\n",
    "obs1 = obs1.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "obs2 = pd.read_csv(\"obs2.csv\")\n",
    "obs2 = obs2.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "obs3 = pd.read_csv(\"obs3.csv\")\n",
    "obs3 = obs3.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "obs4 = pd.read_csv(\"obs4.csv\")\n",
    "obs4 = obs4.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "obs5 = pd.read_csv(\"obs5.csv\")\n",
    "obs5 = obs5.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "obs6 = pd.read_csv(\"obs6.csv\")\n",
    "obs6 = obs6.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "obs7 = pd.read_csv(\"obs7.csv\")\n",
    "obs7 = obs7.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "obs8 = pd.read_csv(\"obs8.csv\")\n",
    "obs8 = obs8.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "obs9 = pd.read_csv(\"obs9.csv\")\n",
    "obs9 = obs9.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "obs10 = pd.read_csv(\"obs10.csv\")\n",
    "obs10 = obs10.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "obs10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge \n",
    "\n",
    "frames = [obs1, obs2, obs3, obs4, obs5, obs6, obs7, obs8, obs9, obs10]\n",
    "\n",
    "params_comp= pd.concat(frames)\n",
    "\n",
    "params_comp.to_csv('params_comp.csv')\n",
    "\n",
    "print(params_comp.shape)\n",
    "params_comp\n",
    "\n",
    "##11,955,825 records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check unique ICU admissions in params_comp\n",
    "\n",
    "params_comp['patientid'].nunique()\n",
    "\n",
    "##20,286 unique ICU admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "\n",
    "params_comp = pd.read_csv(\"params_comp.csv\")\n",
    "params_comp = params_comp.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "print(params_comp.shape)\n",
    "params_comp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_comp.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Check all variable values are in acceptable range with compatible units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check format of 'value' column\n",
    "\n",
    "for i, l in enumerate(params_comp[\"value\"]):\n",
    "    print(\"list\",i,\"is\",type(l))\n",
    "    \n",
    "##all rows in 'value' column are in str format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if necessary - convert timepoint to datetime format\n",
    "params_comp['timepoint'] =  pd.to_datetime(params_comp['timepoint'], format='%Y-%m-%d %H:%M:%S')\n",
    "params_comp.dtypes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check all FiO2 values are in acceptable range\n",
    "\n",
    "check_fio2 = params_comp.copy(deep=True)\n",
    "\n",
    "check_fio2 = check_fio2[check_fio2['variable']=='FiO2']\n",
    "check_fio2['fio2_check'] = check_fio2.value.apply(lambda x: [ all(c >= 21 and c <=100 for c in x)])\n",
    "check_fio2['acceptable_range'] = check_fio2.fio2_check.apply(lambda x: 'Yes' if True in x else 'No')\n",
    "\n",
    "check_fio2\n",
    "\n",
    "##2,983,684 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(check_fio2[check_fio2['acceptable_range']=='No'])\n",
    "\n",
    "#2,439 instances containing FiO2 values outside acceptable range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new column FiO2 values that are in acceptable range\n",
    "\n",
    "check_fio2['inrange_val_list'] = check_fio2.value.apply(lambda x: [ c for c in x if 21 <= c <= 100])\n",
    "check_fio2 = check_fio2.drop(['value','fio2_check','acceptable_range'],axis=1)\n",
    "\n",
    "check_fio2 = check_fio2.dropna()\n",
    "\n",
    "print(check_fio2.shape)\n",
    "check_fio2.head()\n",
    "\n",
    "##2,983,684 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm all FiO2 values are in acceptable range\n",
    "\n",
    "check_fio2['fio2_check'] = check_fio2.inrange_val_list.apply(lambda x: [ all(c >= 21 and c <=100 for c in x)])\n",
    "check_fio2['acceptable_range'] = check_fio2.fio2_check.apply(lambda x: 'Yes' if True in x else 'No')\n",
    "\n",
    "check_fio2['acceptable_range'].unique()\n",
    "\n",
    "##All FiO2 values are now in acceptable range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unncessary columns\n",
    "\n",
    "check_fio2 = check_fio2.drop(['fio2_check','acceptable_range'],axis=1)\n",
    "\n",
    "check_fio2.to_csv('check_fio2.csv')\n",
    "\n",
    "print(check_fio2.shape)\n",
    "check_fio2.head()\n",
    "\n",
    "##2,983,684 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check all SpO2 values are in acceptable range\n",
    "\n",
    "check_spo2 = params_comp.copy(deep=True)\n",
    "\n",
    "check_spo2 = check_spo2[check_spo2['variable']=='SpO2']\n",
    "check_spo2['spo2_check'] = check_spo2.value.apply(lambda x: [ all(c >= 0 and c <=100 for c in x)])\n",
    "check_spo2['acceptable_range'] = check_spo2.spo2_check.apply(lambda x: 'Yes' if True in x else 'No')\n",
    "\n",
    "check_spo2\n",
    "\n",
    "##2,988,937 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(check_spo2[check_spo2['acceptable_range']=='No'])\n",
    "\n",
    "#14 instances contain SpO2 values outside acceptable ranges "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new column SpO2 values that are in acceptable range\n",
    "\n",
    "check_spo2['inrange_val_list'] = check_spo2.value.apply(lambda x: [ c for c in x if 0 <= c <= 100])\n",
    "check_spo2 = check_spo2.drop(['value','spo2_check','acceptable_range'],axis=1)\n",
    "\n",
    "check_spo2 = check_spo2.dropna()\n",
    "\n",
    "print(check_spo2.shape)\n",
    "check_spo2.head()\n",
    "\n",
    "##2,988,937 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm all SpO2 values are in acceptable range\n",
    "\n",
    "check_spo2['spo2_check'] = check_spo2.inrange_val_list.apply(lambda x: [ all(c >= 0 and c <=100 for c in x)])\n",
    "check_spo2['acceptable_range'] = check_spo2.spo2_check.apply(lambda x: 'Yes' if True in x else 'No')\n",
    "\n",
    "check_spo2['acceptable_range'].unique()\n",
    "\n",
    "##All SpO2 values are now in acceptable range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unncessary columns\n",
    "\n",
    "check_spo2 = check_spo2.drop(['spo2_check','acceptable_range'],axis=1)\n",
    "\n",
    "check_spo2.to_csv('check_spo2.csv')\n",
    "\n",
    "print(check_spo2.shape)\n",
    "check_spo2.head()\n",
    "\n",
    "##2,988,937 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check all MAP values are in acceptable range\n",
    "\n",
    "check_map = params_comp.copy(deep=True)\n",
    "\n",
    "check_map = check_map[check_map['variable']=='MAP']\n",
    "check_map['map_check'] = check_map.value.apply(lambda x: [ all(c >= 0 and c <=200 for c in x)])\n",
    "check_map['acceptable_range'] = check_map.map_check.apply(lambda x: 'Yes' if True in x else 'No')\n",
    "\n",
    "check_map\n",
    "\n",
    "##2,989,610 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(check_map[check_map['acceptable_range']=='No'])\n",
    "\n",
    "#61,820 instances contain an MAP reading outside the acceptable range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new column MAP values that are in acceptable range\n",
    "\n",
    "check_map['inrange_val_list'] = check_map.value.apply(lambda x: [ c for c in x if 0 <= c <= 200])\n",
    "check_map = check_map.drop(['value','map_check','acceptable_range'],axis=1)\n",
    "\n",
    "check_map = check_map.dropna()\n",
    "\n",
    "print(check_map.shape)\n",
    "check_map.head()\n",
    "\n",
    "##2,989,610 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm all MAP values are in acceptable range\n",
    "\n",
    "check_map['map_check'] = check_map.inrange_val_list.apply(lambda x: [ all(c >= 0 and c <=200 for c in x)])\n",
    "check_map['acceptable_range'] = check_map.map_check.apply(lambda x: 'Yes' if True in x else 'No')\n",
    "\n",
    "check_map['acceptable_range'].unique()\n",
    "\n",
    "##All MAP values are now in acceptable range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unncessary columns\n",
    "\n",
    "check_map = check_map.drop(['map_check','acceptable_range'],axis=1)\n",
    "\n",
    "check_map.to_csv('check_map.csv')\n",
    "\n",
    "print(check_map.shape)\n",
    "check_map.head()\n",
    "\n",
    "##2,989,610 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check all HR values are in acceptable range\n",
    "\n",
    "check_hr = params_comp.copy(deep=True)\n",
    "\n",
    "check_hr = check_hr[check_hr['variable']=='HR']\n",
    "check_hr['hr_check'] = check_hr.value.apply(lambda x: [ all(c >= 0 and c <=300 for c in x)])\n",
    "check_hr['acceptable_range'] = check_hr.hr_check.apply(lambda x: 'Yes' if True in x else 'No')\n",
    "check_hr\n",
    "\n",
    "##2,993,594 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(check_hr[check_hr['acceptable_range']=='No'])\n",
    "\n",
    "#All HR values are in acceptable range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unncessary columns\n",
    "\n",
    "check_hr = check_hr.drop(['hr_check','acceptable_range'],axis=1)\n",
    "check_hr.rename({'value': 'inrange_val_list'}, axis=1, inplace=True)\n",
    "\n",
    "check_hr.to_csv('check_hr.csv')\n",
    "\n",
    "print(check_hr.shape)\n",
    "check_hr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(check_fio2.columns)\n",
    "print(check_spo2.columns)\n",
    "print(check_map.columns)\n",
    "print(check_hr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate check,fio2, check_spo2, check_map check_hr\n",
    "\n",
    "frames = [check_fio2, check_spo2, check_map, check_hr]\n",
    "\n",
    "params_comp = pd.concat(frames)\n",
    "\n",
    "params_comp.sort_values(by=['patientid','timepoint'], ascending=[True,True])\n",
    "\n",
    "params_comp.to_csv('params_comp2.csv')\n",
    "\n",
    "print(params_comp.shape)\n",
    "params_comp\n",
    "\n",
    "##11,955,825 records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8)  Extract minimum, maximum and average variable values per unique instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract minimum and maximum variable values\n",
    "\n",
    "params_comp['min_val'] = params_comp.inrange_val_list.apply(lambda x: min(x, default=np.nan))\n",
    "params_comp['max_val'] = params_comp.inrange_val_list.apply(lambda x: max(x, default=np.nan))\n",
    "\n",
    "params_comp.to_csv('params_comp3.csv')\n",
    "\n",
    "print(params_comp.shape)\n",
    "params_comp.head()\n",
    "\n",
    "##11,955,825 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_comp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract average variable value\n",
    "\n",
    "import statistics\n",
    "\n",
    "params_comp = params_comp.dropna() #drop all rows containing null values (indicates empty value list)\n",
    "\n",
    "params_comp['avg_val'] = params_comp.inrange_val_list.apply(lambda x: round(statistics.mean(x),1))\n",
    "\n",
    "params_comp.to_csv('params_comp4.csv')\n",
    "\n",
    "print(params_comp.shape)\n",
    "params_comp.head()\n",
    "\n",
    "##11,955,670 records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Remove instances where % variation is larger than defined cut-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As advised by medical advisor, a large variation should not be observed within a such a short time period, and is indicative of a data error. <br>\n",
    "•\t**% variation** = (( max_val – min_val ) / min_val ) x 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check % variation of variable readings for each instance\n",
    "\n",
    "params_comp['variation(%)'] = ((params_comp['max_val']-params_comp['min_val'])/params_comp['min_val'])*100\n",
    "\n",
    "print(params_comp.shape)\n",
    "params_comp.head() \n",
    "\n",
    "##11,955,670 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove instances where the % variation of values within each 15-minute timepoint is within is greater than defined cut-off\n",
    "###Instances to remove - cut-offs advised by X (medical advisor to project)\n",
    "\n",
    "params_comp = params_comp[((params_comp['variable']=='FiO2') & (params_comp['variation(%)']<=5))|\n",
    "                          ((params_comp['variable']=='SpO2') & (params_comp['variation(%)']<=10))|\n",
    "                          ((params_comp['variable']=='MAP') & (params_comp['variation(%)']<=25))|\n",
    "                          ((params_comp['variable']=='HR') & (params_comp['variation(%)']<=25))]\n",
    "\n",
    "\n",
    "params_comp.to_csv('params_comp5.csv')\n",
    "\n",
    "print(params_comp.shape)\n",
    "params_comp.head() \n",
    "\n",
    "#11,170,443 records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) Check average parameter values match training data units and ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract relevant columns from params_comp\n",
    "##Reformat dataframe to similar structure as Glasgow training data, using average parameter values \n",
    "\n",
    "avg_params = params_comp[['patientid','timepoint','variable','avg_val']]\n",
    "\n",
    "avg_params = avg_params.groupby(['patientid', 'timepoint', 'variable'])['avg_val'].sum().unstack('variable')\n",
    "\n",
    "avg_params = avg_params.reset_index()\n",
    "avg_params = avg_params.rename_axis(None, axis=1)\n",
    "avg_params = avg_params.dropna() #drop any rows that contain null values \n",
    "\n",
    "print(avg_params.shape)\n",
    "avg_params\n",
    "\n",
    "##2,022,310 records\n",
    "##need to convert FiO2 units from % to decFrc (HR, MAP, SpO2 have correct units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert FiO2 to decFrc (compatible with Glasgow training data)\n",
    "\n",
    "avg_params['FiO2']=round((avg_params['FiO2']/100),4)\n",
    "print(avg_params.shape)\n",
    "avg_params\n",
    "\n",
    "#2,022,310 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Confirm all average values are within acceptable range\n",
    "\n",
    "avg_params = avg_params[avg_params['FiO2']>=0.21]\n",
    "avg_params = avg_params[avg_params['FiO2']<=1.0]\n",
    "\n",
    "avg_params = avg_params[avg_params['SpO2']>0]\n",
    "avg_params = avg_params[avg_params['SpO2']<=100]\n",
    "\n",
    "avg_params = avg_params[avg_params['HR']>0]\n",
    "avg_params = avg_params[avg_params['HR']<=300]\n",
    "\n",
    "avg_params = avg_params[avg_params['MAP']>0]\n",
    "avg_params = avg_params[avg_params['MAP']<=200]\n",
    "\n",
    "avg_params.to_csv('avg_params.csv')\n",
    "print(avg_params.shape)\n",
    "avg_params\n",
    "\n",
    "##2,022,310 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check number of unique ICU admissions in avg_params\n",
    "\n",
    "avg_params['patientid'].nunique()\n",
    "\n",
    "##20,073 unique ICU admissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11) Create HiRID external dataset using Average parameter values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_params.shape)\n",
    "avg_params.head()\n",
    "\n",
    "##2,022,310 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_rate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if necessary - convert timepoint to correct datetime format\n",
    "drugs_rate['timepoint'] =  pd.to_datetime(drugs_rate['timepoint'], format='%Y-%m-%d %H:%M:%S')\n",
    "drugs_rate.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_params['timepoint'] =  pd.to_datetime(avg_params['timepoint'], format='%Y-%m-%d %H:%M:%S')\n",
    "avg_params.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge avg_params and drugs_rate on (patientid + timepoint)\n",
    "\n",
    "comp_avg_params = pd.merge(avg_params, drugs_rate, how='left', on=['patientid','timepoint'])\n",
    "comp_avg_params = comp_avg_params[['patientid','timepoint','Adrenaline','Noradrenaline','FiO2','SpO2','MAP','HR']]\n",
    "\n",
    "comp_avg_params['rowID']=comp_avg_params.index #create new index column\n",
    "\n",
    "#move index col to start\n",
    "col_at_start = ['rowID']\n",
    "comp_avg_params = comp_avg_params[[c for c in col_at_start if c in comp_avg_params] + [c for c in comp_avg_params if c not in col_at_start]]\n",
    "\n",
    "#replace null with 0 in drug fields (as blank value indicates value=0)\n",
    "comp_avg_params['Adrenaline'] = comp_avg_params['Adrenaline'].replace(np.nan, 0)\n",
    "comp_avg_params['Noradrenaline'] = comp_avg_params['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "comp_avg_params.to_csv('comp_avg_params.csv')\n",
    "\n",
    "print(comp_avg_params.shape)\n",
    "comp_avg_params\n",
    "\n",
    "##2,022,310 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check number of non-zero drug fields\n",
    "\n",
    "print(len(comp_avg_params[comp_avg_params['Adrenaline']>0]))\n",
    "print(len(comp_avg_params[comp_avg_params['Noradrenaline']>0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12) Find discharge & death times for each unique ICU admission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the HiRID databse does not contain data for an ICU discharge/death time, the last Heart Rate reading was used as the discharge time (if discharge_status=alive) or death time (if discharge_status=dead). This approach was recommended by the HiRID v1.1.1 database corresponding author, Martin Faltys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to observations_table_1 - find all HR readings\n",
    "\n",
    "hr_tab1 = pd.read_sql_query(\" SELECT * FROM hirid.observations_table_1 \\\n",
    "                                    WHERE variableid IN (200) \\\n",
    "                                    ORDER BY patientid, datetime \", conn)\n",
    "\n",
    "hr_tab1['row_idx']=hr_tab1.index #create new index column\n",
    "\n",
    "print(hr_tab1.shape)\n",
    "hr_tab1.head()\n",
    "\n",
    "##6,076,840 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_tab1_patlist = list(hr_tab1['patientid'].unique())\n",
    "len(hr_tab1_patlist)\n",
    "\n",
    "##3386 unique admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find date of latest Heart Rate for each patientid in hr_tab1 -> this is equivalent to discharge/death time \n",
    "\n",
    "temp = hr_tab1.groupby('patientid')\n",
    "dtime_tab1 = temp.agg(max_date=('datetime', np.max))\n",
    "dtime_tab1 = dtime_tab1.reset_index()\n",
    "\n",
    "dtime_tab1.to_csv('dtime_tab1.csv')\n",
    "dtime_tab1\n",
    "\n",
    "#max_date = discharge time/ death time for all patientids in hr_tab1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to observations_table_2 - find all HR readings\n",
    "\n",
    "hr_tab2 = pd.read_sql_query(\" SELECT * FROM hirid.observations_table_2 \\\n",
    "                                    WHERE variableid IN (200) \\\n",
    "                                    ORDER BY patientid, datetime \", conn)\n",
    "\n",
    "hr_tab2['row_idx']=hr_tab2.index #create new index column\n",
    "\n",
    "print(hr_tab2.shape)\n",
    "hr_tab2.head()\n",
    "\n",
    "##5,943,365 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_tab2_patlist = list(hr_tab2['patientid'].unique())\n",
    "len(hr_tab2_patlist)\n",
    "\n",
    "##3391 unique admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find date of latest Heart Rate for each patientid in hr_tab2\n",
    "\n",
    "temp = hr_tab2.groupby('patientid')\n",
    "dtime_tab2 = temp.agg(max_date=('datetime', np.max))\n",
    "dtime_tab2 = dtime_tab2.reset_index()\n",
    "\n",
    "dtime_tab2.to_csv('dtime_tab2.csv')\n",
    "\n",
    "dtime_tab2\n",
    "\n",
    "#max_date = discharge time/ death time for all patientids in hr_tab2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to observations_table_3 - find all HR readings\n",
    "\n",
    "hr_tab3 = pd.read_sql_query(\" SELECT * FROM hirid.observations_table_3 \\\n",
    "                                    WHERE variableid IN (200) \\\n",
    "                                    ORDER BY patientid, datetime \", conn)\n",
    "\n",
    "hr_tab3['row_idx']=hr_tab3.index #create new index column\n",
    "\n",
    "print(hr_tab3.shape)\n",
    "hr_tab3.head()\n",
    "\n",
    "##6,157,038 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_tab3_patlist = list(hr_tab3['patientid'].unique())\n",
    "len(hr_tab3_patlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find date of latest Heart Rate for each patientid in hr_tab3\n",
    "\n",
    "temp = hr_tab3.groupby('patientid')\n",
    "dtime_tab3 = temp.agg(max_date=('datetime', np.max))\n",
    "dtime_tab3 = dtime_tab3.reset_index()\n",
    "\n",
    "dtime_tab3.to_csv('dtime_tab3.csv')\n",
    "dtime_tab3\n",
    "\n",
    "#max_date = discharge time/ death time for all patientids in hr_tab3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to observations_table_4 - find all HR readings\n",
    "\n",
    "hr_tab4 = pd.read_sql_query(\" SELECT * FROM hirid.observations_table_4 \\\n",
    "                                    WHERE variableid IN (200) \\\n",
    "                                    ORDER BY patientid, datetime \", conn)\n",
    "\n",
    "hr_tab4['row_idx']=hr_tab4.index #create new index column\n",
    "\n",
    "print(hr_tab4.shape)\n",
    "hr_tab4.head()\n",
    "\n",
    "##6,072,003 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_tab4_patlist = list(hr_tab4['patientid'].unique())\n",
    "len(hr_tab4_patlist)\n",
    "\n",
    "##3391 unique admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find date of latest Heart Rate for each patientid in hr_tab4\n",
    "\n",
    "temp = hr_tab4.groupby('patientid')\n",
    "dtime_tab4 = temp.agg(max_date=('datetime', np.max))\n",
    "dtime_tab4 = dtime_tab4.reset_index()\n",
    "\n",
    "dtime_tab4.to_csv('dtime_tab4.csv')\n",
    "dtime_tab4\n",
    "\n",
    "#max_date = discharge time/ death time for all patientids in hr_tab4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to observations_table_5 - find all HR readings\n",
    "\n",
    "hr_tab5 = pd.read_sql_query(\" SELECT * FROM hirid.observations_table_5 \\\n",
    "                                    WHERE variableid IN (200) \\\n",
    "                                    ORDER BY patientid, datetime \", conn)\n",
    "\n",
    "hr_tab5['row_idx']=hr_tab5.index #create new index column\n",
    "\n",
    "print(hr_tab5.shape)\n",
    "hr_tab5.head()\n",
    "\n",
    "##6,059,325 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_tab5_patlist = list(hr_tab5['patientid'].unique())\n",
    "len(hr_tab5_patlist)\n",
    "\n",
    "##3388 unique admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find date of latest Heart Rate for each patientid in hr_tab5\n",
    "\n",
    "temp = hr_tab5.groupby('patientid')\n",
    "dtime_tab5 = temp.agg(max_date=('datetime', np.max))\n",
    "dtime_tab5 = dtime_tab5.reset_index()\n",
    "\n",
    "dtime_tab5.to_csv('dtime_tab5.csv')\n",
    "dtime_tab5\n",
    "\n",
    "#max_date = discharge time/ death time for all patientids in hr_tab5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to observations_table_6 - find all HR readings\n",
    "\n",
    "hr_tab6 = pd.read_sql_query(\" SELECT * FROM hirid.observations_table_6 \\\n",
    "                                    WHERE variableid IN (200) \\\n",
    "                                    ORDER BY patientid, datetime \", conn)\n",
    "\n",
    "hr_tab6['row_idx']=hr_tab6.index #create new index column\n",
    "\n",
    "print(hr_tab6.shape)\n",
    "hr_tab6.head()\n",
    "\n",
    "##6,098,969 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_tab6_patlist = list(hr_tab6['patientid'].unique())\n",
    "len(hr_tab6_patlist)\n",
    "\n",
    "##3391 unique admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find date of latest Heart Rate for each patientid in hr_tab6\n",
    "\n",
    "temp = hr_tab6.groupby('patientid')\n",
    "dtime_tab6 = temp.agg(max_date=('datetime', np.max))\n",
    "dtime_tab6 = dtime_tab6.reset_index()\n",
    "\n",
    "dtime_tab6.to_csv('dtime_tab6.csv')\n",
    "dtime_tab6\n",
    "\n",
    "#max_date = discharge time/ death time for all patientids in hr_tab6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to observations_table_7 - find all HR readings\n",
    "\n",
    "hr_tab7 = pd.read_sql_query(\" SELECT * FROM hirid.observations_table_7 \\\n",
    "                                    WHERE variableid IN (200) \\\n",
    "                                    ORDER BY patientid, datetime \", conn)\n",
    "\n",
    "hr_tab7['row_idx']=hr_tab7.index #create new index column\n",
    "\n",
    "print(hr_tab7.shape)\n",
    "hr_tab7.head()\n",
    "\n",
    "##6,126,086 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_tab7_patlist = list(hr_tab7['patientid'].unique())\n",
    "len(hr_tab7_patlist)\n",
    "\n",
    "##3390 unique admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find date of latest Heart Rate for each patientid in hr_tab7\n",
    "\n",
    "temp = hr_tab7.groupby('patientid')\n",
    "dtime_tab7 = temp.agg(max_date=('datetime', np.max))\n",
    "dtime_tab7 = dtime_tab7.reset_index()\n",
    "\n",
    "dtime_tab7.to_csv('dtime_tab7.csv')\n",
    "dtime_tab7\n",
    "\n",
    "#max_date = discharge time/ death time for all patientids in hr_tab7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to observations_table_8 - find all HR readings \n",
    "\n",
    "hr_tab8 = pd.read_sql_query(\" SELECT * FROM hirid.observations_table_8 \\\n",
    "                                    WHERE variableid IN (200) \\\n",
    "                                    ORDER BY patientid, datetime \", conn)\n",
    "\n",
    "hr_tab8['row_idx']=hr_tab8.index #create new index column\n",
    "\n",
    "print(hr_tab8.shape)\n",
    "hr_tab8.head()\n",
    "\n",
    "##5,964,420 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_tab8_patlist = list(hr_tab8['patientid'].unique())\n",
    "len(hr_tab8_patlist)\n",
    "\n",
    "##3389 unique admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find date of latest Heart Rate for each patientid in hr_tab8\n",
    "\n",
    "temp = hr_tab8.groupby('patientid')\n",
    "dtime_tab8 = temp.agg(max_date=('datetime', np.max))\n",
    "dtime_tab8 = dtime_tab8.reset_index()\n",
    "\n",
    "dtime_tab8.to_csv('dtime_tab8.csv')\n",
    "dtime_tab8\n",
    "\n",
    "#max_date = discharge time/ death time for all patientids in hr_tab8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to observations_table_9 - find all HR readings\n",
    "\n",
    "hr_tab9 = pd.read_sql_query(\" SELECT * FROM hirid.observations_table_9 \\\n",
    "                                    WHERE variableid IN (200) \\\n",
    "                                    ORDER BY patientid, datetime \", conn)\n",
    "\n",
    "hr_tab9['row_idx']=hr_tab9.index #create new index column\n",
    "\n",
    "print(hr_tab9.shape)\n",
    "hr_tab9.head()\n",
    "\n",
    "##6,163,893 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_tab9_patlist = list(hr_tab9['patientid'].unique())\n",
    "len(hr_tab9_patlist)\n",
    "\n",
    "##3389 unique admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find date of latest Heart Rate for each patientid in hr_tab9\n",
    "\n",
    "temp = hr_tab9.groupby('patientid')\n",
    "dtime_tab9 = temp.agg(max_date=('datetime', np.max))\n",
    "dtime_tab9 = dtime_tab9.reset_index()\n",
    "\n",
    "dtime_tab9.to_csv('dtime_tab9.csv')\n",
    "dtime_tab9\n",
    "\n",
    "#max_date = discharge time/ death time for all patientids in hr_tab9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to observations_table_10 - find all HR readings\n",
    "\n",
    "hr_tab10 = pd.read_sql_query(\" SELECT * FROM hirid.observations_table_10 \\\n",
    "                                    WHERE variableid IN (200) \\\n",
    "                                    ORDER BY patientid, datetime \", conn)\n",
    "\n",
    "hr_tab10['row_idx']=hr_tab10.index #create new index column\n",
    "\n",
    "print(hr_tab10.shape)\n",
    "hr_tab10.head()\n",
    "\n",
    "##5,770,397 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_tab10_patlist = list(hr_tab10['patientid'].unique())\n",
    "len(hr_tab10_patlist)\n",
    "\n",
    "##3392 unique admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find date of latest Heart Rate for each patientid in hr_tab10\n",
    "\n",
    "temp = hr_tab10.groupby('patientid')\n",
    "dtime_tab10 = temp.agg(max_date=('datetime', np.max))\n",
    "dtime_tab10 = dtime_tab10.reset_index()\n",
    "\n",
    "dtime_tab10.to_csv('dtime_tab10.csv')\n",
    "dtime_tab10\n",
    "\n",
    "#max_date = discharge time/ death time for all patientids in hr_tab10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm no overlapping patientids across the 10 observations tables\n",
    "\n",
    "pat_in_all = list(set.intersection(*map(set, [hr_tab1_patlist, hr_tab2_patlist, hr_tab3_patlist, hr_tab4_patlist, hr_tab5_patlist, hr_tab6_patlist,hr_tab7_patlist, hr_tab8_patlist, hr_tab9_patlist, hr_tab10_patlist ])))\n",
    "pat_in_all\n",
    "\n",
    "#no overlapping patientids across hr_tab1, ht_tab2, hr_tab3, hr_tab4, ht_tab5, hr_tab6, hr_tab7, ht_tab8, hr_tab9 and hr_tab10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge above dtime tables to create table of discharge/death times per patientid \n",
    "\n",
    "frames = [dtime_tab1, dtime_tab2, dtime_tab3, dtime_tab4, dtime_tab5,\n",
    "         dtime_tab6, dtime_tab7, dtime_tab8, dtime_tab9, dtime_tab10]\n",
    "\n",
    "dtimes = pd.concat(frames)\n",
    "\n",
    "dtimes.sort_values(['patientid'], ascending=[True], inplace=True)\n",
    "\n",
    "dtimes.to_csv('dtimes.csv')\n",
    "\n",
    "print(dtimes.shape)\n",
    "dtimes\n",
    "\n",
    "##33,897 unique patientids (ICU admissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to HiRID 'Patient' table (contains discharge_status info)\n",
    "\n",
    "pat = pd.read_sql_query(\"SELECT * FROM hirid.patient\", conn)\n",
    "\n",
    "pat.to_csv('patient_table.csv')\n",
    "\n",
    "print(pat.shape)\n",
    "pat.head()\n",
    "\n",
    "#33,905 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge dtimes and pat dataframes\n",
    "\n",
    "pat_info = pd.merge(pat, dtimes,  how='left', left_on=['patientid'], right_on = ['patientid'])\n",
    "pat_info.rename({'max_date': 'd_time'}, axis=1, inplace=True)\n",
    "\n",
    "pat_info.to_csv('pat_info.csv')\n",
    "\n",
    "print(pat_info.shape)\n",
    "pat_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create ICU LOS column\n",
    "\n",
    "pat_info['icu_los'] = pat_info['d_time'] - pat_info['admissiontime']\n",
    "\n",
    "#convert to hours\n",
    "pat_info['icu_los'] = (pat_info['icu_los'] / np.timedelta64(1, 'h'))\n",
    "pat_info.rename({'icu_los': 'icu_los(hrs)'}, axis=1, inplace=True)\n",
    "\n",
    "print(pat_info.shape)\n",
    "pat_info.head()\n",
    "\n",
    "#33,905 records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create fields for last hours before discharge/death (1hr/2hrs/3hrs/4hrs/7hrs/8hrs)\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "pat_info['1hr before d_time'] = pd.to_datetime(pat_info['d_time'])- timedelta(hours=1)\n",
    "pat_info['2hrs before d_time'] = pd.to_datetime(pat_info['d_time'])- timedelta(hours=2)\n",
    "pat_info['3hrs before d_time'] = pd.to_datetime(pat_info['d_time'])- timedelta(hours=3)\n",
    "pat_info['4hrs before d_time'] = pd.to_datetime(pat_info['d_time'])- timedelta(hours=4)\n",
    "pat_info['5hrs before d_time'] = pd.to_datetime(pat_info['d_time'])- timedelta(hours=5)\n",
    "pat_info['6hrs before d_time'] = pd.to_datetime(pat_info['d_time'])- timedelta(hours=6)\n",
    "pat_info['7hrs before d_time'] = pd.to_datetime(pat_info['d_time'])- timedelta(hours=7)\n",
    "pat_info['8hrs before d_time'] = pd.to_datetime(pat_info['d_time'])- timedelta(hours=8)\n",
    "\n",
    "print(pat_info.shape)\n",
    "pat_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13) Explore patient characteristics of final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comp_avg_params.shape)\n",
    "comp_avg_params\n",
    "\n",
    "##2,022,310 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract patientids within comp_avg_params\n",
    "\n",
    "pat_rel = comp_avg_params['patientid'].unique()\n",
    "pat_rel = pd.DataFrame(data=pat_rel)\n",
    "pat_rel.columns = ['patientid']\n",
    "\n",
    "print(pat_rel.shape)\n",
    "pat_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pat_info.shape)\n",
    "pat_info\n",
    "\n",
    "##33,905 unique ICU admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge pat_rel and pat_info\n",
    "\n",
    "df_patmerge = pd.merge(pat_rel, pat_info, how='left', left_on=['patientid'], right_on=['patientid'])\n",
    "print(df_patmerge.shape)\n",
    "df_patmerge.head()\n",
    "\n",
    "##20,073 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patmerge.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patmerge['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patmerge['discharge_status'].value_counts()\n",
    "\n",
    "##17,919 discharged alive\n",
    "##1,951 discharged dead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14) Define time periods for the last hours before discharge/death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comp_avg_params.shape)\n",
    "comp_avg_params.head()\n",
    "\n",
    "##2,022,310 records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1hr before discharge/death time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1hr before d_time\n",
    "\n",
    "dtime_pre1hr = pat_info.copy(deep=True)\n",
    "dtime_pre1hr = dtime_pre1hr[['patientid', 'd_time', '1hr before d_time']]\n",
    "\n",
    "print(dtime_pre1hr.shape)\n",
    "dtime_pre1hr\n",
    "\n",
    "##33,905 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_avg_params['timepoint'] =  pd.to_datetime(comp_avg_params['timepoint'] , format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "comp_avg_params.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter for only instances with readings in the last hour before discharge/death\n",
    "\n",
    "params_1hr = pd.merge(comp_avg_params, dtime_pre1hr, how='left', left_on=['patientid'], right_on = ['patientid'])\n",
    "params_1hr = params_1hr[(params_1hr['timepoint']<params_1hr['d_time']) & (params_1hr['timepoint']>params_1hr['1hr before d_time'])]\n",
    "\n",
    "params_1hr['Hrs before d_time'] = 1\n",
    "\n",
    "params_1hr.to_csv(\"params1hr.csv\")\n",
    "\n",
    "print(params_1hr.shape)\n",
    "params_1hr\n",
    "\n",
    "##3,117 records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2hrs before discharge/death time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2hrs before d_time\n",
    "\n",
    "dtime_pre2hrs = pat_info.copy(deep=True)\n",
    "dtime_pre2hrs = dtime_pre2hrs[['patientid', 'd_time', '1hr before d_time' ,'2hrs before d_time']]\n",
    "\n",
    "print(dtime_pre2hrs.shape)\n",
    "dtime_pre2hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter for only instances with readings within the 2nd hour before discharge/death\n",
    "\n",
    "params_2hrs = pd.merge(comp_avg_params, dtime_pre2hrs, how='left', left_on=['patientid'], right_on = ['patientid'])\n",
    "params_2hrs = params_2hrs[(params_2hrs['timepoint']<params_2hrs['1hr before d_time']) & (params_2hrs['timepoint']>params_2hrs['2hrs before d_time'])]\n",
    "\n",
    "params_2hrs['Hrs before d_time'] = 2\n",
    "\n",
    "params_2hrs.to_csv(\"params2hrs.csv\")\n",
    "\n",
    "print(params_2hrs.shape)\n",
    "params_2hrs\n",
    "\n",
    "##5,337 records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3hrs before discharge/death time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3hrs before d_time\n",
    "\n",
    "dtime_pre3hrs = pat_info.copy(deep=True)\n",
    "dtime_pre3hrs = dtime_pre3hrs[['patientid', 'd_time', '2hrs before d_time', '3hrs before d_time']]\n",
    "\n",
    "print(dtime_pre3hrs.shape)\n",
    "dtime_pre3hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter for only instances with readings within the 3rd hour before discharge/death\n",
    "\n",
    "params_3hrs = pd.merge(comp_avg_params, dtime_pre3hrs, how='left', left_on=['patientid'], right_on = ['patientid'])\n",
    "params_3hrs = params_3hrs[(params_3hrs['timepoint']<params_3hrs['2hrs before d_time']) & (params_3hrs['timepoint']>params_3hrs['3hrs before d_time'])]\n",
    "\n",
    "params_3hrs['Hrs before d_time'] = 3\n",
    "\n",
    "params_3hrs.to_csv(\"params3hrs.csv\")\n",
    "\n",
    "print(params_3hrs.shape)\n",
    "params_3hrs\n",
    "\n",
    "##6,316 records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4hrs before discharge/death time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4hrs before d_time\n",
    "\n",
    "dtime_pre4hrs = pat_info.copy(deep=True)\n",
    "dtime_pre4hrs = dtime_pre4hrs[['patientid', 'd_time', '3hrs before d_time', '4hrs before d_time']]\n",
    "\n",
    "print(dtime_pre4hrs.shape)\n",
    "dtime_pre4hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter for only instances with readings within the 4th hour before discharge/death\n",
    "\n",
    "params_4hrs = pd.merge(comp_avg_params, dtime_pre4hrs, how='left', left_on=['patientid'], right_on = ['patientid'])\n",
    "params_4hrs = params_4hrs[(params_4hrs['timepoint']<params_4hrs['3hrs before d_time']) & (params_4hrs['timepoint']>params_4hrs['4hrs before d_time'])]\n",
    "\n",
    "params_4hrs['Hrs before d_time'] = 4\n",
    "\n",
    "params_4hrs.to_csv(\"params4hrs.csv\")\n",
    "\n",
    "print(params_4hrs.shape)\n",
    "params_4hrs\n",
    "\n",
    "##7,502 records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 hrs before discharge/death time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe - 5hrs before d_time\n",
    "\n",
    "dtime_pre5hrs = pat_info.copy(deep=True)\n",
    "dtime_pre5hrs = dtime_pre5hrs[['patientid', 'd_time', '4hrs before d_time', '5hrs before d_time']]\n",
    "\n",
    "print(dtime_pre5hrs.shape)\n",
    "dtime_pre5hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter for only instances with readings within the 5th hour before discharge/death\n",
    "\n",
    "params_5hrs = pd.merge(comp_avg_params, dtime_pre5hrs, how='left', left_on=['patientid'], right_on = ['patientid'])\n",
    "params_5hrs = params_5hrs[(params_5hrs['timepoint']<params_5hrs['4hrs before d_time']) & (params_5hrs['timepoint']>params_5hrs['5hrs before d_time'])]\n",
    "\n",
    "params_5hrs['Hrs before d_time'] = 5\n",
    "\n",
    "params_5hrs.to_csv(\"params5hrs.csv\")\n",
    "\n",
    "print(params_5hrs.shape)\n",
    "params_5hrs\n",
    "\n",
    "##9,185 records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 hrs before discharge/death time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6hrs before d_time\n",
    "\n",
    "dtime_pre6hrs = pat_info.copy(deep=True)\n",
    "dtime_pre6hrs = dtime_pre6hrs[['patientid', 'd_time', '5hrs before d_time', '6hrs before d_time']]\n",
    "\n",
    "print(dtime_pre6hrs.shape)\n",
    "dtime_pre6hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter for only instances with readings within the 6th hour before discharge/death\n",
    "\n",
    "params_6hrs = pd.merge(comp_avg_params, dtime_pre6hrs, how='left', left_on=['patientid'], right_on = ['patientid'])\n",
    "params_6hrs = params_6hrs[(params_6hrs['timepoint']<params_6hrs['5hrs before d_time']) & (params_6hrs['timepoint']>params_6hrs['6hrs before d_time'])]\n",
    "\n",
    "params_6hrs['Hrs before d_time'] = 6\n",
    "\n",
    "params_6hrs.to_csv(\"params6hrs.csv\")\n",
    "\n",
    "print(params_6hrs.shape)\n",
    "params_6hrs\n",
    "\n",
    "##11,564 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check which patient ids have records present in: params1hr_mod, params2hrs_mod, params3hrs_mod, params4hrs_mod AND params5hrs_mod\n",
    "pat1hr = set(params_1hr['patientid'])\n",
    "pat2hrs = set(params_2hrs['patientid'])\n",
    "pat3hrs = set(params_3hrs['patientid'])\n",
    "pat4hrs = set(params_4hrs['patientid'])\n",
    "pat5hrs = set(params_5hrs['patientid'])\n",
    "\n",
    "#check common patientids\n",
    "pat_comm = set(pat1hr) & set(pat2hrs) & set(pat3hrs) & set(pat4hrs) & set(pat5hrs)\n",
    "print(len(pat_comm))\n",
    "pat_comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params1hr_mod = params_1hr[params_1hr['patientid'].isin(pat_comm)]\n",
    "params1hr_mod = params1hr_mod.drop(['rowID', '1hr before d_time'],axis=1)\n",
    "\n",
    "params2hrs_mod = params_2hrs[params_2hrs['patientid'].isin(pat_comm)]\n",
    "params2hrs_mod = params2hrs_mod.drop(['rowID', '1hr before d_time', '2hrs before d_time'],axis=1)\n",
    "\n",
    "params3hrs_mod = params_3hrs[params_3hrs['patientid'].isin(pat_comm)]\n",
    "params3hrs_mod = params3hrs_mod.drop(['rowID', '2hrs before d_time', '3hrs before d_time'],axis=1)\n",
    "\n",
    "params4hrs_mod = params_4hrs[params_4hrs['patientid'].isin(pat_comm)]\n",
    "params4hrs_mod = params4hrs_mod.drop(['rowID', '3hrs before d_time', '4hrs before d_time'],axis=1)\n",
    "\n",
    "params5hrs_mod = params_5hrs[params_5hrs['patientid'].isin(pat_comm)]\n",
    "params5hrs_mod = params5hrs_mod.drop(['rowID', '4hrs before d_time', '5hrs before d_time'],axis=1)\n",
    "\n",
    "params6hrs_mod = params_6hrs[params_6hrs['patientid'].isin(pat_comm)]\n",
    "params6hrs_mod = params6hrs_mod.drop(['rowID', '5hrs before d_time', '6hrs before d_time'],axis=1)\n",
    "\n",
    "params7hrs_mod = params_7hrs[params_7hrs['patientid'].isin(pat_comm)]\n",
    "params7hrs_mod = params7hrs_mod.drop(['rowID', '6hrs before d_time', '7hrs before d_time'],axis=1)\n",
    "\n",
    "params8hrs_mod = params_8hrs[params_8hrs['patientid'].isin(pat_comm)]\n",
    "params8hrs_mod = params8hrs_mod.drop(['rowID', '7hrs before d_time', '8hrs before d_time'],axis=1)\n",
    "\n",
    "print(params3hrs_mod.shape)\n",
    "params3hrs_mod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate boave dataframes\n",
    "\n",
    "frames = [params1hr_mod, params2hrs_mod, params3hrs_mod, params4hrs_mod, \n",
    "          params5hrs_mod]\n",
    "\n",
    "params_full = pd.concat(frames, axis=0)\n",
    "\n",
    "params_full = params_full.sort_values(by=['patientid','timepoint'], ascending=[True,False])\n",
    "params_full = params_full.reset_index()\n",
    "params_full['rowID'] = params_full.index #create unique col\n",
    "\n",
    "params_full.to_csv('params_full.csv')\n",
    "\n",
    "print(params_full.shape)\n",
    "print('Number of patients:', params_full.patientid.nunique())\n",
    "params_full.head(30)\n",
    "\n",
    "#15,616 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by patientid and Hhrs before d_time\n",
    "\n",
    "adr_params = params_full.groupby(['patientid','Hrs before d_time'])['Adrenaline'].apply(list)\n",
    "adr_params = pd.DataFrame(data=adr_params)\n",
    "\n",
    "nor_params = params_full.groupby(['patientid','Hrs before d_time'])['Noradrenaline'].apply(list)\n",
    "nor_params = pd.DataFrame(data=nor_params)\n",
    "\n",
    "fio2_params = params_full.groupby(['patientid','Hrs before d_time'])['FiO2'].apply(list)\n",
    "fio2_params = pd.DataFrame(data=fio2_params)\n",
    "\n",
    "spo2_params = params_full.groupby(['patientid','Hrs before d_time'])['SpO2'].apply(list)\n",
    "spo2_params = pd.DataFrame(data=spo2_params)\n",
    "\n",
    "map_params = params_full.groupby(['patientid','Hrs before d_time'])['MAP'].apply(list)\n",
    "map_params = pd.DataFrame(data=map_params)\n",
    "\n",
    "hr_params = params_full.groupby(['patientid','Hrs before d_time'])['HR'].apply(list)\n",
    "hr_params = pd.DataFrame(data=hr_params)\n",
    "\n",
    "print(map_params.shape)\n",
    "map_params.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge above dataframes\n",
    "\n",
    "params_mod_temp= adr_params.merge(nor_params,on=['patientid','Hrs before d_time']).merge(fio2_params,on=['patientid','Hrs before d_time']).merge(spo2_params,on=['patientid','Hrs before d_time']).merge(map_params,on=['patientid','Hrs before d_time']).merge(hr_params,on=['patientid','Hrs before d_time'])\n",
    "params_mod_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "params_mod_temp['Avg_Adrenaline'] = params_mod_temp.Adrenaline.apply(lambda x: round(statistics.mean(x),3))\n",
    "params_mod_temp['Avg_Nordrenaline'] = params_mod_temp.Noradrenaline.apply(lambda x: round(statistics.mean(x),3))\n",
    "params_mod_temp['Avg_FiO2'] = params_mod_temp.FiO2.apply(lambda x: round(statistics.mean(x),3))\n",
    "params_mod_temp['Avg_SpO2'] = params_mod_temp.SpO2.apply(lambda x: round(statistics.mean(x),1))\n",
    "params_mod_temp['Avg_MAP'] = params_mod_temp.MAP.apply(lambda x: round(statistics.mean(x),1))\n",
    "params_mod_temp['Avg_HR'] = params_mod_temp.HR.apply(lambda x: round(statistics.mean(x),1))\n",
    "\n",
    "#Drop column of lists, keep avg parameter values\n",
    "cols = ['Adrenaline','Noradrenaline','FiO2','SpO2','MAP','HR']\n",
    "params_mod_temp = params_mod_temp.drop(cols,axis=1)\n",
    "\n",
    "#Rename avg_ columns\n",
    "params_mod_temp.rename({'Avg_Adrenaline': 'Adrenaline'}, axis=1, inplace=True)\n",
    "params_mod_temp.rename({'Avg_Nordrenaline': 'Nordrenaline'}, axis=1, inplace=True)\n",
    "params_mod_temp.rename({'Avg_FiO2': 'FiO2'}, axis=1, inplace=True)\n",
    "params_mod_temp.rename({'Avg_SpO2': 'SpO2'}, axis=1, inplace=True)\n",
    "params_mod_temp.rename({'Avg_MAP': 'MAP'}, axis=1, inplace=True)\n",
    "params_mod_temp.rename({'Avg_HR': 'HR'}, axis=1, inplace=True)\n",
    "params_mod_temp = params_mod_temp.reset_index()\n",
    "\n",
    "params_mod_temp.to_csv('params_mod_temp.csv')\n",
    "\n",
    "print(params_mod_temp.shape)\n",
    "params_mod_temp\n",
    "\n",
    "#5,495 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_list = set(params_mod_temp.patientid)\n",
    "\n",
    "#check if all ids in pat_list exist in pat table\n",
    "all_pat = set(pat.patientid)\n",
    "\n",
    "print(len(all_pat))\n",
    "print(len(pat_list))\n",
    "\n",
    "result =  all(elem in all_pat for elem in pat_list)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add pat discharge_status columns\n",
    "\n",
    "discharge_status = pat.copy(deep=True)\n",
    "cols = ['admissiontime','sex','age']\n",
    "discharge_status = discharge_status.drop(cols,axis=1)\n",
    "\n",
    "params_temp_cohort = pd.merge(params_mod_temp, discharge_status, how='left', on='patientid')\n",
    "\n",
    "#Remove all rows where discharge_status is unknown\n",
    "params_temp_cohort = params_temp_cohort[params_temp_cohort['discharge_status'].notna()]\n",
    "\n",
    "params_temp_cohort.to_csv('hirid_extval_temporal_cohort.csv')\n",
    "\n",
    "print(params_temp_cohort.shape)\n",
    "params_temp_cohort.head()\n",
    "\n",
    "#5,320 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params_temp_cohort.shape)\n",
    "print('Number of patients:', params_temp_cohort.patientid.nunique())\n",
    "params_temp_cohort.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_list = set(params_temp_cohort['patientid'])\n",
    "\n",
    "pat_rel = pat[pat['patientid'].isin(pat_list)]\n",
    "pat_rel.discharge_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_temp_cohort = pd.read_csv('hirid_extval_temporal_cohort.csv')\n",
    "params_temp_cohort = params_temp_cohort.drop('Unnamed: 0',axis=1)\n",
    "\n",
    "print(params_temp_cohort.shape)\n",
    "params_temp_cohort.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_temp_cohort['patientid'].nunique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
