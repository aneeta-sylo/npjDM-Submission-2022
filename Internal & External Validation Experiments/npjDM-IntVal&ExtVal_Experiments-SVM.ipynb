{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statistics\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import multilabel_confusion_matrix \n",
    "from sklearn.metrics import plot_confusion_matrix \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score \n",
    "from statsmodels.stats.inter_rater import fleiss_kappa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to HiRID database\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2 import Error\n",
    "\n",
    "#Connect to HiRID\n",
    "conn = psycopg2.connect(user=\"mimicuser\",\n",
    "                                  password=\"knowlabMIMIC\",\n",
    "                                  host=\"172.17.0.1\",\n",
    "                                  port=\"5433\",\n",
    "                                  database=\"HiRID\")\n",
    "\n",
    "#Cursor \n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Training Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: All annotated datasets were provided by the data controller (Prof. Malcolm Sim) as excel files. In this section, all datasets are imported in their raw format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define funtion to add numeric label columns to all 11 QEUH annotated datasets\n",
    "\n",
    "def num_labels(df):\n",
    "\n",
    "    #Add numeric multiclass Annotation column\n",
    "    df['Annotation_Num'] = 0\n",
    "    df.loc[df['Annotation'] == 'A', 'Annotation_Num'] = 0\n",
    "    df.loc[df['Annotation'] == 'B', 'Annotation_Num'] = 1\n",
    "    df.loc[df['Annotation'] == 'C', 'Annotation_Num'] = 2\n",
    "    df.loc[df['Annotation'] == 'D', 'Annotation_Num'] = 3\n",
    "    df.loc[df['Annotation'] == 'E', 'Annotation_Num'] = 4\n",
    "\n",
    "    #Create binary class column: A=0, B/C/D/E = 1\n",
    "    df['Ann_Bin_A'] = 0\n",
    "    df.loc[df['Annotation'] == 'A', 'Ann_Bin_A'] = 0\n",
    "    df.loc[df['Annotation'] == 'B', 'Ann_Bin_A'] = 1\n",
    "    df.loc[df['Annotation'] == 'C', 'Ann_Bin_A'] = 1\n",
    "    df.loc[df['Annotation'] == 'D', 'Ann_Bin_A'] = 1\n",
    "    df.loc[df['Annotation'] == 'E', 'Ann_Bin_A'] = 1\n",
    "\n",
    "    #Create binary class column: A/B = 0, C/D/E = 1\n",
    "    df['Ann_Bin_B'] = 0\n",
    "    df.loc[df['Annotation'] == 'A', 'Ann_Bin_B'] = 0\n",
    "    df.loc[df['Annotation'] == 'B', 'Ann_Bin_B'] = 0\n",
    "    df.loc[df['Annotation'] == 'C', 'Ann_Bin_B'] = 1\n",
    "    df.loc[df['Annotation'] == 'D', 'Ann_Bin_B'] = 1\n",
    "    df.loc[df['Annotation'] == 'E', 'Ann_Bin_B'] = 1\n",
    "\n",
    "    #Create binary class column: A/B/C = 0, D/E = 1\n",
    "    df['Ann_Bin_C'] = 0\n",
    "    df.loc[df['Annotation'] == 'A', 'Ann_Bin_C'] = 0\n",
    "    df.loc[df['Annotation'] == 'B', 'Ann_Bin_C'] = 0\n",
    "    df.loc[df['Annotation'] == 'C', 'Ann_Bin_C'] = 0\n",
    "    df.loc[df['Annotation'] == 'D', 'Ann_Bin_C'] = 1\n",
    "    df.loc[df['Annotation'] == 'E', 'Ann_Bin_C'] = 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Consultant no.1 dataset\n",
    "\n",
    "c1 = pd.read_excel('./p01.xlsx').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c1 = c1.drop(columns = cols)\n",
    "c1 = c1.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "c1['Adrenaline'] = c1['Adrenaline'].replace(np.nan, 0)\n",
    "c1['Noradrenaline'] = c1['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "c1 = num_labels(c1)\n",
    "\n",
    "print(c1.shape)\n",
    "c1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Consultant no.2 dataset\n",
    "\n",
    "c2 = pd.read_csv('./p02.csv').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c2 = c2.drop(columns = cols)\n",
    "c2 = c2.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "c2['Adrenaline'] = c2['Adrenaline'].replace(np.nan, 0)\n",
    "c2['Noradrenaline'] = c2['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "c2 = num_labels(c2)\n",
    "\n",
    "print(c2.shape)\n",
    "c2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Consultant no.3 dataset\n",
    "\n",
    "c3 = pd.read_csv('./p03.csv').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c3 = c3.drop(columns = cols)\n",
    "c3 = c3.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "c3['Adrenaline'] = c3['Adrenaline'].replace(np.nan, 0)\n",
    "c3['Noradrenaline'] = c3['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "c3 = num_labels(c3)\n",
    "\n",
    "print(c3.shape)\n",
    "c3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Consultant no.4 dataset\n",
    "\n",
    "c4 = pd.read_excel('./p04.xlsx').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c4 = c4.drop(columns = cols)\n",
    "c4 = c4.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "c4['Adrenaline'] = c4['Adrenaline'].replace(np.nan, 0)\n",
    "c4['Noradrenaline'] = c4['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "c4 = num_labels(c4)\n",
    "\n",
    "print(c4.shape)\n",
    "c4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Consultant no.5 dataset\n",
    "\n",
    "c5 = pd.read_csv('./p05.csv').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c5 = c5.drop(columns = cols)\n",
    "c5 = c5.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "c5['Adrenaline'] = c5['Adrenaline'].replace(np.nan, 0)\n",
    "c5['Noradrenaline'] = c5['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "c5 = num_labels(c5)\n",
    "\n",
    "print(c5.shape)\n",
    "c5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Consultant no.6 dataset\n",
    "\n",
    "c6 = pd.read_excel('./p06.xlsx').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c6 = c6.drop(columns = cols)\n",
    "c6 = c6.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "c6['Adrenaline'] = c6['Adrenaline'].replace(np.nan, 0)\n",
    "c6['Noradrenaline'] = c6['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "c6 = num_labels(c6)\n",
    "\n",
    "print(c6.shape)\n",
    "c6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Consultant no.7 dataset\n",
    "\n",
    "c7 = pd.read_csv('./p07.csv').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c7 = c7.drop(columns = cols)\n",
    "c7 = c7.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "c7['Adrenaline'] = c7['Adrenaline'].replace(np.nan, 0)\n",
    "c7['Noradrenaline'] = c7['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "c7 = num_labels(c7)\n",
    "\n",
    "print(c7.shape)\n",
    "c7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Consultant no.8 dataset\n",
    "\n",
    "c8 = pd.read_csv('./p08.csv').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c8 = c8.drop(columns = cols)\n",
    "c8 = c8.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "c8['Adrenaline'] = c8['Adrenaline'].replace(np.nan, 0)\n",
    "c8['Noradrenaline'] = c8['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "c8 = num_labels(c8)\n",
    "\n",
    "print(c8.shape)\n",
    "c8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Consultant no.9 dataset\n",
    "\n",
    "c9 = pd.read_csv('./p09.csv').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c9 = c9.drop(columns = cols)\n",
    "c9 = c9.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "c9['Adrenaline'] = c9['Adrenaline'].replace(np.nan, 0)\n",
    "c9['Noradrenaline'] = c9['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "c9 = num_labels(c9)\n",
    "\n",
    "print(c9.shape)\n",
    "c9.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Consultant no.10 dataset\n",
    "\n",
    "c10 = pd.read_csv('./p10.csv').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c10 = c10.drop(columns = cols)\n",
    "c10 = c10.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "c10['Adrenaline'] = c10['Adrenaline'].replace(np.nan, 0)\n",
    "c10['Noradrenaline'] = c10['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "c10 = num_labels(c10)\n",
    "\n",
    "print(c10.shape)\n",
    "c10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Consultant no.11 dataset\n",
    "\n",
    "c11 = pd.read_excel('./p11.xlsx').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c11 = c11.drop(columns = cols)\n",
    "c11 = c11.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "c11['Adrenaline'] = c11['Adrenaline'].replace(np.nan, 0)\n",
    "c11['Noradrenaline'] = c11['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "c11['Annotation'] = c11['Annotation'].str.upper()\n",
    "\n",
    "c11 = num_labels(c11)\n",
    "\n",
    "print(c11.shape)\n",
    "c11.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Majority MV Consensus Dataset\n",
    "##See jupyter notebook 'npjDM-MV_Consensus_Dataset' for steps to create this Majority MV Consensus Dataset\n",
    "\n",
    "mv = pd.read_csv('MV-Consensus-Dataset.csv')\n",
    "\n",
    "mv = mv.drop('Unnamed: 0',axis=1)\n",
    "mv = mv.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "mv['Adrenaline'] = mv['Adrenaline'].replace(np.nan, 0)\n",
    "mv['Noradrenaline'] = mv['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "mv = num_labels(mv)\n",
    "\n",
    "print(mv.shape)\n",
    "mv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TMV\n",
    "##Create a TMV dataset by taking the majority-vote labels across only the expert annotated datasets which generate models that have high internal validation Performance (i.e., where internal F1 >= 0.7).\n",
    "##See jupyter notebook 'npjDM-IntVal-Top_Models' for steps to find top Performing models\n",
    "##Top Performaing models within internal validation: C2, C4, C8\n",
    "\n",
    "c2_ann = pd.read_csv('./p02.csv').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c2_ann = c2_ann.drop(columns = cols)\n",
    "c2_ann = c2_ann.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "c4_ann = pd.read_excel('./p04.xlsx').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c4_ann = c4_ann.drop(columns = cols)\n",
    "c4_ann = c4_ann.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "c8_ann = pd.read_csv('./p08.csv').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c8_ann = c8_ann.drop(columns = cols)\n",
    "c8_ann = c8_ann.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "cols = ['Adrenaline','Noradrenaline','FiO2','SpO2','MAP','HR']\n",
    "ann_top = c2_ann.merge(c4_ann,on=cols).merge(c8_ann,on=cols)\n",
    "\n",
    "ann_top.columns = ['Adrenaline','Noradrenaline','FiO2','SpO2','MAP','HR', 'c2_ann', 'c4_ann', 'c8_ann']\n",
    "\n",
    "colsb = ['Adrenaline', 'Noradrenaline','FiO2','SpO2','MAP','HR']\n",
    "ann_top.drop(colsb,axis=1,inplace=True)\n",
    "\n",
    "ann_top['Annotation']= ann_top.mode(axis=1)[0]\n",
    "colsc = ['c2_ann', 'c4_ann','c8_ann']\n",
    "ann_top.drop(colsc,axis=1,inplace=True)\n",
    "\n",
    "colsd = ['Adrenaline','Noradrenaline','FiO2','SpO2','MAP','HR']\n",
    "tmv = c2_ann.merge(c4_ann,on=colsd).merge(c8_ann,on=colsd)\n",
    "tmv.columns = ['Adrenaline','Noradrenaline','FiO2','SpO2','MAP','HR', 'c2_ann', 'c4_ann', 'c8_ann']\n",
    "\n",
    "tmv = pd.concat([tmv,ann_top],axis=1)\n",
    "colse = ['c2_ann', 'c4_ann','c8_ann']\n",
    "tmv.drop(colse,axis=1,inplace=True)\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "tmv['Adrenaline'] = tmv['Adrenaline'].replace(np.nan, 0)\n",
    "tmv['Noradrenaline'] = tmv['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "tmv = num_labels(tmv)\n",
    "\n",
    "print(tmv.shape)\n",
    "tmv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Internal Validation Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Parameter Grid for hyperparameter optimisation\n",
    "##Create a dictionary with all SVM parameter options \n",
    "\n",
    "parameters = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], \n",
    "              'gamma': ['scale', 'auto']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Function - svm Model Evaluation via 5-fold CV\n",
    "\n",
    "def do_cv_learning_svm(X, y, verbose=False, do_scale=False, random_state=1):\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    f1s = []\n",
    "\n",
    "    if do_scale:\n",
    "        sc = StandardScaler()\n",
    "        X = sc.fit_transform(X)\n",
    "        \n",
    "    for i, (train,test) in enumerate(cv.split(X,y)):\n",
    "        gcsv = GridSearchCV(svm.SVC(random_state=1), \n",
    "                            param_grid=parameters, \n",
    "                            cv=5, \n",
    "                            scoring='f1_micro')\n",
    "        grid_result = gcsv.fit(X[train],y[train])\n",
    "        best_params = grid_result.best_params_\n",
    "        if verbose:\n",
    "            print('fold', i,'best_params', best_params)\n",
    "        clf = grid_result.best_estimator_\n",
    "        f1 = metrics.f1_score(y[test], clf.predict(X[test]), average='micro')\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    ##Performance metrics \n",
    "    dfsvm_multi_f1data = [['ann', 'multi', 'F1_micro', np.mean(f1s), np.std(f1s)]]\n",
    "\n",
    "    ##print data as DF\n",
    "    dfsvm_multi_f1data = pd.DataFrame(data=dfsvm_multi_f1data)\n",
    "    dfsvm_multi_f1data.columns = ['Annotator','Model','Optimisation','F1_micro','S.D.']\n",
    "    \n",
    "    return dfsvm_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Function - Find highest Performing model after 5-fold CV\n",
    "\n",
    "def model_opt_svm(X, y, verbose=False, do_scale=False, random_state=1):\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    f1s = []\n",
    "    models = []\n",
    "\n",
    "    if do_scale:\n",
    "        sc = StandardScaler()\n",
    "        X = sc.fit_transform(X)\n",
    "        \n",
    "    for i, (train,test) in enumerate(cv.split(X,y)):\n",
    "        gcsv = GridSearchCV(svm.SVC(random_state=1), \n",
    "                            param_grid=parameters, \n",
    "                            cv=5, \n",
    "                            scoring='f1_micro')\n",
    "        grid_result = gcsv.fit(X[train],y[train])\n",
    "        best_params = grid_result.best_params_\n",
    "        if verbose:\n",
    "            print('fold', i,'best_params', best_params)\n",
    "        clf = grid_result.best_estimator_\n",
    "        f1 = metrics.f1_score(y[test], clf.predict(X[test]), average='micro')\n",
    "        f1s.append(f1)\n",
    "        models.append(grid_result.best_estimator_)\n",
    "        \n",
    "    #find opt model\n",
    "    df_multi_opt = [f1s, models]\n",
    "    max_val = max(df_multi_opt[0])\n",
    "    max_index = df_multi_opt[0].index(max_val)\n",
    "    opt_model = df_multi_opt[1][max_index]\n",
    "    \n",
    "    return opt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C1 - IntVal\n",
    "\n",
    "array = c1.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c1svm_multi_f1data = do_cv_learning_svm(X,y)\n",
    "c1svm_multi_f1data['Annotator'] = 'C1'\n",
    "\n",
    "#Find Opt model\n",
    "c1svm_multi_opt = model_opt_svm(X,y)\n",
    "\n",
    "print(c1svm_multi_opt)\n",
    "c1svm_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C2 - IntVal\n",
    "\n",
    "array = c2.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c2svm_multi_f1data = do_cv_learning_svm(X,y)\n",
    "c2svm_multi_f1data['Annotator'] = 'C2'\n",
    "\n",
    "#Find Opt model\n",
    "c2svm_multi_opt = model_opt_svm(X,y)\n",
    "\n",
    "print(c2svm_multi_opt)\n",
    "c2svm_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C3 - IntVal\n",
    "\n",
    "array = c3.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c3svm_multi_f1data = do_cv_learning_svm(X,y)\n",
    "c3svm_multi_f1data['Annotator'] = 'C3'\n",
    "\n",
    "#Find Opt model\n",
    "c3svm_multi_opt = model_opt_svm(X,y)\n",
    "\n",
    "print(c3svm_multi_opt)\n",
    "c3svm_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C4 - IntVal\n",
    "\n",
    "array = c4.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c4svm_multi_f1data = do_cv_learning_svm(X,y)\n",
    "c4svm_multi_f1data['Annotator'] = 'C4'\n",
    "\n",
    "#Find Opt model\n",
    "c4svm_multi_opt = model_opt_svm(X,y)\n",
    "\n",
    "print(c4svm_multi_opt)\n",
    "c4svm_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C5 - IntVal\n",
    "\n",
    "array = c5.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c5svm_multi_f1data = do_cv_learning_svm(X,y)\n",
    "c5svm_multi_f1data['Annotator'] = 'C5'\n",
    "\n",
    "#Find Opt model\n",
    "c5svm_multi_opt = model_opt_svm(X,y)\n",
    "\n",
    "print(c5svm_multi_opt)\n",
    "c5svm_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C6 - IntVal\n",
    "\n",
    "array = c6.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c6svm_multi_f1data = do_cv_learning_svm(X,y)\n",
    "c6svm_multi_f1data['Annotator'] = 'C6'\n",
    "\n",
    "#Find Opt model\n",
    "c6svm_multi_opt = model_opt_svm(X,y)\n",
    "\n",
    "print(c6svm_multi_opt)\n",
    "c6svm_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C7 - IntVal\n",
    "\n",
    "array = c7.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c7svm_multi_f1data = do_cv_learning_svm(X,y)\n",
    "c7svm_multi_f1data['Annotator'] = 'C7'\n",
    "\n",
    "#Find Opt model\n",
    "c7svm_multi_opt = model_opt_svm(X,y)\n",
    "\n",
    "print(c7svm_multi_opt)\n",
    "c7svm_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C8 - IntVal\n",
    "\n",
    "array = c8.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c8svm_multi_f1data = do_cv_learning_svm(X,y)\n",
    "c8svm_multi_f1data['Annotator'] = 'C8'\n",
    "\n",
    "#Find Opt model\n",
    "c8svm_multi_opt = model_opt_svm(X,y)\n",
    "\n",
    "print(c8svm_multi_opt)\n",
    "c8svm_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C9 - IntVal\n",
    "\n",
    "array = c9.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c9svm_multi_f1data = do_cv_learning_svm(X,y)\n",
    "c9svm_multi_f1data['Annotator'] = 'C9'\n",
    "\n",
    "#Find Opt model\n",
    "c9svm_multi_opt = model_opt_svm(X,y)\n",
    "\n",
    "print(c9svm_multi_opt)\n",
    "c9svm_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C10 - IntVal\n",
    "\n",
    "array = c10.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c10svm_multi_f1data = do_cv_learning_svm(X,y)\n",
    "c10svm_multi_f1data['Annotator'] = 'C10'\n",
    "\n",
    "#Find Opt model\n",
    "c10svm_multi_opt = model_opt_svm(X,y)\n",
    "\n",
    "print(c10svm_multi_opt)\n",
    "c10svm_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C11 - IntVal\n",
    "\n",
    "array = c11.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c11svm_multi_f1data = do_cv_learning_svm(X,y)\n",
    "c11svm_multi_f1data['Annotator'] = 'C11'\n",
    "\n",
    "#Find Opt model\n",
    "c11svm_multi_opt = model_opt_svm(X,y)\n",
    "\n",
    "print(c11svm_multi_opt)\n",
    "c11svm_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MV - IntVal\n",
    "\n",
    "array = mv.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "mvsvm_multi_f1data = do_cv_learning_svm(X,y)\n",
    "mvsvm_multi_f1data['Annotator'] = 'MV'\n",
    "\n",
    "#Find Opt model\n",
    "mvsvm_multi_opt = model_opt_svm(X,y)\n",
    "\n",
    "print(mvsvm_multi_opt)\n",
    "mvsvm_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TMV - IntVal\n",
    "\n",
    "array = tmv.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "tmvsvm_multi_f1data = do_cv_learning_svm(X,y)\n",
    "tmvsvm_multi_f1data['Annotator'] = 'TMV'\n",
    "\n",
    "#Find Opt model\n",
    "tmvsvm_multi_opt = model_opt_svm(X,y)\n",
    "\n",
    "print(tmvsvm_multi_opt)\n",
    "tmvsvm_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Internal Validation Performances - Summary\n",
    "\n",
    "frames = [c1svm_multi_f1data, c2svm_multi_f1data, c3svm_multi_f1data, c4svm_multi_f1data, \n",
    "          c5svm_multi_f1data, c6svm_multi_f1data, c7svm_multi_f1data, c8svm_multi_f1data,\n",
    "          c9svm_multi_f1data, c10svm_multi_f1data, c11svm_multi_f1data, mvsvm_multi_f1data,\n",
    "          tmvsvm_multi_f1data]\n",
    "\n",
    "multi_int = pd.concat(frames)\n",
    "print(multi_int.shape)\n",
    "multi_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot chart - Internal Validation F1 (micro)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#Define x and y data\n",
    "x1 = multi_int['Annotator']\n",
    "y1 = multi_int['F1_micro']\n",
    "\n",
    "#Plot chart data\n",
    "plt.figure(figsize=(8,2.5))\n",
    "plt.plot(x1, y1, color='#1F57C8', marker='o', linestyle=\"solid\", label='Multi')\n",
    "\n",
    "plt.ylim([0.0,1.1])\n",
    "plt.yticks(np.arange(0.0,1.01, 0.2))\n",
    "\n",
    "#Add title and labels\n",
    "plt.title('Internal Validation: Multiclass - svm', fontsize=14)\n",
    "plt.xlabel('Annotator', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylabel('F1_micro', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. External Validation Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Define HiRID External Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import HiRID 'Patient' table (contains discharge_status info)\n",
    "\n",
    "pat = pd.read_sql_query(\"SELECT * FROM hirid.patient\", conn)\n",
    "\n",
    "pat.to_csv('patient_table.csv')\n",
    "\n",
    "print(pat.shape)\n",
    "pat.head()\n",
    "\n",
    "#33,905 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import HiRID Validation Dataset - data for patients 1hr before discharge/death\n",
    "##See jupyter notebook 'npjDM-HiRID_ExtVal_Dataset' to see steps on creating this HiRID External Validation Dataset\n",
    "\n",
    "params1hr = pd.read_csv(\"HiRID_extval_params1hr.csv\")\n",
    "params1hr.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "params1hr['binary_status'] = np.where(params1hr['discharge_status']== 'alive', 0, 4)\n",
    "\n",
    "print(params1hr.shape)\n",
    "params1hr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check dishcarge status classes are balanced\n",
    "\n",
    "params1hr.discharge_status.value_counts()\n",
    "params1hr.binary_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define hirid validation dataset\n",
    "\n",
    "array = params1hr.to_numpy()\n",
    "X_test = array[:,3:9]  \n",
    "y_test = array[:,12]  \n",
    "\n",
    "X_test = X_test.astype(float) \n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Run QEUH models on HiRID External Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C1 - HiRID Ext val \n",
    "\n",
    "f1 = metrics.f1_score(list(y_test), c1svm_multi_opt.predict(X_test), average='micro')\n",
    "c1svm_multi_ext  = [['C1', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "c1svm_multi_ext = pd.DataFrame(data=c1svm_multi_ext)\n",
    "c1svm_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c1svm_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C2 - HiRID Ext val \n",
    "\n",
    "f1 = metrics.f1_score(list(y_test), c2svm_multi_opt.predict(X_test), average='micro')\n",
    "c2svm_multi_ext  = [['C2', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "c2svm_multi_ext = pd.DataFrame(data=c2svm_multi_ext)\n",
    "c2svm_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c2svm_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C3 - HiRID Ext val \n",
    "\n",
    "f1 = metrics.f1_score(list(y_test), c3svm_multi_opt.predict(X_test), average='micro')\n",
    "c3svm_multi_ext  = [['C3', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c3svm_multi_ext = pd.DataFrame(data=c3svm_multi_ext)\n",
    "c3svm_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c3svm_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C4 - HiRID Ext val  \n",
    "\n",
    "f1 = metrics.f1_score(list(y_test), c4svm_multi_opt.predict(X_test), average='micro')\n",
    "c4svm_multi_ext  = [['C4', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "c4svm_multi_ext = pd.DataFrame(data=c4svm_multi_ext)\n",
    "c4svm_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c4svm_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C5 - HiRID Ext val  \n",
    "\n",
    "f1 = metrics.f1_score(list(y_test), c5svm_multi_opt.predict(X_test), average='micro')\n",
    "c5svm_multi_ext  = [['C5', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "c5svm_multi_ext = pd.DataFrame(data=c5svm_multi_ext)\n",
    "c5svm_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c5svm_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C6 - HiRID Ext val \n",
    "\n",
    "f1 = metrics.f1_score(list(y_test), c6svm_multi_opt.predict(X_test), average='micro')\n",
    "c6svm_multi_ext  = [['C6', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "c6svm_multi_ext = pd.DataFrame(data=c6svm_multi_ext)\n",
    "c6svm_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c6svm_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C7 - HiRID Ext val \n",
    "\n",
    "f1 = metrics.f1_score(list(y_test), c7svm_multi_opt.predict(X_test), average='micro')\n",
    "c7svm_multi_ext  = [['C7', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "c7svm_multi_ext = pd.DataFrame(data=c7svm_multi_ext)\n",
    "c7svm_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c7svm_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C8 - HiRID Ext val \n",
    "\n",
    "f1 = metrics.f1_score(list(y_test), c8svm_multi_opt.predict(X_test), average='micro')\n",
    "c8svm_multi_ext  = [['C8', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "c8svm_multi_ext = pd.DataFrame(data=c8svm_multi_ext)\n",
    "c8svm_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c8svm_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C9 - HiRID Ext val  \n",
    "\n",
    "f1 = metrics.f1_score(list(y_test), c9svm_multi_opt.predict(X_test), average='micro')\n",
    "c9svm_multi_ext  = [['C9', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "c9svm_multi_ext = pd.DataFrame(data=c9svm_multi_ext)\n",
    "c9svm_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c9svm_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C10 - HiRID Ext val \n",
    "\n",
    "f1 = metrics.f1_score(list(y_test), c10svm_multi_opt.predict(X_test), average='micro')\n",
    "c10svm_multi_ext  = [['C10', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "c10svm_multi_ext = pd.DataFrame(data=c10svm_multi_ext)\n",
    "c10svm_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c10svm_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C11- HiRID Ext val \n",
    "\n",
    "f1 = metrics.f1_score(list(y_test), c11svm_multi_opt.predict(X_test), average='micro')\n",
    "c11svm_multi_ext  = [['C11', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "c11svm_multi_ext = pd.DataFrame(data=c11svm_multi_ext)\n",
    "c11svm_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c11svm_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MV - HiRID Ext val \n",
    "\n",
    "f1 = metrics.f1_score(list(y_test), mvsvm_multi_opt.predict(X_test), average='micro')\n",
    "mvsvm_multi_ext  = [['MV', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "mvsvm_multi_ext = pd.DataFrame(data=mvsvm_multi_ext)\n",
    "mvsvm_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "mvsvm_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TMV - HiRID Ext val \n",
    "\n",
    "f1 = metrics.f1_score(list(y_test), tmvsvm_multi_opt.predict(X_test), average='micro')\n",
    "tmvsvm_multi_ext  = [['TMV', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "tmvsvm_multi_ext = pd.DataFrame(data=tmvsvm_multi_ext)\n",
    "tmvsvm_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "tmvsvm_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#External Validation - Summary\n",
    "\n",
    "frames = [c1svm_multi_ext, c2svm_multi_ext, c3svm_multi_ext, c4svm_multi_ext, \n",
    "          c5svm_multi_ext, c6svm_multi_ext, c7svm_multi_ext, c8svm_multi_ext,\n",
    "          c9svm_multi_ext, c10svm_multi_ext, c11svm_multi_ext, mvsvm_multi_ext,\n",
    "          tmvsvm_multi_ext]\n",
    "\n",
    "multi_ext = pd.concat(frames)\n",
    "\n",
    "print(multi_ext.shape)\n",
    "multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot chart - External Validation\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#Define x and y data\n",
    "x1 = multi_ext_ann['Annotator']\n",
    "y1 = multi_ext_ann['F1_micro']\n",
    "mv = multi_ext_mvs.iloc[0,3]\n",
    "tmv = multi_ext_mvs.iloc[1,3]\n",
    "\n",
    "#Plot chart data\n",
    "plt.figure(figsize=(8.5,4))\n",
    "plt.plot(x1, y1, color='#1F57C8', marker='o', linestyle=\"solid\")\n",
    "plt.ylim([0.0,0.61])\n",
    "plt.yticks(np.arange(0.0,0.61, 0.1))\n",
    "plt.axhline(y=mv, color='#DA4802', linestyle='-', label = 'Majority Vote (MV)')\n",
    "plt.axhline(y=tmv, color='#65C314', linestyle='-', label = 'Top Majority Vote (TMV)')\n",
    "\n",
    "#Add title and labels\n",
    "plt.title('Random Forest External Validation Performance', fontsize=14)\n",
    "plt.xlabel('Annotator', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylabel('F1 micro', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12, loc='upper center', bbox_to_anchor=(0.5, -0.4), fancybox=True, shadow=True, ncol=2)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
