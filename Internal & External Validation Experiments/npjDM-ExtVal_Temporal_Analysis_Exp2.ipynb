{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External Validation: Static vs Temporal (Experiment 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statistics\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import multilabel_confusion_matrix \n",
    "from sklearn.metrics import plot_confusion_matrix \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score \n",
    "from statsmodels.stats.inter_rater import fleiss_kappa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to HiRID database\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2 import Error\n",
    "\n",
    "#Connect to HiRID\n",
    "conn = psycopg2.connect(user=\"mimicuser\",\n",
    "                                  password=\"knowlabMIMIC\",\n",
    "                                  host=\"172.17.0.1\",\n",
    "                                  port=\"5433\",\n",
    "                                  database=\"HiRID\")\n",
    "\n",
    "#Cursor \n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.pandas.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define funtion to add numeric label columns to all 11 QEUH annotated datasets\n",
    "\n",
    "def num_labels(df):\n",
    "\n",
    "    #Add numeric multiclass Annotation column - pooling D and E labels\n",
    "    df['Annotation_Num'] = 0\n",
    "    df.loc[df['Annotation'] == 'A', 'Annotation_Num'] = 0\n",
    "    df.loc[df['Annotation'] == 'B', 'Annotation_Num'] = 1\n",
    "    df.loc[df['Annotation'] == 'C', 'Annotation_Num'] = 2\n",
    "    df.loc[df['Annotation'] == 'D', 'Annotation_Num'] = 3\n",
    "    df.loc[df['Annotation'] == 'E', 'Annotation_Num'] = 4\n",
    "\n",
    "    #Create binary class column: A=0, B/C/D/E = 1\n",
    "    df['Ann_Bin_A'] = 0\n",
    "    df.loc[df['Annotation'] == 'A', 'Ann_Bin_A'] = 0\n",
    "    df.loc[df['Annotation'] == 'B', 'Ann_Bin_A'] = 1\n",
    "    df.loc[df['Annotation'] == 'C', 'Ann_Bin_A'] = 1\n",
    "    df.loc[df['Annotation'] == 'D', 'Ann_Bin_A'] = 1\n",
    "    df.loc[df['Annotation'] == 'E', 'Ann_Bin_A'] = 1\n",
    "\n",
    "    #Create binary class column: A/B = 0, C/D/E = 1\n",
    "    df['Ann_Bin_B'] = 0\n",
    "    df.loc[df['Annotation'] == 'A', 'Ann_Bin_B'] = 0\n",
    "    df.loc[df['Annotation'] == 'B', 'Ann_Bin_B'] = 0\n",
    "    df.loc[df['Annotation'] == 'C', 'Ann_Bin_B'] = 1\n",
    "    df.loc[df['Annotation'] == 'D', 'Ann_Bin_B'] = 1\n",
    "    df.loc[df['Annotation'] == 'E', 'Ann_Bin_B'] = 1\n",
    "\n",
    "    #Create binary class column: A/B/C = 0, D/E = 1\n",
    "    df['Ann_Bin_C'] = 0\n",
    "    df.loc[df['Annotation'] == 'A', 'Ann_Bin_C'] = 0\n",
    "    df.loc[df['Annotation'] == 'B', 'Ann_Bin_C'] = 0\n",
    "    df.loc[df['Annotation'] == 'C', 'Ann_Bin_C'] = 0\n",
    "    df.loc[df['Annotation'] == 'D', 'Ann_Bin_C'] = 1\n",
    "    df.loc[df['Annotation'] == 'E', 'Ann_Bin_C'] = 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Consultant no.1 dataset\n",
    "\n",
    "c1 = pd.read_excel('./p01.xlsx').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c1 = c1.drop(columns = cols)\n",
    "c1 = c1.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "c1['Adrenaline'] = c1['Adrenaline'].replace(np.nan, 0)\n",
    "c1['Noradrenaline'] = c1['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "c1 = num_labels(c1)\n",
    "\n",
    "print(c1.shape)\n",
    "c1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Consultant no.2 dataset\n",
    "c2 = pd.read_csv('./p02.csv').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c2 = c2.drop(columns = cols)\n",
    "c2 = c2.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "c2['Adrenaline'] = c2['Adrenaline'].replace(np.nan, 0)\n",
    "c2['Noradrenaline'] = c2['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "c2 = num_labels(c2)\n",
    "\n",
    "print(c2.shape)\n",
    "c2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Consultant no.3 dataset\n",
    "\n",
    "c3 = pd.read_csv('./p03.csv').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c3 = c3.drop(columns = cols)\n",
    "c3 = c3.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "c3['Adrenaline'] = c3['Adrenaline'].replace(np.nan, 0)\n",
    "c3['Noradrenaline'] = c3['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "c3 = num_labels(c3)\n",
    "\n",
    "print(c3.shape)\n",
    "c3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Consultant no.4 dataset\n",
    "\n",
    "c4 = pd.read_excel('./p04.xlsx').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c4 = c4.drop(columns = cols)\n",
    "c4 = c4.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "c4['Adrenaline'] = c4['Adrenaline'].replace(np.nan, 0)\n",
    "c4['Noradrenaline'] = c4['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "c4 = num_labels(c4)\n",
    "\n",
    "print(c4.shape)\n",
    "c4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Consultant no.5 dataset\n",
    "\n",
    "c5 = pd.read_csv('./p05.csv').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c5 = c5.drop(columns = cols)\n",
    "c5 = c5.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "c5['Adrenaline'] = c5['Adrenaline'].replace(np.nan, 0)\n",
    "c5['Noradrenaline'] = c5['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "c5 = num_labels(c5)\n",
    "\n",
    "print(c5.shape)\n",
    "c5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Consultant no.6 dataset\n",
    "\n",
    "c6 = pd.read_excel('./p06.xlsx').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c6 = c6.drop(columns = cols)\n",
    "c6 = c6.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "c6['Adrenaline'] = c6['Adrenaline'].replace(np.nan, 0)\n",
    "c6['Noradrenaline'] = c6['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "c6 = num_labels(c6)\n",
    "\n",
    "print(c6.shape)\n",
    "c6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Consultant no.7 dataset\n",
    "\n",
    "c7 = pd.read_csv('./p07.csv').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c7 = c7.drop(columns = cols)\n",
    "c7 = c7.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "c7['Adrenaline'] = c7['Adrenaline'].replace(np.nan, 0)\n",
    "c7['Noradrenaline'] = c7['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "c7 = num_labels(c7)\n",
    "\n",
    "print(c7.shape)\n",
    "c7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Consultant no.8 dataset\n",
    "\n",
    "c8 = pd.read_csv('./p08.csv').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c8 = c8.drop(columns = cols)\n",
    "c8 = c8.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "c8['Adrenaline'] = c8['Adrenaline'].replace(np.nan, 0)\n",
    "c8['Noradrenaline'] = c8['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "c8 = num_labels(c8)\n",
    "\n",
    "print(c8.shape)\n",
    "c8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Consultant no.9 dataset\n",
    "\n",
    "c9 = pd.read_csv('./p09.csv').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c9 = c9.drop(columns = cols)\n",
    "c9 = c9.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "c9['Adrenaline'] = c9['Adrenaline'].replace(np.nan, 0)\n",
    "c9['Noradrenaline'] = c9['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "c9 = num_labels(c9)\n",
    "\n",
    "print(c9.shape)\n",
    "c9.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Consultant no.10 dataset\n",
    "\n",
    "c10 = pd.read_csv('./p10.csv').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c10 = c10.drop(columns = cols)\n",
    "c10 = c10.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "c10['Adrenaline'] = c10['Adrenaline'].replace(np.nan, 0)\n",
    "c10['Noradrenaline'] = c10['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "c10 = num_labels(c10)\n",
    "\n",
    "print(c10.shape)\n",
    "c10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Consultant no.11 dataset\n",
    "\n",
    "c11 = pd.read_excel('./p11.xlsx').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c11 = c11.drop(columns = cols)\n",
    "c11 = c11.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "c11['Adrenaline'] = c11['Adrenaline'].replace(np.nan, 0)\n",
    "c11['Noradrenaline'] = c11['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "c11['Annotation'] = c11['Annotation'].str.upper()\n",
    "\n",
    "c11 = num_labels(c11)\n",
    "\n",
    "print(c11.shape)\n",
    "c11.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Majority MV Consensus Dataset\n",
    "##See jupyter notebook 'npjDM-MV_Consensus_Dataset' for steps to create this Majority MV Consensus Dataset\n",
    "\n",
    "mv = pd.read_csv('MV-Consensus-Dataset.csv')\n",
    "mv.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "mv['Adrenaline'] = mv['Adrenaline'].replace(np.nan, 0)\n",
    "mv['Noradrenaline'] = mv['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "mv = num_labels(mv)\n",
    "\n",
    "print(mv.shape)\n",
    "mv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TMV\n",
    "##Create a TMV dataset by taking the majority-vote labels across only the expert annotated datasets which generate models that have high internal validation performance (i.e., where internal F1 >= 0.7).\n",
    "##Top performaing models within internal validation: C2, C4, C8\n",
    "\n",
    "c2_ann = pd.read_csv('./p02.csv').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c2_ann = c2_ann.drop(columns = cols)\n",
    "c2_ann = c2_ann.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "c4_ann = pd.read_excel('./p04.xlsx').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c4_ann = c4_ann.drop(columns = cols)\n",
    "c4_ann = c4_ann.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "c8_ann = pd.read_csv('./p09.csv').sort_values(by = ['PseudoID'], ascending=[True])\n",
    "cols = ['Dobutamine','Time','Bckgrnd','PseudoID','Line of Selected Timepoint']\n",
    "c8_ann = c8_ann.drop(columns = cols)\n",
    "c8_ann = c8_ann.rename(columns={'Mean': 'MAP'}) #rename Mean to MAP\n",
    "\n",
    "cols = ['Adrenaline','Noradrenaline','FiO2','SpO2','MAP','HR']\n",
    "ann_top = c2_ann.merge(c4_ann,on=cols).merge(c8_ann,on=cols)\n",
    "\n",
    "ann_top.columns = ['Adrenaline','Noradrenaline','FiO2','SpO2','MAP','HR', 'c2_ann', 'c4_ann', 'c8_ann']\n",
    "\n",
    "colsb = ['Adrenaline', 'Noradrenaline','FiO2','SpO2','MAP','HR']\n",
    "ann_top.drop(colsb,axis=1,inplace=True)\n",
    "\n",
    "ann_top['Annotation']= ann_top.mode(axis=1)[0]\n",
    "colsc = ['c2_ann', 'c4_ann','c8_ann']\n",
    "ann_top.drop(colsc,axis=1,inplace=True)\n",
    "\n",
    "colsd = ['Adrenaline','Noradrenaline','FiO2','SpO2','MAP','HR']\n",
    "tmv = c2_ann.merge(c4_ann,on=colsd).merge(c8_ann,on=colsd)\n",
    "tmv.columns = ['Adrenaline','Noradrenaline','FiO2','SpO2','MAP','HR', 'c2_ann', 'c4_ann', 'c8_ann']\n",
    "\n",
    "tmv = pd.concat([tmv,ann_top],axis=1)\n",
    "colse = ['c2_ann', 'c4_ann','c8_ann']\n",
    "tmv.drop(colse,axis=1,inplace=True)\n",
    "\n",
    "#Replace null with 0 in drug fields (as blank value indicates value=0, as confirmed by Prof Sim)\n",
    "tmv['Adrenaline'] = tmv['Adrenaline'].replace(np.nan, 0)\n",
    "tmv['Noradrenaline'] = tmv['Noradrenaline'].replace(np.nan, 0)\n",
    "\n",
    "tmv = num_labels(tmv)\n",
    "\n",
    "print(tmv.shape)\n",
    "tmv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Internal Validation (DT QEUH Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Parameter Grid for hyperparameter optimisation\n",
    "##Create a dictionary with all DT parameter options \n",
    "\n",
    "parameters = {'max_depth': [1,2,3,4,5,6,7,9,11,12, None], \n",
    "              'max_features': ['auto', 'sqrt','log2', None],\n",
    "              'criterion': ['gini','entropy']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Function - DT Model Evaluation via 5-fold CV\n",
    "\n",
    "def do_cv_learning_dt(X, y, verbose=False, do_scale=False, random_state=1):\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    f1s = []\n",
    "\n",
    "    if do_scale:\n",
    "        sc = StandardScaler()\n",
    "        X = sc.fit_transform(X)\n",
    "        \n",
    "    for i, (train,test) in enumerate(cv.split(X,y)):\n",
    "        gcsv = GridSearchCV(DecisionTreeClassifier(random_state=1), \n",
    "                            param_grid=parameters, \n",
    "                            cv=5, \n",
    "                            scoring='f1_micro')\n",
    "        grid_result = gcsv.fit(X[train],y[train])\n",
    "        best_params = grid_result.best_params_\n",
    "        if verbose:\n",
    "            print('fold', i,'best_params', best_params)\n",
    "        clf = grid_result.best_estimator_\n",
    "        f1 = metrics.f1_score(y[test], clf.predict(X[test]), average='micro')\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    ##Performance metrics \n",
    "    dfdt_multi_f1data = [['ann', 'multi', 'F1_micro', np.mean(f1s), np.std(f1s)]]\n",
    "\n",
    "    ##print data as DF\n",
    "    dfdt_multi_f1data = pd.DataFrame(data=dfdt_multi_f1data)\n",
    "    dfdt_multi_f1data.columns = ['Annotator','Model','Optimisation','F1_micro','S.D.']\n",
    "    \n",
    "    return dfdt_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Function - Find highest performing model after 5-fold CV\n",
    "\n",
    "def model_opt_dt(X, y, verbose=False, do_scale=False, random_state=1):\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    f1s = []\n",
    "    models = []\n",
    "\n",
    "    if do_scale:\n",
    "        sc = StandardScaler()\n",
    "        X = sc.fit_transform(X)\n",
    "        \n",
    "    for i, (train,test) in enumerate(cv.split(X,y)):\n",
    "        gcsv = GridSearchCV(DecisionTreeClassifier(random_state=1), \n",
    "                            param_grid=parameters, \n",
    "                            cv=5, \n",
    "                            scoring='f1_micro')\n",
    "        grid_result = gcsv.fit(X[train],y[train])\n",
    "        best_params = grid_result.best_params_\n",
    "        if verbose:\n",
    "            print('fold', i,'best_params', best_params)\n",
    "        clf = grid_result.best_estimator_\n",
    "        f1 = metrics.f1_score(y[test], clf.predict(X[test]), average='micro')\n",
    "        f1s.append(f1)\n",
    "        models.append(grid_result.best_estimator_)\n",
    "        \n",
    "    #find opt model\n",
    "    df_multi_opt = [f1s, models]\n",
    "    max_val = max(df_multi_opt[0])\n",
    "    max_index = df_multi_opt[0].index(max_val)\n",
    "    opt_model = df_multi_opt[1][max_index]\n",
    "    \n",
    "    return opt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C1 - IntVal\n",
    "\n",
    "array = c1.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c1dt_multi_f1data = do_cv_learning_dt(X,y)\n",
    "c1dt_multi_f1data['Annotator'] = 'c1'\n",
    "\n",
    "#Opt model\n",
    "c1dt_multi_opt = model_opt_dt(X,y)\n",
    "\n",
    "print(c1dt_multi_opt)\n",
    "c1dt_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C2 - IntVal\n",
    "\n",
    "array = c2.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "le.classes_\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c2dt_multi_f1data = do_cv_learning_dt(X,y)\n",
    "c2dt_multi_f1data['Annotator'] = 'c2'\n",
    "\n",
    "#Opt model\n",
    "c2dt_multi_opt = model_opt_dt(X,y)\n",
    "\n",
    "print(c2dt_multi_opt)\n",
    "c2dt_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C3 - IntVal\n",
    "\n",
    "array = c3.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "le.classes_\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c3dt_multi_f1data = do_cv_learning_dt(X,y)\n",
    "c3dt_multi_f1data['Annotator'] = 'c3'\n",
    "\n",
    "#Opt model\n",
    "c3dt_multi_opt = model_opt_dt(X,y)\n",
    "\n",
    "print(c3dt_multi_opt)\n",
    "c3dt_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C4 - IntVal\n",
    "\n",
    "array = c4.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "le.classes_\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c4dt_multi_f1data = do_cv_learning_dt(X,y)\n",
    "c4dt_multi_f1data['Annotator'] = 'c4'\n",
    "\n",
    "#Opt model\n",
    "c4dt_multi_opt = model_opt_dt(X,y)\n",
    "\n",
    "print(c4dt_multi_opt)\n",
    "c4dt_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C5 - IntVal\n",
    "\n",
    "array = c5.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "le.classes_\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c5dt_multi_f1data = do_cv_learning_dt(X,y)\n",
    "c5dt_multi_f1data['Annotator'] = 'c5'\n",
    "\n",
    "#Opt model\n",
    "c5dt_multi_opt = model_opt_dt(X,y)\n",
    "\n",
    "print(c5dt_multi_opt)\n",
    "c5dt_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C6 - IntVal\n",
    "\n",
    "array = c6.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "le.classes_\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c6dt_multi_f1data = do_cv_learning_dt(X,y)\n",
    "c6dt_multi_f1data['Annotator'] = 'c6'\n",
    "\n",
    "#Opt model\n",
    "c6dt_multi_opt = model_opt_dt(X,y)\n",
    "\n",
    "print(c6dt_multi_opt)\n",
    "c6dt_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C7 - IntVal\n",
    "\n",
    "array = c7.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "le.classes_\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c7dt_multi_f1data = do_cv_learning_dt(X,y)\n",
    "c7dt_multi_f1data['Annotator'] = 'c7'\n",
    "\n",
    "#Opt model\n",
    "c7dt_multi_opt = model_opt_dt(X,y)\n",
    "\n",
    "print(c7dt_multi_opt)\n",
    "c7dt_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C8 - IntVal\n",
    "\n",
    "array = c8.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "le.classes_\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c8dt_multi_f1data = do_cv_learning_dt(X,y)\n",
    "c8dt_multi_f1data['Annotator'] = 'c8'\n",
    "\n",
    "#Opt model\n",
    "c8dt_multi_opt = model_opt_dt(X,y)\n",
    "\n",
    "print(c8dt_multi_opt)\n",
    "c8dt_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C9 - IntVal\n",
    "\n",
    "array = c9.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "le.classes_\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c9dt_multi_f1data = do_cv_learning_dt(X,y)\n",
    "c9dt_multi_f1data['Annotator'] = 'c9'\n",
    "\n",
    "#Opt model\n",
    "c9dt_multi_opt = model_opt_dt(X,y)\n",
    "\n",
    "print(c9dt_multi_opt)\n",
    "c9dt_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c10 - IntVal\n",
    "\n",
    "array = c10.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "le.classes_\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c10dt_multi_f1data = do_cv_learning_dt(X,y)\n",
    "c10dt_multi_f1data['Annotator'] = 'c10'\n",
    "\n",
    "#Opt model\n",
    "c10dt_multi_opt = model_opt_dt(X,y)\n",
    "\n",
    "print(c10dt_multi_opt)\n",
    "c10dt_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c11 - IntVal\n",
    "\n",
    "array = c11.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "le.classes_\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c11dt_multi_f1data = do_cv_learning_dt(X,y)\n",
    "c11dt_multi_f1data['Annotator'] = 'c11'\n",
    "\n",
    "#Opt model\n",
    "c11dt_multi_opt = model_opt_dt(X,y)\n",
    "\n",
    "print(c11dt_multi_opt)\n",
    "c11dt_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MV - IntVal\n",
    "\n",
    "array = mv.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "le.classes_\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "mvdt_multi_f1data = do_cv_learning_dt(X,y)\n",
    "mvdt_multi_f1data['Annotator'] = 'MV'\n",
    "\n",
    "#Opt model\n",
    "mvdt_multi_opt = model_opt_dt(X,y)\n",
    "\n",
    "print(mvdt_multi_opt)\n",
    "mvdt_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TMV - IntVal\n",
    "\n",
    "array = tmv.to_numpy()\n",
    "X = array[:,0:6]  \n",
    "y = array[:,7]  \n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "le.classes_\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(le.classes_)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "tmvdt_multi_f1data = do_cv_learning_dt(X,y)\n",
    "tmvdt_multi_f1data['Annotator'] = 'TMV'\n",
    "\n",
    "#Opt model\n",
    "tmvdt_multi_opt = model_opt_dt(X,y)\n",
    "\n",
    "print(tmvdt_multi_opt)\n",
    "tmvdt_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Internal Validation Performance - Summary\n",
    "\n",
    "frames = [c1dt_multi_f1data, c2dt_multi_f1data, c3dt_multi_f1data, c4dt_multi_f1data, \n",
    "          c5dt_multi_f1data, c6dt_multi_f1data, c7dt_multi_f1data, c8dt_multi_f1data,\n",
    "          c9dt_multi_f1data, c10dt_multi_f1data, c11dt_multi_f1data, mvdt_multi_f1data,\n",
    "          tmvdt_multi_f1data]\n",
    "\n",
    "multi_int = pd.concat(frames)\n",
    "\n",
    "print(multi_int.shape)\n",
    "multi_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot chart - Internal Validation\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#Define x and y data\n",
    "x1 = multi_int['Annotator']\n",
    "y1 = multi_int['F1_micro']\n",
    "\n",
    "#Plot chart data\n",
    "plt.figure(figsize=(8,2.5))\n",
    "plt.plot(x1, y1, color='#1F57C8', marker='o', linestyle=\"solid\", label='Multi')\n",
    "\n",
    "plt.ylim([0.0,1.1])\n",
    "plt.yticks(np.arange(0.0,1.01, 0.2))\n",
    "\n",
    "#Add title and labels\n",
    "plt.title('Internal Validation: Multiclass - DT', fontsize=14)\n",
    "plt.xlabel('Annotator', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylabel('F1_micro', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. External Validation Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Static HiRID Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Static HiRID Validation Dataset, using Temporal HiRID Validation Dataset\n",
    "#See jupyter notebook 'npjDM-HiRID_ExtVal_Dataset' to see steps on creating the Temporal HiRID Validation Dataset\n",
    "\n",
    "hirid_stat = pd.read_csv(\"hirid_extval_temporal_cohort.csv\")\n",
    "hirid_stat = hirid_stat.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "#generate binary discharge_status\n",
    "hirid_stat['binary_discharge'] = np.where(hirid_stat['discharge_status']== 'alive', 0, 4)\n",
    "hirid_stat = hirid_stat.sort_values(by='patientid',ascending=True)\n",
    "\n",
    "#only keep records for 1hr before discharge/death\n",
    "hirid_stat = hirid_stat[hirid_stat['Hrs before d_time']==1]\n",
    "\n",
    "print(hirid_stat.shape)\n",
    "print('Number of patients:', hirid_stat.patientid.nunique())\n",
    "hirid_stat.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks\n",
    "print(hirid_stat.patientid.nunique())\n",
    "print(hirid_stat['Hrs before d_time'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define hirid (static) test dataset\n",
    "\n",
    "array = hirid_stat.to_numpy()\n",
    "X_test = array[:,2:8]  \n",
    "y_test = array[:,9]  \n",
    "\n",
    "X_test = X_test.astype(float) \n",
    "y_test = y_test.astype(int) \n",
    "\n",
    "len(X_test)\n",
    "len(y_test)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C1 - Static Ext val \n",
    "\n",
    "f1 = metrics.f1_score(y_test, c1dt_multi_opt.predict(X_test), average='micro')\n",
    "c1dt_multi_ext  = [['c1', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c1dt_multi_ext = pd.DataFrame(data=c1dt_multi_ext)\n",
    "c1dt_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c1dt_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C1 - plot confusion matrix\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "disp = plot_confusion_matrix(c1dt_multi_opt, X_test, list(y_test),\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             ax=ax)\n",
    "\n",
    "cnf_matrix = confusion_matrix(list(y_test), c1dt_multi_opt.predict(X_test))\n",
    "pred_labels = cnf_matrix.sum(axis=0)\n",
    "print(pred_labels)\n",
    "c1_A_pred = pred_labels[0]\n",
    "c1_B_pred = pred_labels[1]\n",
    "c1_C_pred = pred_labels[2]\n",
    "c1_D_pred = pred_labels[3]\n",
    "c1_E_pred = pred_labels[4]\n",
    "print(c1_A_pred,c1_B_pred,c1_C_pred,c1_D_pred,c1_E_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C2 - Static Ext val \n",
    "\n",
    "f1 = metrics.f1_score(y_test, c2dt_multi_opt.predict(X_test), average='micro')\n",
    "c2dt_multi_ext  = [['c2', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c2dt_multi_ext = pd.DataFrame(data=c2dt_multi_ext)\n",
    "c2dt_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c2dt_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(c2dt_multi_opt.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C2 - plot confusion matrix\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "disp = plot_confusion_matrix(c2dt_multi_opt, X_test, list(y_test),\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(list(y_test), c2dt_multi_opt.predict(X_test))\n",
    "\n",
    "pred_labels = cnf_matrix.sum(axis=0)\n",
    "print(pred_labels)\n",
    "\n",
    "c2_A_pred = pred_labels[0]\n",
    "c2_B_pred = pred_labels[1]\n",
    "c2_C_pred = 0\n",
    "c2_D_pred = pred_labels[2]\n",
    "c2_E_pred = pred_labels[3]\n",
    "print(c2_A_pred,c2_B_pred,c2_C_pred,c2_D_pred,c2_E_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C3 - Static Ext val \n",
    "\n",
    "f1 = metrics.f1_score(y_test, c3dt_multi_opt.predict(X_test), average='micro')\n",
    "c3dt_multi_ext  = [['c3', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c3dt_multi_ext = pd.DataFrame(data=c3dt_multi_ext)\n",
    "c3dt_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c3dt_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C3 - plot confusion matrix\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "disp = plot_confusion_matrix(c3dt_multi_opt, X_test, list(y_test),\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             ax=ax)\n",
    "\n",
    "cnf_matrix = confusion_matrix(list(y_test), c3dt_multi_opt.predict(X_test))\n",
    "pred_labels = cnf_matrix.sum(axis=0)\n",
    "print(pred_labels)\n",
    "c3_A_pred = pred_labels[0]\n",
    "c3_B_pred = pred_labels[1]\n",
    "c3_C_pred = pred_labels[2]\n",
    "c3_D_pred = pred_labels[3]\n",
    "c3_E_pred = pred_labels[4]\n",
    "print(c3_A_pred,c3_B_pred,c3_C_pred,c3_D_pred,c3_E_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C4 - Static Ext val \n",
    "\n",
    "f1 = metrics.f1_score(y_test, c4dt_multi_opt.predict(X_test), average='micro')\n",
    "c4dt_multi_ext  = [['c4', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c4dt_multi_ext = pd.DataFrame(data=c4dt_multi_ext)\n",
    "c4dt_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c4dt_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C4 - plot confusion matrix\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "disp = plot_confusion_matrix(c4dt_multi_opt, X_test, list(y_test),\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             ax=ax)\n",
    "\n",
    "cnf_matrix = confusion_matrix(list(y_test), c4dt_multi_opt.predict(X_test))\n",
    "pred_labels = cnf_matrix.sum(axis=0)\n",
    "print(pred_labels)\n",
    "c4_A_pred = pred_labels[0]\n",
    "c4_B_pred = pred_labels[1]\n",
    "c4_C_pred = pred_labels[2]\n",
    "c4_D_pred = pred_labels[3]\n",
    "c4_E_pred = pred_labels[4]\n",
    "print(c4_A_pred,c4_B_pred,c4_C_pred,c4_D_pred,c4_E_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C5 - Static Ext val \n",
    "\n",
    "f1 = metrics.f1_score(y_test, c5dt_multi_opt.predict(X_test), average='micro')\n",
    "c5dt_multi_ext  = [['c5', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c5dt_multi_ext = pd.DataFrame(data=c5dt_multi_ext)\n",
    "c5dt_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c5dt_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C5 - plot confusion matrix\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "disp = plot_confusion_matrix(c5dt_multi_opt, X_test, list(y_test),\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             ax=ax)\n",
    "\n",
    "cnf_matrix = confusion_matrix(list(y_test), c5dt_multi_opt.predict(X_test))\n",
    "pred_labels = cnf_matrix.sum(axis=0)\n",
    "print(pred_labels)\n",
    "c5_A_pred = pred_labels[0]\n",
    "c5_B_pred = pred_labels[1]\n",
    "c5_C_pred = pred_labels[2]\n",
    "c5_D_pred = pred_labels[3]\n",
    "c5_E_pred = pred_labels[4]\n",
    "print(c5_A_pred,c5_B_pred,c5_C_pred,c5_D_pred,c5_E_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C6 - Static Ext val \n",
    "\n",
    "f1 = metrics.f1_score(y_test, c6dt_multi_opt.predict(X_test), average='micro')\n",
    "c6dt_multi_ext  = [['c6', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c6dt_multi_ext = pd.DataFrame(data=c6dt_multi_ext)\n",
    "c6dt_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c6dt_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C6 - plot confusion matrix\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "disp = plot_confusion_matrix(c6dt_multi_opt, X_test, list(y_test),\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             ax=ax)\n",
    "\n",
    "cnf_matrix = confusion_matrix(list(y_test), c6dt_multi_opt.predict(X_test))\n",
    "pred_labels = cnf_matrix.sum(axis=0)\n",
    "print(pred_labels)\n",
    "c6_A_pred = pred_labels[0]\n",
    "c6_B_pred = pred_labels[1]\n",
    "c6_C_pred = pred_labels[2]\n",
    "c6_D_pred = pred_labels[3]\n",
    "c6_E_pred = pred_labels[4]\n",
    "print(c6_A_pred,c6_B_pred,c6_C_pred,c6_D_pred,c6_E_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C7 - Static Ext val \n",
    "\n",
    "f1 = metrics.f1_score(y_test, c7dt_multi_opt.predict(X_test), average='micro')\n",
    "c7dt_multi_ext  = [['c7', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c7dt_multi_ext = pd.DataFrame(data=c7dt_multi_ext)\n",
    "c7dt_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c7dt_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C7 - plot confusion matrix\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "disp = plot_confusion_matrix(c7dt_multi_opt, X_test, list(y_test),\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             ax=ax)\n",
    "\n",
    "cnf_matrix = confusion_matrix(list(y_test), c7dt_multi_opt.predict(X_test))\n",
    "pred_labels = cnf_matrix.sum(axis=0)\n",
    "print(pred_labels)\n",
    "c7_A_pred = pred_labels[0]\n",
    "c7_B_pred = pred_labels[1]\n",
    "c7_C_pred = pred_labels[2]\n",
    "c7_D_pred = pred_labels[3]\n",
    "c7_E_pred = pred_labels[4]\n",
    "print(c7_A_pred,c7_B_pred,c7_C_pred,c7_D_pred,c7_E_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C8 - Static Ext val \n",
    "\n",
    "f1 = metrics.f1_score(y_test, c8dt_multi_opt.predict(X_test), average='micro')\n",
    "c8dt_multi_ext  = [['c8', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c8dt_multi_ext = pd.DataFrame(data=c8dt_multi_ext)\n",
    "c8dt_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c8dt_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C8 - plot confusion matrix\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "disp = plot_confusion_matrix(c8dt_multi_opt, X_test, list(y_test),\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             ax=ax)\n",
    "\n",
    "cnf_matrix = confusion_matrix(list(y_test), c8dt_multi_opt.predict(X_test))\n",
    "pred_labels = cnf_matrix.sum(axis=0)\n",
    "print(pred_labels)\n",
    "c8_A_pred = pred_labels[0]\n",
    "c8_B_pred = pred_labels[1]\n",
    "c8_C_pred = pred_labels[2]\n",
    "c8_D_pred = pred_labels[3]\n",
    "c8_E_pred = pred_labels[4]\n",
    "print(c8_A_pred,c8_B_pred,c8_C_pred,c8_D_pred,c8_E_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C9 - Static Ext val \n",
    "\n",
    "f1 = metrics.f1_score(y_test, c9dt_multi_opt.predict(X_test), average='micro')\n",
    "c9dt_multi_ext  = [['c9', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c9dt_multi_ext = pd.DataFrame(data=c9dt_multi_ext)\n",
    "c9dt_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c9dt_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C9 - plot confusion matrix\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "disp = plot_confusion_matrix(c9dt_multi_opt, X_test, list(y_test),\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             ax=ax)\n",
    "\n",
    "cnf_matrix = confusion_matrix(list(y_test), c9dt_multi_opt.predict(X_test))\n",
    "pred_labels = cnf_matrix.sum(axis=0)\n",
    "print(pred_labels)\n",
    "c9_A_pred = pred_labels[0]\n",
    "c9_B_pred = pred_labels[1]\n",
    "c9_C_pred = pred_labels[2]\n",
    "c9_D_pred = pred_labels[3]\n",
    "c9_E_pred = pred_labels[4]\n",
    "print(c9_A_pred,c9_B_pred,c9_C_pred,c9_D_pred,c9_E_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C10 -Static Ext val \n",
    "\n",
    "f1 = metrics.f1_score(y_test, c10dt_multi_opt.predict(X_test), average='micro')\n",
    "c10dt_multi_ext  = [['c10', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c10dt_multi_ext = pd.DataFrame(data=c10dt_multi_ext)\n",
    "c10dt_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c10dt_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C10 - plot confusion matrix\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "disp = plot_confusion_matrix(c10dt_multi_opt, X_test, list(y_test),\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             ax=ax)\n",
    "\n",
    "cnf_matrix = confusion_matrix(list(y_test), c10dt_multi_opt.predict(X_test))\n",
    "pred_labels = cnf_matrix.sum(axis=0)\n",
    "print(pred_labels)\n",
    "c10_A_pred = pred_labels[0]\n",
    "c10_B_pred = pred_labels[1]\n",
    "c10_C_pred = pred_labels[2]\n",
    "c10_D_pred = pred_labels[3]\n",
    "c10_E_pred = pred_labels[4]\n",
    "print(c10_A_pred,c10_B_pred,c10_C_pred,c10_D_pred,c10_E_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c11 - Static Ext val \n",
    "\n",
    "f1 = metrics.f1_score(y_test, c11dt_multi_opt.predict(X_test), average='micro')\n",
    "c11dt_multi_ext  = [['c11', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c11dt_multi_ext = pd.DataFrame(data=c11dt_multi_ext)\n",
    "c11dt_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c11dt_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C11 - plot confusion matrix\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "disp = plot_confusion_matrix(c11dt_multi_opt, X_test, list(y_test),\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             ax=ax)\n",
    "\n",
    "cnf_matrix = confusion_matrix(list(y_test), c11dt_multi_opt.predict(X_test))\n",
    "pred_labels = cnf_matrix.sum(axis=0)\n",
    "print(pred_labels)\n",
    "c11_A_pred = pred_labels[0]\n",
    "c11_B_pred = pred_labels[1]\n",
    "c11_C_pred = pred_labels[2]\n",
    "c11_D_pred = pred_labels[3]\n",
    "c11_E_pred = pred_labels[4]\n",
    "print(c11_A_pred,c11_B_pred,c11_C_pred,c11_D_pred,c11_E_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MV - Static Ext val \n",
    "\n",
    "f1 = metrics.f1_score(y_test, mvdt_multi_opt.predict(X_test), average='micro')\n",
    "mvdt_multi_ext  = [['MV', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "mvdt_multi_ext = pd.DataFrame(data=mvdt_multi_ext)\n",
    "mvdt_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "mvdt_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MV - plot confusion matrix\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "disp = plot_confusion_matrix(mvdt_multi_opt, X_test, list(y_test),\n",
    "                             cmap=plt.cm.Blues,\n",
    "                             ax=ax)\n",
    "\n",
    "cnf_matrix = confusion_matrix(list(y_test), mvdt_multi_opt.predict(X_test))\n",
    "pred_labels = cnf_matrix.sum(axis=0)\n",
    "print(pred_labels)\n",
    "mv_A_pred = pred_labels[0]\n",
    "mv_B_pred = pred_labels[1]\n",
    "mv_C_pred = pred_labels[2]\n",
    "mv_D_pred = pred_labels[3]\n",
    "mv_E_pred = pred_labels[4]\n",
    "print(mv_A_pred,mv_B_pred,mv_C_pred,mv_D_pred,mv_E_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TMV - Static Ext val \n",
    "\n",
    "f1 = metrics.f1_score(y_test, tmvdt_multi_opt.predict(X_test), average='micro')\n",
    "tmvdt_multi_ext  = [['TMV', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "tmvdt_multi_ext = pd.DataFrame(data=tmvdt_multi_ext)\n",
    "tmvdt_multi_ext.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "tmvdt_multi_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(tmvdt_multi_opt.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TMV - plot confusion matrix\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "disp = plot_confusion_matrix(tmvdt_multi_opt, X_test, list(y_test),cmap=plt.cm.Blues,ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(list(y_test), tmvdt_multi_opt.predict(X_test))\n",
    "\n",
    "pred_labels = cnf_matrix.sum(axis=0)\n",
    "print(pred_labels)\n",
    "\n",
    "tmv_A_pred = pred_labels[0]\n",
    "tmv_B_pred = pred_labels[1]\n",
    "tmv_C_pred = 0\n",
    "tmv_D_pred = pred_labels[2]\n",
    "tmv_E_pred = pred_labels[3]\n",
    "print(tmv_A_pred,tmv_B_pred,tmv_C_pred,tmv_D_pred,tmv_E_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#External Validation Performances (Static) - Summary\n",
    "\n",
    "frames = [c1dt_multi_ext, c2dt_multi_ext, c3dt_multi_ext, c4dt_multi_ext, \n",
    "          c5dt_multi_ext, c6dt_multi_ext, c7dt_multi_ext, c8dt_multi_ext,\n",
    "          c9dt_multi_ext, c10dt_multi_ext, c11dt_multi_ext, mvdt_multi_ext,\n",
    "          tmvdt_multi_ext]\n",
    "\n",
    "multi_ext_stat = pd.concat(frames)\n",
    "\n",
    "print(multi_ext_stat.shape)\n",
    "multi_ext_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Temporal HiRID Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Temporal HiRID Validation Dataset\n",
    "##See jupyter notebook 'npjDM-HiRID_ExtVal_Dataset' to see steps on creating the Temporal HiRID Validation Dataset\n",
    "\n",
    "hirid_val = pd.read_csv(\"hirid_extval_temporal_cohort.csv\")\n",
    "hirid_val = hirid_val.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "#generate binary discharge_status\n",
    "hirid_val['binary_discharge'] = np.where(hirid_val['discharge_status']== 'alive', 0, 1)\n",
    "hirid_val = hirid_val.sort_values(by='patientid',ascending=True)\n",
    "\n",
    "print(hirid_val.shape)\n",
    "print('Number of patients:', hirid_val.patientid.nunique())\n",
    "hirid_val.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import HiRID patient table (contains discharge status info)\n",
    "\n",
    "pat = pd.read_sql_query(\"SELECT * FROM hirid.patient\", conn)\n",
    "\n",
    "print(pat.shape)\n",
    "pat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_list = set(hirid_val['patientid'])\n",
    "\n",
    "pat_rel = pat[pat['patientid'].isin(pat_list)]\n",
    "pat_rel.discharge_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hirid_val.discharge_status.value_counts())\n",
    "print(hirid_val.binary_discharge.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define HiRID Temporal vlidation data - X_test\n",
    "\n",
    "array = hirid_val.to_numpy()\n",
    "X_test = array[:,2:8]   \n",
    "\n",
    "X_test = X_test.astype(float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hirid_val['c1_pred'] = c1dt_multi_opt.predict(X_test)\n",
    "hirid_val['c2_pred'] = c2dt_multi_opt.predict(X_test)\n",
    "hirid_val['c3_pred'] = c3dt_multi_opt.predict(X_test)\n",
    "hirid_val['c4_pred'] = c4dt_multi_opt.predict(X_test)\n",
    "hirid_val['c5_pred'] = c5dt_multi_opt.predict(X_test)\n",
    "hirid_val['c6_pred'] = c6dt_multi_opt.predict(X_test)\n",
    "hirid_val['c7_pred'] = c7dt_multi_opt.predict(X_test)\n",
    "hirid_val['c8_pred'] = c8dt_multi_opt.predict(X_test)\n",
    "hirid_val['c9_pred'] = c9dt_multi_opt.predict(X_test)\n",
    "hirid_val['c10_pred'] = c10dt_multi_opt.predict(X_test)\n",
    "hirid_val['c11_pred'] = c11dt_multi_opt.predict(X_test)\n",
    "hirid_val['mv_pred'] = mvdt_multi_opt.predict(X_test)\n",
    "hirid_val['tmv_pred'] = tmvdt_multi_opt.predict(X_test)\n",
    "\n",
    "print(hirid_val.shape)\n",
    "hirid_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hirid_val.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Approach 1: Weighted Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ws = hirid_val.copy(deep=True)\n",
    "\n",
    "#define weights to each hour before event (discharge/death) - with higher bias towards hours nearer event\n",
    "val_ws['weight'] = 0\n",
    "val_ws.loc[val_ws['Hrs before d_time'] == 1, 'weight'] = 0.3\n",
    "val_ws.loc[val_ws['Hrs before d_time'] == 2, 'weight'] = 0.3\n",
    "val_ws.loc[val_ws['Hrs before d_time'] == 3, 'weight'] = 0.2\n",
    "val_ws.loc[val_ws['Hrs before d_time'] == 4, 'weight'] = 0.1\n",
    "val_ws.loc[val_ws['Hrs before d_time'] == 5, 'weight'] = 0.1\n",
    "\n",
    "#Renumber to get rid of 0\n",
    "cols = ['c1_pred','c2_pred', 'c3_pred', 'c4_pred', 'c5_pred', 'c6_pred', 'c7_pred','c8_pred', 'c9_pred', \n",
    "        'c10_pred', 'c11_pred', 'mv_pred', 'tmv_pred']\n",
    "\n",
    "#relabel 0-4 predicted labels to 1-5 (still representing A-E)\n",
    "val_ws[cols] = val_ws[cols].replace({0:1, 1:2, 2:3, 3:4, 4:5})\n",
    "\n",
    "#weighted x prediction\n",
    "val_ws['c1_ws_pred'] = val_ws['c1_pred']*val_ws['weight']\n",
    "val_ws['c2_ws_pred'] = val_ws['c2_pred']*val_ws['weight']\n",
    "val_ws['c3_ws_pred'] = val_ws['c3_pred']*val_ws['weight']\n",
    "val_ws['c4_ws_pred'] = val_ws['c4_pred']*val_ws['weight']\n",
    "val_ws['c5_ws_pred'] = val_ws['c5_pred']*val_ws['weight']\n",
    "val_ws['c6_ws_pred'] = val_ws['c6_pred']*val_ws['weight']\n",
    "val_ws['c7_ws_pred'] = val_ws['c7_pred']*val_ws['weight']\n",
    "val_ws['c8_ws_pred'] = val_ws['c8_pred']*val_ws['weight']\n",
    "val_ws['c9_ws_pred'] = val_ws['c9_pred']*val_ws['weight']\n",
    "val_ws['c10_ws_pred'] = val_ws['c10_pred']*val_ws['weight']\n",
    "val_ws['c11_ws_pred'] = val_ws['c11_pred']*val_ws['weight']\n",
    "val_ws['mv_ws_pred'] = val_ws['mv_pred']*val_ws['weight']\n",
    "val_ws['tmv_ws_pred'] = val_ws['tmv_pred']*val_ws['weight']\n",
    "\n",
    "#drop original pred columns\n",
    "val_ws = val_ws.drop(cols,axis=1)\n",
    "\n",
    "print(val_ws.shape)\n",
    "val_ws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate weighted sum per annotator predictions\n",
    "\n",
    "c1_ws = val_ws.groupby(by=['patientid'])['c1_ws_pred'].sum().reset_index()\n",
    "c2_ws = val_ws.groupby(by=['patientid'])['c2_ws_pred'].sum().reset_index()\n",
    "c3_ws = val_ws.groupby(by=['patientid'])['c3_ws_pred'].sum().reset_index()\n",
    "c4_ws = val_ws.groupby(by=['patientid'])['c4_ws_pred'].sum().reset_index()\n",
    "c5_ws = val_ws.groupby(by=['patientid'])['c5_ws_pred'].sum().reset_index()\n",
    "c6_ws = val_ws.groupby(by=['patientid'])['c6_ws_pred'].sum().reset_index()\n",
    "c7_ws = val_ws.groupby(by=['patientid'])['c7_ws_pred'].sum().reset_index()\n",
    "c8_ws = val_ws.groupby(by=['patientid'])['c8_ws_pred'].sum().reset_index()\n",
    "c9_ws = val_ws.groupby(by=['patientid'])['c9_ws_pred'].sum().reset_index()\n",
    "c10_ws = val_ws.groupby(by=['patientid'])['c10_ws_pred'].sum().reset_index()\n",
    "c11_ws = val_ws.groupby(by=['patientid'])['c11_ws_pred'].sum().reset_index()\n",
    "mv_ws = val_ws.groupby(by=['patientid'])['mv_ws_pred'].sum().reset_index()\n",
    "tmv_ws = val_ws.groupby(by=['patientid'])['tmv_ws_pred'].sum().reset_index()\n",
    "\n",
    "c7_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate weighted sum dfs\n",
    "\n",
    "ann_pred = c1_ws.merge(c2_ws,on=['patientid']).merge(c3_ws,on=['patientid']).merge(c4_ws,on=['patientid']).merge(c5_ws,on=['patientid']).merge(c6_ws,on=['patientid']).merge(c7_ws,on=['patientid']).merge(c8_ws,on=['patientid']).merge(c9_ws,on=['patientid']).merge(c10_ws,on=['patientid']).merge(c11_ws,on=['patientid']).merge(mv_ws,on=['patientid']).merge(tmv_ws,on=['patientid'])\n",
    "ann_pred = ann_pred.sort_values(by='patientid',ascending=True)\n",
    "ann_pred\n",
    "#range of all weighted sum pred labels: 1-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check range of all columns\n",
    "print('c1 range:', round(ann_pred['c1_ws_pred'].min(),3), '-', ann_pred['c1_ws_pred'].max() ) \n",
    "print('c2 range:', round(ann_pred['c2_ws_pred'].min(),3), '-', ann_pred['c2_ws_pred'].max() ) \n",
    "print('c3 range:', round(ann_pred['c3_ws_pred'].min(),3), '-', round(ann_pred['c3_ws_pred'].max(),3) ) \n",
    "print('c4 range:', round(ann_pred['c4_ws_pred'].min(),3), '-', ann_pred['c4_ws_pred'].max() ) \n",
    "print('c5 range:', round(ann_pred['c5_ws_pred'].min(),3), '-', ann_pred['c5_ws_pred'].max() ) \n",
    "print('c6 range:', round(ann_pred['c6_ws_pred'].min(),3), '-', ann_pred['c6_ws_pred'].max() ) \n",
    "print('c7 range:', round(ann_pred['c7_ws_pred'].min(),3), '-', ann_pred['c7_ws_pred'].max() ) \n",
    "print('c8 range:', round(ann_pred['c8_ws_pred'].min(),3), '-', ann_pred['c8_ws_pred'].max() ) \n",
    "print('c9 range:', round(ann_pred['c9_ws_pred'].min(),3), '-', ann_pred['c9_ws_pred'].max() ) \n",
    "print('c10 range:', round(ann_pred['c10_ws_pred'].min(),3), '-', ann_pred['c10_ws_pred'].max() ) \n",
    "print('c11 range:', round(ann_pred['c11_ws_pred'].min(),3), '-', ann_pred['c11_ws_pred'].max() ) \n",
    "print('mv range:', round(ann_pred['mv_ws_pred'].min(),3), '-', ann_pred['mv_ws_pred'].max() ) \n",
    "print('tmv range:', round(ann_pred['tmv_ws_pred'].min(),3), '-', ann_pred['tmv_ws_pred'].max() ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function defining extreme(1) cut-off \n",
    "\n",
    "def ex1_cat(row, col):\n",
    "    if row[col] == 1 :\n",
    "        return 0\n",
    "    if row[col] > 4:\n",
    "        return 1\n",
    "    return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to weighted sum labels to binary labels using following rules: 1 = discharged alive, >4 = died\n",
    "\n",
    "ann_ex1_pred = ann_pred.copy(deep=True)\n",
    "\n",
    "ann_ex1_pred['c1_ex1_pred'] = ann_ex1_pred.apply(lambda row: ex1_cat(row, 'c1_ws_pred'), axis=1)\n",
    "ann_ex1_pred['c2_ex1_pred'] = ann_ex1_pred.apply(lambda row: ex1_cat(row, 'c2_ws_pred'), axis=1)\n",
    "ann_ex1_pred['c3_ex1_pred'] = ann_ex1_pred.apply(lambda row: ex1_cat(row, 'c3_ws_pred'), axis=1)\n",
    "ann_ex1_pred['c4_ex1_pred'] = ann_ex1_pred.apply(lambda row: ex1_cat(row, 'c4_ws_pred'), axis=1)\n",
    "ann_ex1_pred['c5_ex1_pred'] = ann_ex1_pred.apply(lambda row: ex1_cat(row, 'c5_ws_pred'), axis=1)\n",
    "ann_ex1_pred['c6_ex1_pred'] = ann_ex1_pred.apply(lambda row: ex1_cat(row, 'c6_ws_pred'), axis=1)\n",
    "ann_ex1_pred['c7_ex1_pred'] = ann_ex1_pred.apply(lambda row: ex1_cat(row, 'c7_ws_pred'), axis=1)\n",
    "ann_ex1_pred['c8_ex1_pred'] = ann_ex1_pred.apply(lambda row: ex1_cat(row, 'c8_ws_pred'), axis=1)\n",
    "ann_ex1_pred['c9_ex1_pred'] = ann_ex1_pred.apply(lambda row: ex1_cat(row, 'c9_ws_pred'), axis=1)\n",
    "ann_ex1_pred['c10_ex1_pred'] = ann_ex1_pred.apply(lambda row: ex1_cat(row, 'c10_ws_pred'), axis=1)\n",
    "ann_ex1_pred['c11_ex1_pred'] = ann_ex1_pred.apply(lambda row: ex1_cat(row, 'c11_ws_pred'), axis=1)\n",
    "ann_ex1_pred['mv_ex1_pred'] = ann_ex1_pred.apply(lambda row: ex1_cat(row, 'mv_ws_pred'), axis=1)\n",
    "ann_ex1_pred['tmv_ex1_pred'] = ann_ex1_pred.apply(lambda row: ex1_cat(row, 'tmv_ws_pred'), axis=1)\n",
    "\n",
    "\n",
    "#drop cols\n",
    "cols = ['c1_ws_pred', 'c2_ws_pred', 'c3_ws_pred','c4_ws_pred', 'c5_ws_pred', 'c6_ws_pred', 'c7_ws_pred',\n",
    "       'c8_ws_pred', 'c9_ws_pred', 'c10_ws_pred', 'c11_ws_pred','mv_ws_pred', 'tmv_ws_pred']\n",
    "\n",
    "ann_ex1_pred = ann_ex1_pred.drop(cols, axis=1)\n",
    "\n",
    "print(ann_ex1_pred.shape)\n",
    "ann_ex1_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function defining extreme(2) cut-off \n",
    "\n",
    "def ex2_cat(row, col):\n",
    "    if row[col] <= 2 :\n",
    "        return 0\n",
    "    if row[col] > 4:\n",
    "        return 1\n",
    "    return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to binary labels using following rules: <=2 = discharged alive, >4 = died\n",
    "\n",
    "ann_ex2_pred = ann_pred.copy(deep=True)\n",
    "\n",
    "ann_ex2_pred['c1_ex2_pred'] = ann_ex2_pred.apply(lambda row: ex2_cat(row, 'c1_ws_pred'), axis=1)\n",
    "ann_ex2_pred['c2_ex2_pred'] = ann_ex2_pred.apply(lambda row: ex2_cat(row, 'c2_ws_pred'), axis=1)\n",
    "ann_ex2_pred['c3_ex2_pred'] = ann_ex2_pred.apply(lambda row: ex2_cat(row, 'c3_ws_pred'), axis=1)\n",
    "ann_ex2_pred['c4_ex2_pred'] = ann_ex2_pred.apply(lambda row: ex2_cat(row, 'c4_ws_pred'), axis=1)\n",
    "ann_ex2_pred['c5_ex2_pred'] = ann_ex2_pred.apply(lambda row: ex2_cat(row, 'c5_ws_pred'), axis=1)\n",
    "ann_ex2_pred['c6_ex2_pred'] = ann_ex2_pred.apply(lambda row: ex2_cat(row, 'c6_ws_pred'), axis=1)\n",
    "ann_ex2_pred['c7_ex2_pred'] = ann_ex2_pred.apply(lambda row: ex2_cat(row, 'c7_ws_pred'), axis=1)\n",
    "ann_ex2_pred['c8_ex2_pred'] = ann_ex2_pred.apply(lambda row: ex2_cat(row, 'c8_ws_pred'), axis=1)\n",
    "ann_ex2_pred['c9_ex2_pred'] = ann_ex2_pred.apply(lambda row: ex2_cat(row, 'c9_ws_pred'), axis=1)\n",
    "ann_ex2_pred['c10_ex2_pred'] = ann_ex2_pred.apply(lambda row: ex2_cat(row, 'c10_ws_pred'), axis=1)\n",
    "ann_ex2_pred['c11_ex2_pred'] = ann_ex2_pred.apply(lambda row: ex2_cat(row, 'c11_ws_pred'), axis=1)\n",
    "ann_ex2_pred['mv_ex2_pred'] = ann_ex2_pred.apply(lambda row: ex2_cat(row, 'mv_ws_pred'), axis=1)\n",
    "ann_ex2_pred['tmv_ex2_pred'] = ann_ex2_pred.apply(lambda row: ex2_cat(row, 'tmv_ws_pred'), axis=1)\n",
    "\n",
    "\n",
    "#drop cols\n",
    "cols = ['c1_ws_pred', 'c2_ws_pred', 'c3_ws_pred','c4_ws_pred', 'c5_ws_pred', 'c6_ws_pred', 'c7_ws_pred',\n",
    "       'c8_ws_pred', 'c9_ws_pred', 'c10_ws_pred', 'c11_ws_pred','mv_ws_pred', 'tmv_ws_pred']\n",
    "\n",
    "ann_ex2_pred = ann_ex2_pred.drop(cols, axis=1)\n",
    "\n",
    "print(ann_ex2_pred.shape)\n",
    "ann_ex2_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function define neutral cut-off \n",
    "\n",
    "def neut_cat(row, col):\n",
    "    if row[col] <= 3 :\n",
    "        return 0\n",
    "    if row[col] >= 4:\n",
    "        return 1\n",
    "    return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to binary labels using following rules: <=3 = discharged alive, >=4 = died\n",
    "\n",
    "ann_neut_pred = ann_pred.copy(deep=True)\n",
    "\n",
    "ann_neut_pred['c1_neut_pred'] = ann_neut_pred.apply(lambda row: neut_cat(row, 'c1_ws_pred'), axis=1)\n",
    "ann_neut_pred['c2_neut_pred'] = ann_neut_pred.apply(lambda row: neut_cat(row, 'c2_ws_pred'), axis=1)\n",
    "ann_neut_pred['c3_neut_pred'] = ann_neut_pred.apply(lambda row: neut_cat(row, 'c3_ws_pred'), axis=1)\n",
    "ann_neut_pred['c4_neut_pred'] = ann_neut_pred.apply(lambda row: neut_cat(row, 'c4_ws_pred'), axis=1)\n",
    "ann_neut_pred['c5_neut_pred'] = ann_neut_pred.apply(lambda row: neut_cat(row, 'c5_ws_pred'), axis=1)\n",
    "ann_neut_pred['c6_neut_pred'] = ann_neut_pred.apply(lambda row: neut_cat(row, 'c6_ws_pred'), axis=1)\n",
    "ann_neut_pred['c7_neut_pred'] = ann_neut_pred.apply(lambda row: neut_cat(row, 'c7_ws_pred'), axis=1)\n",
    "ann_neut_pred['c8_neut_pred'] = ann_neut_pred.apply(lambda row: neut_cat(row, 'c8_ws_pred'), axis=1)\n",
    "ann_neut_pred['c9_neut_pred'] = ann_neut_pred.apply(lambda row: neut_cat(row, 'c9_ws_pred'), axis=1)\n",
    "ann_neut_pred['c10_neut_pred'] = ann_neut_pred.apply(lambda row: neut_cat(row, 'c10_ws_pred'), axis=1)\n",
    "ann_neut_pred['c11_neut_pred'] = ann_neut_pred.apply(lambda row: neut_cat(row, 'c11_ws_pred'), axis=1)\n",
    "ann_neut_pred['mv_neut_pred'] = ann_neut_pred.apply(lambda row: neut_cat(row, 'mv_ws_pred'), axis=1)\n",
    "ann_neut_pred['tmv_neut_pred'] = ann_neut_pred.apply(lambda row: neut_cat(row, 'tmv_ws_pred'), axis=1)\n",
    "\n",
    "#drop cols\n",
    "cols = ['c1_ws_pred', 'c2_ws_pred', 'c3_ws_pred','c4_ws_pred', 'c5_ws_pred', 'c6_ws_pred', 'c7_ws_pred',\n",
    "       'c8_ws_pred', 'c9_ws_pred', 'c10_ws_pred', 'c11_ws_pred','mv_ws_pred', 'tmv_ws_pred']\n",
    "\n",
    "ann_neut_pred = ann_neut_pred.drop(cols, axis=1)\n",
    "\n",
    "print(ann_neut_pred.shape)\n",
    "ann_neut_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define y_test - part1\n",
    "\n",
    "hirid_test = pd.read_csv(\"hirid_extval_temporal_cohort.csv\")\n",
    "hirid_test = hirid_test.drop(['Unnamed: 0','Hrs before d_time','Adrenaline','Nordrenaline','FiO2','SpO2','MAP','HR'],axis=1)\n",
    "\n",
    "hirid_test = hirid_test.drop_duplicates()\n",
    "hirid_test = hirid_test.sort_values(by='patientid',ascending=True)\n",
    "\n",
    "#binary discharge status\n",
    "hirid_test['binary_discharge'] = np.where(hirid_test['discharge_status']== 'alive', 0, 1)\n",
    "\n",
    "print(hirid_test.shape)\n",
    "print('Number of patients:', hirid_test.patientid.nunique())\n",
    "hirid_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define y_test - part2\n",
    "\n",
    "array = hirid_test.to_numpy()\n",
    "y_test = array[:,2]  \n",
    "y_test = y_test.astype(int) \n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_test)\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WS - Extreme (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cut-offs: 1 = discharged alive, >4 = died**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c1 - DT Temporal ex1t Val - WS ex1t\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex1_pred['c1_ex1_pred'].to_numpy(), average='micro')\n",
    "c1dt_ex1_ws  = [['c1', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c1dt_ex1_ws = pd.DataFrame(data=c1dt_ex1_ws)\n",
    "c1dt_ex1_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c1dt_ex1_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C1 - Ex1 TPs\n",
    "\n",
    "c1_ex1_pred = ann_ex1_pred[['patientid','c1_ex1_pred']]\n",
    "c1_ex1_pred = pd.merge(c1_ex1_pred, hirid_test, on='patientid')\n",
    "c1_ex1_pred = c1_ex1_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c1_ex1_TP_alive = len(c1_ex1_pred[(c1_ex1_pred['c1_ex1_pred']==0) & (c1_ex1_pred['binary_discharge']==0)])\n",
    "c1_ex1_TP_dead = len(c1_ex1_pred[(c1_ex1_pred['c1_ex1_pred']==1) & (c1_ex1_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c1_ex1_pred['c1_ex1_pred'].value_counts())\n",
    "print('C1 TP - Discharged Alive: ', c1_ex1_TP_alive)\n",
    "print('C1 TP - Discharged Dead: ', c1_ex1_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c2 - DT Temporal ex1t Val - WS ex1t\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex1_pred['c2_ex1_pred'].to_numpy(), average='micro')\n",
    "c2dt_ex1_ws  = [['c2', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c2dt_ex1_ws = pd.DataFrame(data=c2dt_ex1_ws)\n",
    "c2dt_ex1_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c2dt_ex1_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c2 - Ex1 TPs\n",
    "\n",
    "c2_ex1_pred = ann_ex1_pred[['patientid','c2_ex1_pred']]\n",
    "c2_ex1_pred = pd.merge(c2_ex1_pred, hirid_test, on='patientid')\n",
    "c2_ex1_pred = c2_ex1_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c2_ex1_TP_alive = len(c2_ex1_pred[(c2_ex1_pred['c2_ex1_pred']==0) & (c2_ex1_pred['binary_discharge']==0)])\n",
    "c2_ex1_TP_dead = len(c2_ex1_pred[(c2_ex1_pred['c2_ex1_pred']==1) & (c2_ex1_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c2_ex1_pred['c2_ex1_pred'].value_counts())\n",
    "print('c2 TP - Discharged Alive: ', c2_ex1_TP_alive)\n",
    "print('c2 TP - Discharged Dead: ', c2_ex1_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c3 - DT Temporal ex1t Val - WS ex1t\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex1_pred['c3_ex1_pred'].to_numpy(), average='micro')\n",
    "c3dt_ex1_ws  = [['c3', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c3dt_ex1_ws = pd.DataFrame(data=c3dt_ex1_ws)\n",
    "c3dt_ex1_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c3dt_ex1_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c3 - Ex1 TPs\n",
    "\n",
    "c3_ex1_pred = ann_ex1_pred[['patientid','c3_ex1_pred']]\n",
    "c3_ex1_pred = pd.merge(c3_ex1_pred, hirid_test, on='patientid')\n",
    "c3_ex1_pred = c3_ex1_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c3_ex1_TP_alive = len(c3_ex1_pred[(c3_ex1_pred['c3_ex1_pred']==0) & (c3_ex1_pred['binary_discharge']==0)])\n",
    "c3_ex1_TP_dead = len(c3_ex1_pred[(c3_ex1_pred['c3_ex1_pred']==1) & (c3_ex1_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c3_ex1_pred['c3_ex1_pred'].value_counts())\n",
    "print('c3 TP - Discharged Alive: ', c3_ex1_TP_alive)\n",
    "print('c3 TP - Discharged Dead: ', c3_ex1_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c4 - DT Temporal ex1t Val - WS ex1t\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex1_pred['c4_ex1_pred'].to_numpy(), average='micro')\n",
    "c4dt_ex1_ws  = [['c4', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c4dt_ex1_ws = pd.DataFrame(data=c4dt_ex1_ws)\n",
    "c4dt_ex1_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c4dt_ex1_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c4 - Ex1 TPs\n",
    "\n",
    "c4_ex1_pred = ann_ex1_pred[['patientid','c4_ex1_pred']]\n",
    "c4_ex1_pred = pd.merge(c4_ex1_pred, hirid_test, on='patientid')\n",
    "c4_ex1_pred = c4_ex1_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c4_ex1_TP_alive = len(c4_ex1_pred[(c4_ex1_pred['c4_ex1_pred']==0) & (c4_ex1_pred['binary_discharge']==0)])\n",
    "c4_ex1_TP_dead = len(c4_ex1_pred[(c4_ex1_pred['c4_ex1_pred']==1) & (c4_ex1_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c4_ex1_pred['c4_ex1_pred'].value_counts())\n",
    "print('c4 TP - Discharged Alive: ', c4_ex1_TP_alive)\n",
    "print('c4 TP - Discharged Dead: ', c4_ex1_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c5 - DT Temporal ex1t Val - WS ex1t\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex1_pred['c5_ex1_pred'].to_numpy(), average='micro')\n",
    "c5dt_ex1_ws  = [['c5', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c5dt_ex1_ws = pd.DataFrame(data=c5dt_ex1_ws)\n",
    "c5dt_ex1_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c5dt_ex1_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c5 - Ex1 TPs\n",
    "\n",
    "c5_ex1_pred = ann_ex1_pred[['patientid','c5_ex1_pred']]\n",
    "c5_ex1_pred = pd.merge(c5_ex1_pred, hirid_test, on='patientid')\n",
    "c5_ex1_pred = c5_ex1_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c5_ex1_TP_alive = len(c5_ex1_pred[(c5_ex1_pred['c5_ex1_pred']==0) & (c5_ex1_pred['binary_discharge']==0)])\n",
    "c5_ex1_TP_dead = len(c5_ex1_pred[(c5_ex1_pred['c5_ex1_pred']==1) & (c5_ex1_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c5_ex1_pred['c5_ex1_pred'].value_counts())\n",
    "print('c5 TP - Discharged Alive: ', c5_ex1_TP_alive)\n",
    "print('c5 TP - Discharged Dead: ', c5_ex1_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c6 - DT Temporal ex1t Val - WS ex1t\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex1_pred['c6_ex1_pred'].to_numpy(), average='micro')\n",
    "c6dt_ex1_ws  = [['c6', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c6dt_ex1_ws = pd.DataFrame(data=c6dt_ex1_ws)\n",
    "c6dt_ex1_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c6dt_ex1_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c6 - Ex1 TPs\n",
    "\n",
    "c6_ex1_pred = ann_ex1_pred[['patientid','c6_ex1_pred']]\n",
    "c6_ex1_pred = pd.merge(c6_ex1_pred, hirid_test, on='patientid')\n",
    "c6_ex1_pred = c6_ex1_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c6_ex1_TP_alive = len(c6_ex1_pred[(c6_ex1_pred['c6_ex1_pred']==0) & (c6_ex1_pred['binary_discharge']==0)])\n",
    "c6_ex1_TP_dead = len(c6_ex1_pred[(c6_ex1_pred['c6_ex1_pred']==1) & (c6_ex1_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c6_ex1_pred['c6_ex1_pred'].value_counts())\n",
    "print('c6 TP - Discharged Alive: ', c6_ex1_TP_alive)\n",
    "print('c6 TP - Discharged Dead: ', c6_ex1_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c7 - DT Temporal ex1t Val - WS ex1t\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex1_pred['c7_ex1_pred'].to_numpy(), average='micro')\n",
    "c7dt_ex1_ws  = [['c7', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c7dt_ex1_ws = pd.DataFrame(data=c7dt_ex1_ws)\n",
    "c7dt_ex1_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c7dt_ex1_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c7 - Ex1 TPs\n",
    "\n",
    "c7_ex1_pred = ann_ex1_pred[['patientid','c7_ex1_pred']]\n",
    "c7_ex1_pred = pd.merge(c7_ex1_pred, hirid_test, on='patientid')\n",
    "c7_ex1_pred = c7_ex1_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c7_ex1_TP_alive = len(c7_ex1_pred[(c7_ex1_pred['c7_ex1_pred']==0) & (c7_ex1_pred['binary_discharge']==0)])\n",
    "c7_ex1_TP_dead = len(c7_ex1_pred[(c7_ex1_pred['c7_ex1_pred']==1) & (c7_ex1_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c7_ex1_pred['c7_ex1_pred'].value_counts())\n",
    "print('c7 TP - Discharged Alive: ', c7_ex1_TP_alive)\n",
    "print('c7 TP - Discharged Dead: ', c7_ex1_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c8 - DT Temporal ex1t Val - WS ex1t\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex1_pred['c8_ex1_pred'].to_numpy(), average='micro')\n",
    "c8dt_ex1_ws  = [['c8', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c8dt_ex1_ws = pd.DataFrame(data=c8dt_ex1_ws)\n",
    "c8dt_ex1_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c8dt_ex1_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c8 - Ex1 TPs\n",
    "\n",
    "c8_ex1_pred = ann_ex1_pred[['patientid','c8_ex1_pred']]\n",
    "c8_ex1_pred = pd.merge(c8_ex1_pred, hirid_test, on='patientid')\n",
    "c8_ex1_pred = c8_ex1_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c8_ex1_TP_alive = len(c8_ex1_pred[(c8_ex1_pred['c8_ex1_pred']==0) & (c8_ex1_pred['binary_discharge']==0)])\n",
    "c8_ex1_TP_dead = len(c8_ex1_pred[(c8_ex1_pred['c8_ex1_pred']==1) & (c8_ex1_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c8_ex1_pred['c8_ex1_pred'].value_counts())\n",
    "print('c8 TP - Discharged Alive: ', c8_ex1_TP_alive)\n",
    "print('c8 TP - Discharged Dead: ', c8_ex1_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c9 - DT Temporal ex1t Val - WS ex1t\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex1_pred['c9_ex1_pred'].to_numpy(), average='micro')\n",
    "c9dt_ex1_ws  = [['c9', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c9dt_ex1_ws = pd.DataFrame(data=c9dt_ex1_ws)\n",
    "c9dt_ex1_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c9dt_ex1_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c9 - Ex1 TPs\n",
    "\n",
    "c9_ex1_pred = ann_ex1_pred[['patientid','c9_ex1_pred']]\n",
    "c9_ex1_pred = pd.merge(c9_ex1_pred, hirid_test, on='patientid')\n",
    "c9_ex1_pred = c9_ex1_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c9_ex1_TP_alive = len(c9_ex1_pred[(c9_ex1_pred['c9_ex1_pred']==0) & (c9_ex1_pred['binary_discharge']==0)])\n",
    "c9_ex1_TP_dead = len(c9_ex1_pred[(c9_ex1_pred['c9_ex1_pred']==1) & (c9_ex1_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c9_ex1_pred['c9_ex1_pred'].value_counts())\n",
    "print('c9 TP - Discharged Alive: ', c9_ex1_TP_alive)\n",
    "print('c9 TP - Discharged Dead: ', c9_ex1_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c10 - DT Temporal ex1t Val - WS ex1t\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex1_pred['c10_ex1_pred'].to_numpy(), average='micro')\n",
    "c10dt_ex1_ws  = [['c10', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c10dt_ex1_ws = pd.DataFrame(data=c10dt_ex1_ws)\n",
    "c10dt_ex1_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c10dt_ex1_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c10 - Ex1 TPs\n",
    "\n",
    "c10_ex1_pred = ann_ex1_pred[['patientid','c10_ex1_pred']]\n",
    "c10_ex1_pred = pd.merge(c10_ex1_pred, hirid_test, on='patientid')\n",
    "c10_ex1_pred = c10_ex1_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c10_ex1_TP_alive = len(c10_ex1_pred[(c10_ex1_pred['c10_ex1_pred']==0) & (c10_ex1_pred['binary_discharge']==0)])\n",
    "c10_ex1_TP_dead = len(c10_ex1_pred[(c10_ex1_pred['c10_ex1_pred']==1) & (c10_ex1_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c10_ex1_pred['c10_ex1_pred'].value_counts())\n",
    "print('c10 TP - Discharged Alive: ', c10_ex1_TP_alive)\n",
    "print('c10 TP - Discharged Dead: ', c10_ex1_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c11 - DT Temporal ex1t Val - WS ex1t\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex1_pred['c11_ex1_pred'].to_numpy(), average='micro')\n",
    "c11dt_ex1_ws  = [['c11', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c11dt_ex1_ws = pd.DataFrame(data=c11dt_ex1_ws)\n",
    "c11dt_ex1_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c11dt_ex1_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c11 - Ex1 TPs\n",
    "\n",
    "c11_ex1_pred = ann_ex1_pred[['patientid','c11_ex1_pred']]\n",
    "c11_ex1_pred = pd.merge(c11_ex1_pred, hirid_test, on='patientid')\n",
    "c11_ex1_pred = c11_ex1_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c11_ex1_TP_alive = len(c11_ex1_pred[(c11_ex1_pred['c11_ex1_pred']==0) & (c11_ex1_pred['binary_discharge']==0)])\n",
    "c11_ex1_TP_dead = len(c11_ex1_pred[(c11_ex1_pred['c11_ex1_pred']==1) & (c11_ex1_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c11_ex1_pred['c11_ex1_pred'].value_counts())\n",
    "print('c11 TP - Discharged Alive: ', c11_ex1_TP_alive)\n",
    "print('c11 TP - Discharged Dead: ', c11_ex1_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mv - DT Temporal ex1t Val - WS ex1t\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex1_pred['mv_ex1_pred'].to_numpy(), average='micro')\n",
    "mvdt_ex1_ws  = [['MV', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "mvdt_ex1_ws = pd.DataFrame(data=mvdt_ex1_ws)\n",
    "mvdt_ex1_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "mvdt_ex1_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mv - Ex1 TPs\n",
    "\n",
    "mv_ex1_pred = ann_ex1_pred[['patientid','mv_ex1_pred']]\n",
    "mv_ex1_pred = pd.merge(mv_ex1_pred, hirid_test, on='patientid')\n",
    "mv_ex1_pred = mv_ex1_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "mv_ex1_TP_alive = len(mv_ex1_pred[(mv_ex1_pred['mv_ex1_pred']==0) & (mv_ex1_pred['binary_discharge']==0)])\n",
    "mv_ex1_TP_dead = len(mv_ex1_pred[(mv_ex1_pred['mv_ex1_pred']==1) & (mv_ex1_pred['binary_discharge']==1)])\n",
    "\n",
    "print(mv_ex1_pred['mv_ex1_pred'].value_counts())\n",
    "print('mv TP - Discharged Alive: ', mv_ex1_TP_alive)\n",
    "print('mv TP - Discharged Dead: ', mv_ex1_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmv - DT Temporal ex1t Val - WS ex1t\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex1_pred['tmv_ex1_pred'].to_numpy(), average='micro')\n",
    "tmvdt_ex1_ws  = [['TMV', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "tmvdt_ex1_ws = pd.DataFrame(data=tmvdt_ex1_ws)\n",
    "tmvdt_ex1_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "tmvdt_ex1_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmv - Ex1 TPs\n",
    "\n",
    "tmv_ex1_pred = ann_ex1_pred[['patientid','tmv_ex1_pred']]\n",
    "tmv_ex1_pred = pd.merge(tmv_ex1_pred, hirid_test, on='patientid')\n",
    "tmv_ex1_pred = tmv_ex1_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "tmv_ex1_TP_alive = len(tmv_ex1_pred[(tmv_ex1_pred['tmv_ex1_pred']==0) & (tmv_ex1_pred['binary_discharge']==0)])\n",
    "tmv_ex1_TP_dead = len(tmv_ex1_pred[(tmv_ex1_pred['tmv_ex1_pred']==1) & (tmv_ex1_pred['binary_discharge']==1)])\n",
    "\n",
    "print(tmv_ex1_pred['tmv_ex1_pred'].value_counts())\n",
    "print('tmv TP - Discharged Alive: ', tmv_ex1_TP_alive)\n",
    "print('tmv TP - Discharged Dead: ', tmv_ex1_TP_dead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WS - Extreme (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cut-offs: <=2 = discharged alive, >4 = died**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c1 - DT Temporal ex2t Val - WS ex2t\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex2_pred['c1_ex2_pred'].to_numpy(), average='micro')\n",
    "c1dt_ex2_ws  = [['c1', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c1dt_ex2_ws = pd.DataFrame(data=c1dt_ex2_ws)\n",
    "c1dt_ex2_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c1dt_ex2_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C1 - ex2 TPs\n",
    "\n",
    "c1_ex2_pred = ann_ex2_pred[['patientid','c1_ex2_pred']]\n",
    "c1_ex2_pred = pd.merge(c1_ex2_pred, hirid_test, on='patientid')\n",
    "c1_ex2_pred = c1_ex2_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c1_ex2_TP_alive = len(c1_ex2_pred[(c1_ex2_pred['c1_ex2_pred']==0) & (c1_ex2_pred['binary_discharge']==0)])\n",
    "c1_ex2_TP_dead = len(c1_ex2_pred[(c1_ex2_pred['c1_ex2_pred']==1) & (c1_ex2_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c1_ex2_pred['c1_ex2_pred'].value_counts())\n",
    "print('C1 TP - Discharged Alive: ', c1_ex2_TP_alive)\n",
    "print('C1 TP - Discharged Dead: ', c1_ex2_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c2 - DT Temporal Ext Val - WS Ext\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex2_pred['c2_ex2_pred'].to_numpy(), average='micro')\n",
    "c2dt_ex2_ws  = [['c2', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c2dt_ex2_ws = pd.DataFrame(data=c2dt_ex2_ws)\n",
    "c2dt_ex2_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c2dt_ex2_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c2 - ex2 TPs\n",
    "\n",
    "c2_ex2_pred = ann_ex2_pred[['patientid','c2_ex2_pred']]\n",
    "c2_ex2_pred = pd.merge(c2_ex2_pred, hirid_test, on='patientid')\n",
    "c2_ex2_pred = c2_ex2_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c2_ex2_TP_alive = len(c2_ex2_pred[(c2_ex2_pred['c2_ex2_pred']==0) & (c2_ex2_pred['binary_discharge']==0)])\n",
    "c2_ex2_TP_dead = len(c2_ex2_pred[(c2_ex2_pred['c2_ex2_pred']==1) & (c2_ex2_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c2_ex2_pred['c2_ex2_pred'].value_counts())\n",
    "print('c2 TP - Discharged Alive: ', c2_ex2_TP_alive)\n",
    "print('c2 TP - Discharged Dead: ', c2_ex2_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c3 - DT Temporal Ext Val - WS Ext\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex2_pred['c3_ex2_pred'].to_numpy(), average='micro')\n",
    "c3dt_ex2_ws  = [['c3', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c3dt_ex2_ws = pd.DataFrame(data=c3dt_ex2_ws)\n",
    "c3dt_ex2_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c3dt_ex2_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c3 - ex2 TPs\n",
    "\n",
    "c3_ex2_pred = ann_ex2_pred[['patientid','c3_ex2_pred']]\n",
    "c3_ex2_pred = pd.merge(c3_ex2_pred, hirid_test, on='patientid')\n",
    "c3_ex2_pred = c3_ex2_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c3_ex2_TP_alive = len(c3_ex2_pred[(c3_ex2_pred['c3_ex2_pred']==0) & (c3_ex2_pred['binary_discharge']==0)])\n",
    "c3_ex2_TP_dead = len(c3_ex2_pred[(c3_ex2_pred['c3_ex2_pred']==1) & (c3_ex2_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c3_ex2_pred['c3_ex2_pred'].value_counts())\n",
    "print('c3 TP - Discharged Alive: ', c3_ex2_TP_alive)\n",
    "print('c3 TP - Discharged Dead: ', c3_ex2_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c4 - DT Temporal Ext Val - WS Ext\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex2_pred['c4_ex2_pred'].to_numpy(), average='micro')\n",
    "c4dt_ex2_ws  = [['c4', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c4dt_ex2_ws = pd.DataFrame(data=c4dt_ex2_ws)\n",
    "c4dt_ex2_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c4dt_ex2_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c4 - ex2 TPs\n",
    "\n",
    "c4_ex2_pred = ann_ex2_pred[['patientid','c4_ex2_pred']]\n",
    "c4_ex2_pred = pd.merge(c4_ex2_pred, hirid_test, on='patientid')\n",
    "c4_ex2_pred = c4_ex2_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c4_ex2_TP_alive = len(c4_ex2_pred[(c4_ex2_pred['c4_ex2_pred']==0) & (c4_ex2_pred['binary_discharge']==0)])\n",
    "c4_ex2_TP_dead = len(c4_ex2_pred[(c4_ex2_pred['c4_ex2_pred']==1) & (c4_ex2_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c4_ex2_pred['c4_ex2_pred'].value_counts())\n",
    "print('c4 TP - Discharged Alive: ', c4_ex2_TP_alive)\n",
    "print('c4 TP - Discharged Dead: ', c4_ex2_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c5 - DT Temporal Ext Val - WS Ext\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex2_pred['c5_ex2_pred'].to_numpy(), average='micro')\n",
    "c5dt_ex2_ws  = [['c5', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c5dt_ex2_ws = pd.DataFrame(data=c5dt_ex2_ws)\n",
    "c5dt_ex2_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c5dt_ex2_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c5 - ex2 TPs\n",
    "\n",
    "c5_ex2_pred = ann_ex2_pred[['patientid','c5_ex2_pred']]\n",
    "c5_ex2_pred = pd.merge(c5_ex2_pred, hirid_test, on='patientid')\n",
    "c5_ex2_pred = c5_ex2_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c5_ex2_TP_alive = len(c5_ex2_pred[(c5_ex2_pred['c5_ex2_pred']==0) & (c5_ex2_pred['binary_discharge']==0)])\n",
    "c5_ex2_TP_dead = len(c5_ex2_pred[(c5_ex2_pred['c5_ex2_pred']==1) & (c5_ex2_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c5_ex2_pred['c5_ex2_pred'].value_counts())\n",
    "print('c5 TP - Discharged Alive: ', c5_ex2_TP_alive)\n",
    "print('c5 TP - Discharged Dead: ', c5_ex2_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c6 - DT Temporal Ext Val - WS Ext\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex2_pred['c6_ex2_pred'].to_numpy(), average='micro')\n",
    "c6dt_ex2_ws  = [['c6', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c6dt_ex2_ws = pd.DataFrame(data=c6dt_ex2_ws)\n",
    "c6dt_ex2_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c6dt_ex2_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c6 - ex2 TPs\n",
    "\n",
    "c6_ex2_pred = ann_ex2_pred[['patientid','c6_ex2_pred']]\n",
    "c6_ex2_pred = pd.merge(c6_ex2_pred, hirid_test, on='patientid')\n",
    "c6_ex2_pred = c6_ex2_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c6_ex2_TP_alive = len(c6_ex2_pred[(c6_ex2_pred['c6_ex2_pred']==0) & (c6_ex2_pred['binary_discharge']==0)])\n",
    "c6_ex2_TP_dead = len(c6_ex2_pred[(c6_ex2_pred['c6_ex2_pred']==1) & (c6_ex2_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c6_ex2_pred['c6_ex2_pred'].value_counts())\n",
    "print('c6 TP - Discharged Alive: ', c6_ex2_TP_alive)\n",
    "print('c6 TP - Discharged Dead: ', c6_ex2_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c7 - DT Temporal Ext Val - WS Ext\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex2_pred['c7_ex2_pred'].to_numpy(), average='micro')\n",
    "c7dt_ex2_ws  = [['c7', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c7dt_ex2_ws = pd.DataFrame(data=c7dt_ex2_ws)\n",
    "c7dt_ex2_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c7dt_ex2_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c7 - ex2 TPs\n",
    "\n",
    "c7_ex2_pred = ann_ex2_pred[['patientid','c7_ex2_pred']]\n",
    "c7_ex2_pred = pd.merge(c7_ex2_pred, hirid_test, on='patientid')\n",
    "c7_ex2_pred = c7_ex2_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c7_ex2_TP_alive = len(c7_ex2_pred[(c7_ex2_pred['c7_ex2_pred']==0) & (c7_ex2_pred['binary_discharge']==0)])\n",
    "c7_ex2_TP_dead = len(c7_ex2_pred[(c7_ex2_pred['c7_ex2_pred']==1) & (c7_ex2_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c7_ex2_pred['c7_ex2_pred'].value_counts())\n",
    "print('c7 TP - Discharged Alive: ', c7_ex2_TP_alive)\n",
    "print('c7 TP - Discharged Dead: ', c7_ex2_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c8 - DT Temporal Ext Val - WS Ext\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex2_pred['c8_ex2_pred'].to_numpy(), average='micro')\n",
    "c8dt_ex2_ws  = [['c8', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c8dt_ex2_ws = pd.DataFrame(data=c8dt_ex2_ws)\n",
    "c8dt_ex2_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c8dt_ex2_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c8 - ex2 TPs\n",
    "\n",
    "c8_ex2_pred = ann_ex2_pred[['patientid','c8_ex2_pred']]\n",
    "c8_ex2_pred = pd.merge(c8_ex2_pred, hirid_test, on='patientid')\n",
    "c8_ex2_pred = c8_ex2_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c8_ex2_TP_alive = len(c8_ex2_pred[(c8_ex2_pred['c8_ex2_pred']==0) & (c8_ex2_pred['binary_discharge']==0)])\n",
    "c8_ex2_TP_dead = len(c8_ex2_pred[(c8_ex2_pred['c8_ex2_pred']==1) & (c8_ex2_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c8_ex2_pred['c8_ex2_pred'].value_counts())\n",
    "print('c8 TP - Discharged Alive: ', c8_ex2_TP_alive)\n",
    "print('c8 TP - Discharged Dead: ', c8_ex2_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c9 - DT Temporal Ext Val - WS Ext\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex2_pred['c9_ex2_pred'].to_numpy(), average='micro')\n",
    "c9dt_ex2_ws  = [['c9', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c9dt_ex2_ws = pd.DataFrame(data=c9dt_ex2_ws)\n",
    "c9dt_ex2_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c9dt_ex2_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c9 - ex2 TPs\n",
    "\n",
    "c9_ex2_pred = ann_ex2_pred[['patientid','c9_ex2_pred']]\n",
    "c9_ex2_pred = pd.merge(c9_ex2_pred, hirid_test, on='patientid')\n",
    "c9_ex2_pred = c9_ex2_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c9_ex2_TP_alive = len(c9_ex2_pred[(c9_ex2_pred['c9_ex2_pred']==0) & (c9_ex2_pred['binary_discharge']==0)])\n",
    "c9_ex2_TP_dead = len(c9_ex2_pred[(c9_ex2_pred['c9_ex2_pred']==1) & (c9_ex2_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c9_ex2_pred['c9_ex2_pred'].value_counts())\n",
    "print('c9 TP - Discharged Alive: ', c9_ex2_TP_alive)\n",
    "print('c9 TP - Discharged Dead: ', c9_ex2_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c10 - DT Temporal Ext Val - WS Ext\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex2_pred['c10_ex2_pred'].to_numpy(), average='micro')\n",
    "c10dt_ex2_ws  = [['c10', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c10dt_ex2_ws = pd.DataFrame(data=c10dt_ex2_ws)\n",
    "c10dt_ex2_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c10dt_ex2_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c10 - ex2 TPs\n",
    "\n",
    "c10_ex2_pred = ann_ex2_pred[['patientid','c10_ex2_pred']]\n",
    "c10_ex2_pred = pd.merge(c10_ex2_pred, hirid_test, on='patientid')\n",
    "c10_ex2_pred = c10_ex2_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c10_ex2_TP_alive = len(c10_ex2_pred[(c10_ex2_pred['c10_ex2_pred']==0) & (c10_ex2_pred['binary_discharge']==0)])\n",
    "c10_ex2_TP_dead = len(c10_ex2_pred[(c10_ex2_pred['c10_ex2_pred']==1) & (c10_ex2_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c10_ex2_pred['c10_ex2_pred'].value_counts())\n",
    "print('c10 TP - Discharged Alive: ', c10_ex2_TP_alive)\n",
    "print('c10 TP - Discharged Dead: ', c10_ex2_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c11 - DT Temporal Ext Val - WS Ext\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex2_pred['c11_ex2_pred'].to_numpy(), average='micro')\n",
    "c11dt_ex2_ws  = [['c11', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c11dt_ex2_ws = pd.DataFrame(data=c11dt_ex2_ws)\n",
    "c11dt_ex2_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c11dt_ex2_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c11 - ex2 TPs\n",
    "\n",
    "c11_ex2_pred = ann_ex2_pred[['patientid','c11_ex2_pred']]\n",
    "c11_ex2_pred = pd.merge(c11_ex2_pred, hirid_test, on='patientid')\n",
    "c11_ex2_pred = c11_ex2_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c11_ex2_TP_alive = len(c11_ex2_pred[(c11_ex2_pred['c11_ex2_pred']==0) & (c11_ex2_pred['binary_discharge']==0)])\n",
    "c11_ex2_TP_dead = len(c11_ex2_pred[(c11_ex2_pred['c11_ex2_pred']==1) & (c11_ex2_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c11_ex2_pred['c11_ex2_pred'].value_counts())\n",
    "print('c11 TP - Discharged Alive: ', c11_ex2_TP_alive)\n",
    "print('c11 TP - Discharged Dead: ', c11_ex2_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mv - DT Temporal Ext Val - WS Ext\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex2_pred['mv_ex2_pred'].to_numpy(), average='micro')\n",
    "mvdt_ex2_ws  = [['MV', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "mvdt_ex2_ws = pd.DataFrame(data=mvdt_ex2_ws)\n",
    "mvdt_ex2_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "mvdt_ex2_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mv - ex2 TPs\n",
    "\n",
    "mv_ex2_pred = ann_ex2_pred[['patientid','mv_ex2_pred']]\n",
    "mv_ex2_pred = pd.merge(mv_ex2_pred, hirid_test, on='patientid')\n",
    "mv_ex2_pred = mv_ex2_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "mv_ex2_TP_alive = len(mv_ex2_pred[(mv_ex2_pred['mv_ex2_pred']==0) & (mv_ex2_pred['binary_discharge']==0)])\n",
    "mv_ex2_TP_dead = len(mv_ex2_pred[(mv_ex2_pred['mv_ex2_pred']==1) & (mv_ex2_pred['binary_discharge']==1)])\n",
    "\n",
    "print(mv_ex2_pred['mv_ex2_pred'].value_counts())\n",
    "print('mv TP - Discharged Alive: ', mv_ex2_TP_alive)\n",
    "print('mv TP - Discharged Dead: ', mv_ex2_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmv - DT Temporal Ext Val - WS Ext\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_ex2_pred['tmv_ex2_pred'].to_numpy(), average='micro')\n",
    "tmvdt_ex2_ws  = [['TMV', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "tmvdt_ex2_ws = pd.DataFrame(data=tmvdt_ex2_ws)\n",
    "tmvdt_ex2_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "tmvdt_ex2_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmv - ex2 TPs\n",
    "\n",
    "tmv_ex2_pred = ann_ex2_pred[['patientid','tmv_ex2_pred']]\n",
    "tmv_ex2_pred = pd.merge(tmv_ex2_pred, hirid_test, on='patientid')\n",
    "tmv_ex2_pred = tmv_ex2_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "tmv_ex2_TP_alive = len(tmv_ex2_pred[(tmv_ex2_pred['tmv_ex2_pred']==0) & (tmv_ex2_pred['binary_discharge']==0)])\n",
    "tmv_ex2_TP_dead = len(tmv_ex2_pred[(tmv_ex2_pred['tmv_ex2_pred']==1) & (tmv_ex2_pred['binary_discharge']==1)])\n",
    "\n",
    "print(tmv_ex2_pred['tmv_ex2_pred'].value_counts())\n",
    "print('tmv TP - Discharged Alive: ', tmv_ex2_TP_alive)\n",
    "print('tmv TP - Discharged Dead: ', tmv_ex2_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ext Val Extreme(2) - Summary\n",
    "\n",
    "frames = [c1dt_ex2_ws, c2dt_ex2_ws, c3dt_ex2_ws, c4dt_ex2_ws, c5dt_ex2_ws, c6dt_ex2_ws, c7dt_ex2_ws, c8dt_ex2_ws,\n",
    "          c9dt_ex2_ws, c10dt_ex2_ws, c11dt_ex2_ws, mvdt_ex2_ws, tmvdt_ex2_ws]\n",
    "\n",
    "multi_ex2_pred = pd.concat(frames)\n",
    "print(multi_ex2_pred.shape)\n",
    "multi_ex2_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WS - Neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cut-offs: <=3 = discharged alive, >=4 = died**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c1 - DT Temporal Ext Val - WS Neutral\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_neut_pred['c1_neut_pred'].to_numpy(), average='micro')\n",
    "c1dt_neut_ws  = [['c1', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c1dt_neut_ws = pd.DataFrame(data=c1dt_neut_ws)\n",
    "c1dt_neut_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c1dt_neut_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c1 - neut TPs\n",
    "\n",
    "c1_neut_pred = ann_neut_pred[['patientid','c1_neut_pred']]\n",
    "c1_neut_pred = pd.merge(c1_neut_pred, hirid_test, on='patientid')\n",
    "c1_neut_pred = c1_neut_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c1_neut_TP_alive = len(c1_neut_pred[(c1_neut_pred['c1_neut_pred']==0) & (c1_neut_pred['binary_discharge']==0)])\n",
    "c1_neut_TP_dead = len(c1_neut_pred[(c1_neut_pred['c1_neut_pred']==1) & (c1_neut_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c1_neut_pred['c1_neut_pred'].value_counts())\n",
    "print('c1 TP - Discharged Alive: ', c1_neut_TP_alive)\n",
    "print('c1 TP - Discharged Dead: ', c1_neut_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c2 - DT Temporal Ext Val - WS Neutral\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_neut_pred['c2_neut_pred'].to_numpy(), average='micro')\n",
    "c2dt_neut_ws  = [['c2', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c2dt_neut_ws = pd.DataFrame(data=c2dt_neut_ws)\n",
    "c2dt_neut_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c2dt_neut_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c2 - neut TPs\n",
    "\n",
    "c2_neut_pred = ann_neut_pred[['patientid','c2_neut_pred']]\n",
    "c2_neut_pred = pd.merge(c2_neut_pred, hirid_test, on='patientid')\n",
    "c2_neut_pred = c2_neut_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c2_neut_TP_alive = len(c2_neut_pred[(c2_neut_pred['c2_neut_pred']==0) & (c2_neut_pred['binary_discharge']==0)])\n",
    "c2_neut_TP_dead = len(c2_neut_pred[(c2_neut_pred['c2_neut_pred']==1) & (c2_neut_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c2_neut_pred['c2_neut_pred'].value_counts())\n",
    "print('c2 TP - Discharged Alive: ', c2_neut_TP_alive)\n",
    "print('c2 TP - Discharged Dead: ', c2_neut_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c3 - DT Temporal Ext Val - WS Neutral\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_neut_pred['c3_neut_pred'].to_numpy(), average='micro')\n",
    "c3dt_neut_ws  = [['c3', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c3dt_neut_ws = pd.DataFrame(data=c3dt_neut_ws)\n",
    "c3dt_neut_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c3dt_neut_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c3 - neut TPs\n",
    "\n",
    "c3_neut_pred = ann_neut_pred[['patientid','c3_neut_pred']]\n",
    "c3_neut_pred = pd.merge(c3_neut_pred, hirid_test, on='patientid')\n",
    "c3_neut_pred = c3_neut_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c3_neut_TP_alive = len(c3_neut_pred[(c3_neut_pred['c3_neut_pred']==0) & (c3_neut_pred['binary_discharge']==0)])\n",
    "c3_neut_TP_dead = len(c3_neut_pred[(c3_neut_pred['c3_neut_pred']==1) & (c3_neut_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c3_neut_pred['c3_neut_pred'].value_counts())\n",
    "print('c3 TP - Discharged Alive: ', c3_neut_TP_alive)\n",
    "print('c3 TP - Discharged Dead: ', c3_neut_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c4 - DT Temporal Ext Val - WS Neutral\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_neut_pred['c4_neut_pred'].to_numpy(), average='micro')\n",
    "c4dt_neut_ws  = [['c4', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c4dt_neut_ws = pd.DataFrame(data=c4dt_neut_ws)\n",
    "c4dt_neut_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c4dt_neut_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c4 - neut TPs\n",
    "\n",
    "c4_neut_pred = ann_neut_pred[['patientid','c4_neut_pred']]\n",
    "c4_neut_pred = pd.merge(c4_neut_pred, hirid_test, on='patientid')\n",
    "c4_neut_pred = c4_neut_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c4_neut_TP_alive = len(c4_neut_pred[(c4_neut_pred['c4_neut_pred']==0) & (c4_neut_pred['binary_discharge']==0)])\n",
    "c4_neut_TP_dead = len(c4_neut_pred[(c4_neut_pred['c4_neut_pred']==1) & (c4_neut_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c4_neut_pred['c4_neut_pred'].value_counts())\n",
    "print('c4 TP - Discharged Alive: ', c4_neut_TP_alive)\n",
    "print('c4 TP - Discharged Dead: ', c4_neut_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c5 - DT Temporal Ext Val - WS Neutral\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_neut_pred['c5_neut_pred'].to_numpy(), average='micro')\n",
    "c5dt_neut_ws  = [['c5', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c5dt_neut_ws = pd.DataFrame(data=c5dt_neut_ws)\n",
    "c5dt_neut_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c5dt_neut_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c5 - neut TPs\n",
    "\n",
    "c5_neut_pred = ann_neut_pred[['patientid','c5_neut_pred']]\n",
    "c5_neut_pred = pd.merge(c5_neut_pred, hirid_test, on='patientid')\n",
    "c5_neut_pred = c5_neut_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c5_neut_TP_alive = len(c5_neut_pred[(c5_neut_pred['c5_neut_pred']==0) & (c5_neut_pred['binary_discharge']==0)])\n",
    "c5_neut_TP_dead = len(c5_neut_pred[(c5_neut_pred['c5_neut_pred']==1) & (c5_neut_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c5_neut_pred['c5_neut_pred'].value_counts())\n",
    "print('c5 TP - Discharged Alive: ', c5_neut_TP_alive)\n",
    "print('c5 TP - Discharged Dead: ', c5_neut_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c6 - DT Temporal Ext Val - WS Neutral\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_neut_pred['c6_neut_pred'].to_numpy(), average='micro')\n",
    "c6dt_neut_ws  = [['c6', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c6dt_neut_ws = pd.DataFrame(data=c6dt_neut_ws)\n",
    "c6dt_neut_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c6dt_neut_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c6 - neut TPs\n",
    "\n",
    "c6_neut_pred = ann_neut_pred[['patientid','c6_neut_pred']]\n",
    "c6_neut_pred = pd.merge(c6_neut_pred, hirid_test, on='patientid')\n",
    "c6_neut_pred = c6_neut_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c6_neut_TP_alive = len(c6_neut_pred[(c6_neut_pred['c6_neut_pred']==0) & (c6_neut_pred['binary_discharge']==0)])\n",
    "c6_neut_TP_dead = len(c6_neut_pred[(c6_neut_pred['c6_neut_pred']==1) & (c6_neut_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c6_neut_pred['c6_neut_pred'].value_counts())\n",
    "print('c6 TP - Discharged Alive: ', c6_neut_TP_alive)\n",
    "print('c6 TP - Discharged Dead: ', c6_neut_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c7 - DT Temporal Ext Val - WS Neutral\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_neut_pred['c7_neut_pred'].to_numpy(), average='micro')\n",
    "c7dt_neut_ws  = [['c7', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c7dt_neut_ws = pd.DataFrame(data=c7dt_neut_ws)\n",
    "c7dt_neut_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c7dt_neut_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c7 - neut TPs\n",
    "\n",
    "c7_neut_pred = ann_neut_pred[['patientid','c7_neut_pred']]\n",
    "c7_neut_pred = pd.merge(c7_neut_pred, hirid_test, on='patientid')\n",
    "c7_neut_pred = c7_neut_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c7_neut_TP_alive = len(c7_neut_pred[(c7_neut_pred['c7_neut_pred']==0) & (c7_neut_pred['binary_discharge']==0)])\n",
    "c7_neut_TP_dead = len(c7_neut_pred[(c7_neut_pred['c7_neut_pred']==1) & (c7_neut_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c7_neut_pred['c7_neut_pred'].value_counts())\n",
    "print('c7 TP - Discharged Alive: ', c7_neut_TP_alive)\n",
    "print('c7 TP - Discharged Dead: ', c7_neut_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c8 - DT Temporal Ext Val - WS Neutral\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_neut_pred['c8_neut_pred'].to_numpy(), average='micro')\n",
    "c8dt_neut_ws  = [['c8', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c8dt_neut_ws = pd.DataFrame(data=c8dt_neut_ws)\n",
    "c8dt_neut_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c8dt_neut_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c8 - neut TPs\n",
    "\n",
    "c8_neut_pred = ann_neut_pred[['patientid','c8_neut_pred']]\n",
    "c8_neut_pred = pd.merge(c8_neut_pred, hirid_test, on='patientid')\n",
    "c8_neut_pred = c8_neut_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c8_neut_TP_alive = len(c8_neut_pred[(c8_neut_pred['c8_neut_pred']==0) & (c8_neut_pred['binary_discharge']==0)])\n",
    "c8_neut_TP_dead = len(c8_neut_pred[(c8_neut_pred['c8_neut_pred']==1) & (c8_neut_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c8_neut_pred['c8_neut_pred'].value_counts())\n",
    "print('c8 TP - Discharged Alive: ', c8_neut_TP_alive)\n",
    "print('c8 TP - Discharged Dead: ', c8_neut_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c9 - DT Temporal Ext Val - WS Neutral\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_neut_pred['c9_neut_pred'].to_numpy(), average='micro')\n",
    "c9dt_neut_ws  = [['c9', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c9dt_neut_ws = pd.DataFrame(data=c9dt_neut_ws)\n",
    "c9dt_neut_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c9dt_neut_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c9 - neut TPs\n",
    "\n",
    "c9_neut_pred = ann_neut_pred[['patientid','c9_neut_pred']]\n",
    "c9_neut_pred = pd.merge(c9_neut_pred, hirid_test, on='patientid')\n",
    "c9_neut_pred = c9_neut_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c9_neut_TP_alive = len(c9_neut_pred[(c9_neut_pred['c9_neut_pred']==0) & (c9_neut_pred['binary_discharge']==0)])\n",
    "c9_neut_TP_dead = len(c9_neut_pred[(c9_neut_pred['c9_neut_pred']==1) & (c9_neut_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c9_neut_pred['c9_neut_pred'].value_counts())\n",
    "print('c9 TP - Discharged Alive: ', c9_neut_TP_alive)\n",
    "print('c9 TP - Discharged Dead: ', c9_neut_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c10 - DT Temporal Ext Val - WS Neutral\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_neut_pred['c10_neut_pred'].to_numpy(), average='micro')\n",
    "c10dt_neut_ws  = [['c10', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c10dt_neut_ws = pd.DataFrame(data=c10dt_neut_ws)\n",
    "c10dt_neut_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c10dt_neut_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c10 - neut TPs\n",
    "\n",
    "c10_neut_pred = ann_neut_pred[['patientid','c10_neut_pred']]\n",
    "c10_neut_pred = pd.merge(c10_neut_pred, hirid_test, on='patientid')\n",
    "c10_neut_pred = c10_neut_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c10_neut_TP_alive = len(c10_neut_pred[(c10_neut_pred['c10_neut_pred']==0) & (c10_neut_pred['binary_discharge']==0)])\n",
    "c10_neut_TP_dead = len(c10_neut_pred[(c10_neut_pred['c10_neut_pred']==1) & (c10_neut_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c10_neut_pred['c10_neut_pred'].value_counts())\n",
    "print('c10 TP - Discharged Alive: ', c10_neut_TP_alive)\n",
    "print('c10 TP - Discharged Dead: ', c10_neut_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c11 - DT Temporal Ext Val - WS Neutral\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_neut_pred['c11_neut_pred'].to_numpy(), average='micro')\n",
    "c11dt_neut_ws  = [['c11', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "c11dt_neut_ws = pd.DataFrame(data=c11dt_neut_ws)\n",
    "c11dt_neut_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "c11dt_neut_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c11 - neut TPs\n",
    "\n",
    "c11_neut_pred = ann_neut_pred[['patientid','c11_neut_pred']]\n",
    "c11_neut_pred = pd.merge(c11_neut_pred, hirid_test, on='patientid')\n",
    "c11_neut_pred = c11_neut_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "c11_neut_TP_alive = len(c11_neut_pred[(c11_neut_pred['c11_neut_pred']==0) & (c11_neut_pred['binary_discharge']==0)])\n",
    "c11_neut_TP_dead = len(c11_neut_pred[(c11_neut_pred['c11_neut_pred']==1) & (c11_neut_pred['binary_discharge']==1)])\n",
    "\n",
    "print(c11_neut_pred['c11_neut_pred'].value_counts())\n",
    "print('c11 TP - Discharged Alive: ', c11_neut_TP_alive)\n",
    "print('c11 TP - Discharged Dead: ', c11_neut_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mv - DT Temporal Ext Val - WS Neutral\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_neut_pred['mv_neut_pred'].to_numpy(), average='micro')\n",
    "mvdt_neut_ws  = [['MV', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "mvdt_neut_ws = pd.DataFrame(data=mvdt_neut_ws)\n",
    "mvdt_neut_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "mvdt_neut_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mv - neut TPs\n",
    "\n",
    "mv_neut_pred = ann_neut_pred[['patientid','mv_neut_pred']]\n",
    "mv_neut_pred = pd.merge(mv_neut_pred, hirid_test, on='patientid')\n",
    "mv_neut_pred = mv_neut_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "mv_neut_TP_alive = len(mv_neut_pred[(mv_neut_pred['mv_neut_pred']==0) & (mv_neut_pred['binary_discharge']==0)])\n",
    "mv_neut_TP_dead = len(mv_neut_pred[(mv_neut_pred['mv_neut_pred']==1) & (mv_neut_pred['binary_discharge']==1)])\n",
    "\n",
    "print(mv_neut_pred['mv_neut_pred'].value_counts())\n",
    "print('mv TP - Discharged Alive: ', mv_neut_TP_alive)\n",
    "print('mv TP - Discharged Dead: ', mv_neut_TP_dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmv - DT Temporal Ext Val - WS Neutral\n",
    "\n",
    "f1 = metrics.f1_score(y_test, ann_neut_pred['tmv_neut_pred'].to_numpy(), average='micro')\n",
    "tmvdt_neut_ws  = [['TMV', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "##print data as DF\n",
    "tmvdt_neut_ws = pd.DataFrame(data=tmvdt_neut_ws)\n",
    "tmvdt_neut_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "tmvdt_neut_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmv - neut TPs\n",
    "\n",
    "tmv_neut_pred = ann_neut_pred[['patientid','tmv_neut_pred']]\n",
    "tmv_neut_pred = pd.merge(tmv_neut_pred, hirid_test, on='patientid')\n",
    "tmv_neut_pred = tmv_neut_pred.drop('discharge_status',axis=1)\n",
    "\n",
    "tmv_neut_TP_alive = len(tmv_neut_pred[(tmv_neut_pred['tmv_neut_pred']==0) & (tmv_neut_pred['binary_discharge']==0)])\n",
    "tmv_neut_TP_dead = len(tmv_neut_pred[(tmv_neut_pred['tmv_neut_pred']==1) & (tmv_neut_pred['binary_discharge']==1)])\n",
    "\n",
    "print(tmv_neut_pred['tmv_neut_pred'].value_counts())\n",
    "print('tmv TP - Discharged Alive: ', tmv_neut_TP_alive)\n",
    "print('tmv TP - Discharged Dead: ', tmv_neut_TP_dead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WS  - Consensus Fuzzy Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fuzzy Consensus (FC) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hirid_val.shape)\n",
    "hirid_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporal HiRID Validation Dataset\n",
    "mod_hirid_val = hirid_val.copy(deep=True)\n",
    "\n",
    "#drop mv_pred & tmv_pred (not needed)\n",
    "mod_hirid_val = mod_hirid_val.drop(['mv_pred', 'tmv_pred'],axis=1)\n",
    "\n",
    "cols = ['c1_pred','c2_pred', 'c3_pred', 'c4_pred', 'c5_pred', 'c6_pred', 'c7_pred','c8_pred', 'c9_pred', \n",
    "        'c10_pred', 'c11_pred']\n",
    "\n",
    "#relabel 0-4 predicted labels to 1-5 (still representing A-E)\n",
    "mod_hirid_val[cols] = mod_hirid_val[cols].replace({0:1, 1:2, 2:3, 3:4, 4:5})\n",
    "\n",
    "#within annotator predicted labels, change 3 to 0 (as defining 'C' label as uncertain, therefore not considering in fuzzy weighted calculation)\n",
    "mod_hirid_val[cols] = mod_hirid_val[cols].replace({3:0})\n",
    "\n",
    "#count how many predicted 'C' labels (i.e. '0') there are for each instance\n",
    "mod_hirid_val['zero_count'] = mod_hirid_val[cols].eq(0).sum(axis=1)\n",
    "\n",
    "#denominator for fuzzy weighted calculation\n",
    "mod_hirid_val['denominator'] = 11 - mod_hirid_val['zero_count'] \n",
    "\n",
    "mod_hirid_val['fuzzy_wa'] = mod_hirid_val[cols].sum(axis=1) / mod_hirid_val['denominator']\n",
    "\n",
    "print(mod_hirid_val.shape)\n",
    "print(mod_hirid_val.columns)\n",
    "mod_hirid_val.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculated Fuzzy Weighted Sum - Part 1 \n",
    "\n",
    "#define weights to each hour before event (discharge/death) - with higher bias towards hours nearer event\n",
    "mod_hirid_val['weight'] = 0\n",
    "mod_hirid_val.loc[mod_hirid_val['Hrs before d_time'] == 1, 'weight'] = 0.3\n",
    "mod_hirid_val.loc[mod_hirid_val['Hrs before d_time'] == 2, 'weight'] = 0.3\n",
    "mod_hirid_val.loc[mod_hirid_val['Hrs before d_time'] == 3, 'weight'] = 0.2\n",
    "mod_hirid_val.loc[mod_hirid_val['Hrs before d_time'] == 4, 'weight'] = 0.1\n",
    "mod_hirid_val.loc[mod_hirid_val['Hrs before d_time'] == 5, 'weight'] = 0.1\n",
    "\n",
    "#fuzzy weighted prediction\n",
    "mod_hirid_val['fuzzy_ws'] = mod_hirid_val['fuzzy_wa']*mod_hirid_val['weight']\n",
    "\n",
    "print(mod_hirid_val.shape)\n",
    "mod_hirid_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculated Fuzzy Weighted Sum - Part 2\n",
    "\n",
    "fuzzy_ws = mod_hirid_val.groupby(by=['patientid'])['fuzzy_ws'].sum().reset_index()\n",
    "\n",
    "print('fuzzy ws range:', round(fuzzy_ws['fuzzy_ws'].min(),3), '-', fuzzy_ws['fuzzy_ws'].max() ) \n",
    "\n",
    "print(fuzzy_ws.shape)\n",
    "fuzzy_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Extreme(1) logic to get final fuzzy label for each patient: 1 = discharged alive, >4 = died\n",
    "fuzzy_ws['Ext1_label'] = fuzzy_ws.apply(lambda row: ex1_cat(row, 'fuzzy_ws'), axis=1)\n",
    "\n",
    "#Apply Extreme(1) logic to get final fuzzy label for each patient: <=2 = discharged alive, >4 = died\n",
    "fuzzy_ws['Ext2_label'] = fuzzy_ws.apply(lambda row: ex2_cat(row, 'fuzzy_ws'), axis=1)\n",
    "\n",
    "#Apply Neutral logic to get final fuzzy label for each patient: <=3 = discharged alive, >=4 = died\n",
    "fuzzy_ws['Neut_label'] = fuzzy_ws.apply(lambda row: neut_cat(row, 'fuzzy_ws'), axis=1)\n",
    "\n",
    "print(fuzzy_ws.shape)\n",
    "fuzzy_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fuzzy Extreme(1) WS\n",
    "\n",
    "f1 = metrics.f1_score(y_test, fuzzy_ws['Ext1_label'].to_numpy(), average='micro')\n",
    "fcdt_ex1_ws  = [['FC', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "fcdt_ex1_ws = pd.DataFrame(data=fcdt_ex1_ws)\n",
    "fcdt_ex1_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "fcdt_ex1_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fuzzy Extreme(2) WS\n",
    "\n",
    "f1 = metrics.f1_score(y_test, fuzzy_ws['Ext2_label'].to_numpy(), average='micro')\n",
    "fcdt_ex2_ws  = [['FC', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "fcdt_ex2_ws = pd.DataFrame(data=fcdt_ex2_ws)\n",
    "fcdt_ex2_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "fcdt_ex2_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fuzzy Neutral WS\n",
    "\n",
    "f1 = metrics.f1_score(y_test, fuzzy_ws['Neut_label'].to_numpy(), average='micro')\n",
    "fcdt_neut_ws  = [['FC', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "fcdt_neut_ws = pd.DataFrame(data=fcdt_neut_ws)\n",
    "fcdt_neut_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "fcdt_neut_ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Fuzzy Consensus (TFC) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hirid_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporal HiRID Validation Dataset\n",
    "tfc_hirid_val = hirid_val.copy(deep=True)\n",
    "\n",
    "#only keep top performing model predictions (C2, C4, C8 models had highest int validation performance)\n",
    "drop = ['c1_pred', 'c3_pred',  'c5_pred', 'c6_pred', 'c7_pred','c9_pred', 'c10_pred', 'c11_pred']\n",
    "tfc_hirid_val = tfc_hirid_val.drop(drop,axis=1)\n",
    "\n",
    "#relabel 0-4 predicted labels to 1-5 (still representing A-E)\n",
    "cols = ['c2_pred', 'c4_pred', 'c8_pred']\n",
    "tfc_hirid_val[cols] = tfc_hirid_val[cols].replace({0:1, 1:2, 2:3, 3:4, 4:5})\n",
    "\n",
    "#within annotator predicted labels, change 3 to 0 (as defining 'C' label as uncertain, therefore not considering in fuzzy weighted calculation)\n",
    "tfc_hirid_val[cols] = tfc_hirid_val[cols].replace({3:0})\n",
    "\n",
    "#count how many predicted 'C' labels (i.e. '0') there are for each instance\n",
    "tfc_hirid_val['zero_count'] = tfc_hirid_val[cols].eq(0).sum(axis=1)\n",
    "\n",
    "#denominator for fuzzy weighted calculation\n",
    "tfc_hirid_val['denominator'] = 3 - tfc_hirid_val['zero_count'] \n",
    "\n",
    "tfc_hirid_val['fuzzy_wa'] = tfc_hirid_val[cols].sum(axis=1) / tfc_hirid_val['denominator']\n",
    "\n",
    "print(tfc_hirid_val.shape)\n",
    "print(tfc_hirid_val.columns)\n",
    "tfc_hirid_val.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculated Fuzzy Weighted Sum - Part 1 \n",
    "\n",
    "#define weights to each hour before event (discharge/death) - with higher bias towards hours nearer event\n",
    "tfc_hirid_val['weight'] = 0\n",
    "tfc_hirid_val.loc[tfc_hirid_val['Hrs before d_time'] == 1, 'weight'] = 0.3\n",
    "tfc_hirid_val.loc[tfc_hirid_val['Hrs before d_time'] == 2, 'weight'] = 0.3\n",
    "tfc_hirid_val.loc[tfc_hirid_val['Hrs before d_time'] == 3, 'weight'] = 0.2\n",
    "tfc_hirid_val.loc[tfc_hirid_val['Hrs before d_time'] == 4, 'weight'] = 0.1\n",
    "tfc_hirid_val.loc[tfc_hirid_val['Hrs before d_time'] == 5, 'weight'] = 0.1\n",
    "\n",
    "#fuzzy weighted prediction\n",
    "tfc_hirid_val['fuzzy_ws'] = tfc_hirid_val['fuzzy_wa']*tfc_hirid_val['weight']\n",
    "\n",
    "print(tfc_hirid_val.shape)\n",
    "tfc_hirid_val.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculated Fuzzy Weighted Sum - Part 2\n",
    "\n",
    "tfc_fuzzy_ws = tfc_hirid_val.groupby(by=['patientid'])['fuzzy_ws'].sum().reset_index()\n",
    "\n",
    "print('fuzzy ws range:', round(tfc_fuzzy_ws['fuzzy_ws'].min(),3), '-', tfc_fuzzy_ws['fuzzy_ws'].max() ) \n",
    "\n",
    "print(tfc_fuzzy_ws.shape)\n",
    "tfc_fuzzy_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Extreme(1) logic to get final tfc_fuzzy label for each patient: 1 = discharged alive, >4 = died\n",
    "tfc_fuzzy_ws['Ext1_label'] = tfc_fuzzy_ws.apply(lambda row: ex1_cat(row, 'fuzzy_ws'), axis=1)\n",
    "\n",
    "#Apply Extreme(1) logic to get final tfc_fuzzy label for each patient: <=2 = discharged alive, >4 = died\n",
    "tfc_fuzzy_ws['Ext2_label'] = tfc_fuzzy_ws.apply(lambda row: ex2_cat(row, 'fuzzy_ws'), axis=1)\n",
    "\n",
    "#Apply Neutral logic to get final tfc_fuzzy label for each patient: <=3 = discharged alive, >=4 = died\n",
    "tfc_fuzzy_ws['Neut_label'] = tfc_fuzzy_ws.apply(lambda row: neut_cat(row, 'fuzzy_ws'), axis=1)\n",
    "\n",
    "print(tfc_fuzzy_ws.shape)\n",
    "tfc_fuzzy_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFC - Fuzzy Extreme(1) WS\n",
    "\n",
    "f1 = metrics.f1_score(y_test, tfc_fuzzy_ws['Ext1_label'].to_numpy(), average='micro')\n",
    "tfcdt_ex1_ws  = [['TFC', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "tfcdt_ex1_ws = pd.DataFrame(data=tfcdt_ex1_ws)\n",
    "tfcdt_ex1_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "tfcdt_ex1_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFC - Fuzzy Extreme(2) WS\n",
    "\n",
    "f1 = metrics.f1_score(y_test, tfc_fuzzy_ws['Ext2_label'].to_numpy(), average='micro')\n",
    "tfcdt_ex2_ws  = [['TFC', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "tfcdt_ex2_ws = pd.DataFrame(data=tfcdt_ex2_ws)\n",
    "tfcdt_ex2_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "tfcdt_ex2_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fuzzy Neutral WS\n",
    "\n",
    "f1 = metrics.f1_score(y_test, tfc_fuzzy_ws['Neut_label'].to_numpy(), average='micro')\n",
    "tfcdt_neut_ws  = [['TFC', 'multi', 'F1_micro', f1]]\n",
    "\n",
    "tfcdt_neut_ws = pd.DataFrame(data=tfcdt_neut_ws)\n",
    "tfcdt_neut_ws.columns = ['Annotator','Model','Optimisation','F1_micro']\n",
    "tfcdt_neut_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ext Val (Extreme1): multi - Summary\n",
    "\n",
    "frames = [c1dt_ex1_ws, c2dt_ex1_ws, c3dt_ex1_ws, c4dt_ex1_ws, c5dt_ex1_ws, c6dt_ex1_ws, c7dt_ex1_ws, c8dt_ex1_ws,\n",
    "          c9dt_ex1_ws, c10dt_ex1_ws, c11dt_ex1_ws, mvdt_ex1_ws, tmvdt_ex1_ws, fcdt_ex1_ws, tfcdt_ex1_ws]\n",
    "\n",
    "multi_ex1_pred = pd.concat(frames)\n",
    "multi_ex1_pred['Annotator'] = multi_ex1_pred['Annotator'].str.upper()\n",
    "print(multi_ex1_pred.shape)\n",
    "multi_ex1_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ext Val (Extreme2): multi - Summary\n",
    "\n",
    "frames = [c1dt_ex2_ws, c2dt_ex2_ws, c3dt_ex2_ws, c4dt_ex2_ws, c5dt_ex2_ws, c6dt_ex2_ws, c7dt_ex2_ws, c8dt_ex2_ws,\n",
    "          c9dt_ex2_ws, c10dt_ex2_ws, c11dt_ex2_ws, mvdt_ex2_ws, tmvdt_ex2_ws, fcdt_ex2_ws, tfcdt_ex2_ws]\n",
    "\n",
    "multi_ex2_pred = pd.concat(frames)\n",
    "print(multi_ex2_pred.shape)\n",
    "multi_ex2_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Ext Val: Temporal Neutral - Summary\n",
    "\n",
    "frames = [c1dt_neut_ws, c2dt_neut_ws, c3dt_neut_ws, c4dt_neut_ws, c5dt_neut_ws, c6dt_neut_ws, c7dt_neut_ws, \n",
    "          c8dt_neut_ws, c9dt_neut_ws, c10dt_neut_ws, c11dt_neut_ws, mvdt_neut_ws,tmvdt_neut_ws, fcdt_neut_ws,\n",
    "          tfcdt_neut_ws]\n",
    "\n",
    "multi_neut_pred = pd.concat(frames)\n",
    "multi_neut_pred['Annotator'] = multi_neut_pred['Annotator'].str.upper()\n",
    "\n",
    "print(multi_neut_pred.shape)\n",
    "multi_neut_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row_fc = {'Annotator':'FC', 'Model':'NA', 'Optimisation':'NA', 'F1_micro':0}\n",
    "new_row_tfc = {'Annotator':'TFC', 'Model':'NA', 'Optimisation':'NA', 'F1_micro':0}\n",
    "\n",
    "multi_ext_stat = multi_ext_stat.append(new_row_fc, ignore_index=True)\n",
    "multi_ext_stat = multi_ext_stat.append(new_row_tfc, ignore_index=True)\n",
    "\n",
    "multi_ext_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACTION - remove scatterpot point for FC Static\n",
    "#Plot chart\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#Define x and y data\n",
    "x1 = multi_ex1_pred['Annotator']\n",
    "y1 = multi_ex1_pred['F1_micro']\n",
    "y2 = multi_neut_pred['F1_micro']\n",
    "y3 = multi_ext_stat['F1_micro']\n",
    "\n",
    "\n",
    "#Plot chart data\n",
    "plt.figure(figsize=(10,3.5))\n",
    "plt.plot(x1, y1, color='#1F57C8', marker='o', linestyle=\"solid\", label='Temporal: Extreme')\n",
    "plt.plot(x1, y2, color='#DA4802', marker='o', linestyle=\"solid\", label='Temporal: Neutral')\n",
    "plt.plot(x1, y3, color='#62B463', marker='o', linestyle=\"solid\", label='Static')\n",
    "\n",
    "plt.ylim([0.0,1.1])\n",
    "plt.yticks(np.arange(0.0,1.01, 0.2))\n",
    "\n",
    "#Add title and labels\n",
    "plt.title('HiRID External Validation Experiment 2: Static vs Temporal', fontsize=18)\n",
    "plt.xlabel('Annotator', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylabel('F1 micro', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(loc=1, fontsize=12.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot chart\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#Define x and y data\n",
    "x1 = multi_ex1_pred['Annotator']\n",
    "y1 = multi_ex1_pred['F1_micro']\n",
    "y2 = multi_ex2_pred['F1_micro']\n",
    "y3 = multi_neut_pred['F1_micro']\n",
    "y4 = multi_ext_stat['F1_micro']\n",
    "\n",
    "\n",
    "#Plot chart data\n",
    "plt.figure(figsize=(10,3.5))\n",
    "plt.plot(x1, y1, color='#1F57C8', marker='o', linestyle=\"solid\", label='Temporal: Extreme')\n",
    "plt.plot(x1, y2, color='#D4AF37', marker='o', linestyle=\"solid\", label='Temporal: Extreme (2)')\n",
    "plt.plot(x1, y3, color='#DA4802', marker='o', linestyle=\"solid\", label='Temporal: Neutral')\n",
    "plt.plot(x1, y4, color='#62B463', marker='o', linestyle=\"solid\", label='Static')\n",
    "\n",
    "\n",
    "plt.ylim([0.0,1.1])\n",
    "plt.yticks(np.arange(0.0,1.01, 0.2))\n",
    "\n",
    "#Add title and labels\n",
    "plt.title('HiRID External Validation: Static vs Temporal', fontsize=18)\n",
    "plt.xlabel('Annotator', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylabel('F1 micro', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(loc=1, fontsize=10)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Approach 2: Machine-Learning driven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Temporal HiRID Validation Dataset\n",
    "##See jupyter notebook 'npjDM-HiRID_ExtVal_Dataset' to see steps on creating the Temporal HiRID Validation Dataset\n",
    "\n",
    "hirid_val = pd.read_csv(\"hirid_extval_temporal_cohort.csv\")\n",
    "hirid_val = hirid_val.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "#generate binary discharge_status\n",
    "hirid_val['binary_discharge'] = np.where(hirid_val['discharge_status']== 'alive', 0, 1)\n",
    "hirid_val = hirid_val.sort_values(by='patientid',ascending=True)\n",
    "\n",
    "print(hirid_val.shape)\n",
    "print('Number of patients:', hirid_val.patientid.nunique())\n",
    "hirid_val.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1dt_multi_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DT multi Ext val - Annotator predictions\n",
    "\n",
    "hirid_val['c1_pred']= c1dt_multi_opt.predict(X_test)\n",
    "hirid_val['c2_pred']= c2dt_multi_opt.predict(X_test)\n",
    "hirid_val['c3_pred'] = c3dt_multi_opt.predict(X_test)\n",
    "hirid_val['c4_pred'] = c4dt_multi_opt.predict(X_test)\n",
    "hirid_val['c5_pred'] = c5dt_multi_opt.predict(X_test)\n",
    "hirid_val['c6_pred'] = c6dt_multi_opt.predict(X_test)\n",
    "hirid_val['c7_pred'] = c7dt_multi_opt.predict(X_test)\n",
    "hirid_val['c8_pred'] = c8dt_multi_opt.predict(X_test)\n",
    "hirid_val['c9_pred'] = c9dt_multi_opt.predict(X_test)\n",
    "hirid_val['c10_pred'] = c10dt_multi_opt.predict(X_test)\n",
    "hirid_val['c11_pred'] = c11dt_multi_opt.predict(X_test)\n",
    "hirid_val['mv_pred'] = mvdt_multi_opt.predict(X_test)\n",
    "hirid_val['tmv_pred'] = tmvdt_multi_opt.predict(X_test)\n",
    "\n",
    "print(hirid_val.shape)\n",
    "hirid_val.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hirid_val.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Import HiRID patient table (contains discharge_status info)\n",
    "pat = pd.read_sql_query(\"SELECT * FROM hirid.patient\", conn)\n",
    "pat = pat.drop(['admissiontime','sex','age'],axis=1)\n",
    "pat['binary_discharge'] = np.where(pat['discharge_status'] == 'alive', 0, 1)\n",
    "pat = pat.drop('discharge_status',axis=1)\n",
    "\n",
    "#Reformat c1 predictions\n",
    "c1_mlval = hirid_val.copy(deep=True)\n",
    "c1_mlval = c1_mlval[['patientid','Hrs before d_time','c1_pred']]\n",
    "c1_mlval = c1_mlval.groupby(['patientid','Hrs before d_time'])['c1_pred'].sum().unstack('Hrs before d_time')\n",
    "c1_mlval = c1_mlval.reset_index()\n",
    "c1_mlval = c1_mlval.rename_axis(None, axis=1)\n",
    "dict = {1 : 'c1_1hr_before', 2 : 'c1_2hrs_before', 3 : 'c1_3hrs_before', 4 : 'c1_4hrs_before', 5 : 'c1_5hrs_before'}\n",
    "c1_mlval.rename(columns=dict,inplace=True)\n",
    "c1_mlval = pd.merge(c1_mlval, pat, how='left', on='patientid')\n",
    "\n",
    "#Reformat c2 predictions\n",
    "c2_mlval = hirid_val.copy(deep=True)\n",
    "c2_mlval = c2_mlval[['patientid','Hrs before d_time','c2_pred']]\n",
    "c2_mlval = c2_mlval.groupby(['patientid','Hrs before d_time'])['c2_pred'].sum().unstack('Hrs before d_time')\n",
    "c2_mlval = c2_mlval.reset_index()\n",
    "c2_mlval = c2_mlval.rename_axis(None, axis=1)\n",
    "dict = {1 : 'c2_1hr_before', 2 : 'c2_2hrs_before', 3 : 'c2_3hrs_before', 4 : 'c2_4hrs_before', 5 : 'c2_5hrs_before'}\n",
    "c2_mlval.rename(columns=dict,inplace=True)\n",
    "c2_mlval = pd.merge(c2_mlval, pat, how='left', on='patientid')\n",
    "\n",
    "#Reformat c3 predictions\n",
    "c3_mlval = hirid_val.copy(deep=True)\n",
    "c3_mlval = c3_mlval[['patientid','Hrs before d_time','c3_pred']]\n",
    "c3_mlval = c3_mlval.groupby(['patientid','Hrs before d_time'])['c3_pred'].sum().unstack('Hrs before d_time')\n",
    "c3_mlval = c3_mlval.reset_index()\n",
    "c3_mlval = c3_mlval.rename_axis(None, axis=1)\n",
    "dict = {1 : 'c3_1hr_before', 2 : 'c3_2hrs_before', 3 : 'c3_3hrs_before', 4 : 'c3_4hrs_before', 5 : 'c3_5hrs_before'}\n",
    "c3_mlval.rename(columns=dict,inplace=True)\n",
    "c3_mlval = pd.merge(c3_mlval, pat, how='left', on='patientid')\n",
    "\n",
    "#Reformat c4 predictions\n",
    "c4_mlval = hirid_val.copy(deep=True)\n",
    "c4_mlval = c4_mlval[['patientid','Hrs before d_time','c4_pred']]\n",
    "c4_mlval = c4_mlval.groupby(['patientid','Hrs before d_time'])['c4_pred'].sum().unstack('Hrs before d_time')\n",
    "c4_mlval = c4_mlval.reset_index()\n",
    "c4_mlval = c4_mlval.rename_axis(None, axis=1)\n",
    "dict = {1 : 'c4_1hr_before', 2 : 'c4_2hrs_before', 3 : 'c4_3hrs_before', 4 : 'c4_4hrs_before', 5 : 'c4_5hrs_before'}\n",
    "c4_mlval.rename(columns=dict,inplace=True)\n",
    "c4_mlval = pd.merge(c4_mlval, pat, how='left', on='patientid')\n",
    "\n",
    "#Reformat c5 predictions\n",
    "c5_mlval = hirid_val.copy(deep=True)\n",
    "c5_mlval = c5_mlval[['patientid','Hrs before d_time','c5_pred']]\n",
    "c5_mlval = c5_mlval.groupby(['patientid','Hrs before d_time'])['c5_pred'].sum().unstack('Hrs before d_time')\n",
    "c5_mlval = c5_mlval.reset_index()\n",
    "c5_mlval = c5_mlval.rename_axis(None, axis=1)\n",
    "dict = {1 : 'c5_1hr_before', 2 : 'c5_2hrs_before', 3 : 'c5_3hrs_before', 4 : 'c5_4hrs_before', 5 : 'c5_5hrs_before'}\n",
    "c5_mlval.rename(columns=dict,inplace=True)\n",
    "c5_mlval = pd.merge(c5_mlval, pat, how='left', on='patientid')\n",
    "\n",
    "#Reformat c6 predictions\n",
    "c6_mlval = hirid_val.copy(deep=True)\n",
    "c6_mlval = c6_mlval[['patientid','Hrs before d_time','c6_pred']]\n",
    "c6_mlval = c6_mlval.groupby(['patientid','Hrs before d_time'])['c6_pred'].sum().unstack('Hrs before d_time')\n",
    "c6_mlval = c6_mlval.reset_index()\n",
    "c6_mlval = c6_mlval.rename_axis(None, axis=1)\n",
    "dict = {1 : 'c6_1hr_before', 2 : 'c6_2hrs_before', 3 : 'c6_3hrs_before', 4 : 'c6_4hrs_before', 5 : 'c6_5hrs_before'}\n",
    "c6_mlval.rename(columns=dict,inplace=True)\n",
    "c6_mlval = pd.merge(c6_mlval, pat, how='left', on='patientid')\n",
    "\n",
    "#Reformat c7 predictions\n",
    "c7_mlval = hirid_val.copy(deep=True)\n",
    "c7_mlval = c7_mlval[['patientid','Hrs before d_time','c7_pred']]\n",
    "c7_mlval = c7_mlval.groupby(['patientid','Hrs before d_time'])['c7_pred'].sum().unstack('Hrs before d_time')\n",
    "c7_mlval = c7_mlval.reset_index()\n",
    "c7_mlval = c7_mlval.rename_axis(None, axis=1)\n",
    "dict = {1 : 'c7_1hr_before', 2 : 'c7_2hrs_before', 3 : 'c7_3hrs_before', 4 : 'c7_4hrs_before', 5 : 'c7_5hrs_before'}\n",
    "c7_mlval.rename(columns=dict,inplace=True)\n",
    "c7_mlval = pd.merge(c7_mlval, pat, how='left', on='patientid')\n",
    "\n",
    "#Reformat c8 predictions\n",
    "c8_mlval = hirid_val.copy(deep=True)\n",
    "c8_mlval = c8_mlval[['patientid','Hrs before d_time','c8_pred']]\n",
    "c8_mlval = c8_mlval.groupby(['patientid','Hrs before d_time'])['c8_pred'].sum().unstack('Hrs before d_time')\n",
    "c8_mlval = c8_mlval.reset_index()\n",
    "c8_mlval = c8_mlval.rename_axis(None, axis=1)\n",
    "dict = {1 : 'c8_1hr_before', 2 : 'c8_2hrs_before', 3 : 'c8_3hrs_before', 4 : 'c8_4hrs_before', 5 : 'c8_5hrs_before'}\n",
    "c8_mlval.rename(columns=dict,inplace=True)\n",
    "c8_mlval = pd.merge(c8_mlval, pat, how='left', on='patientid')\n",
    "\n",
    "#Reformat c9 predictions\n",
    "c9_mlval = hirid_val.copy(deep=True)\n",
    "c9_mlval = c9_mlval[['patientid','Hrs before d_time','c9_pred']]\n",
    "c9_mlval = c9_mlval.groupby(['patientid','Hrs before d_time'])['c9_pred'].sum().unstack('Hrs before d_time')\n",
    "c9_mlval = c9_mlval.reset_index()\n",
    "c9_mlval = c9_mlval.rename_axis(None, axis=1)\n",
    "dict = {1 : 'c9_1hr_before', 2 : 'c9_2hrs_before', 3 : 'c9_3hrs_before', 4 : 'c9_4hrs_before', 5 : 'c9_5hrs_before'}\n",
    "c9_mlval.rename(columns=dict,inplace=True)\n",
    "c9_mlval = pd.merge(c9_mlval, pat, how='left', on='patientid')\n",
    "\n",
    "#Reformat c10 predictions\n",
    "c10_mlval = hirid_val.copy(deep=True)\n",
    "c10_mlval = c10_mlval[['patientid','Hrs before d_time','c10_pred']]\n",
    "c10_mlval = c10_mlval.groupby(['patientid','Hrs before d_time'])['c10_pred'].sum().unstack('Hrs before d_time')\n",
    "c10_mlval = c10_mlval.reset_index()\n",
    "c10_mlval = c10_mlval.rename_axis(None, axis=1)\n",
    "dict = {1 : 'c10_1hr_before', 2 : 'c10_2hrs_before', 3 : 'c10_3hrs_before', 4 : 'c10_4hrs_before', 5 : 'c10_5hrs_before'}\n",
    "c10_mlval.rename(columns=dict,inplace=True)\n",
    "c10_mlval = pd.merge(c10_mlval, pat, how='left', on='patientid')\n",
    "\n",
    "#Reformat c11 predictions\n",
    "c11_mlval = hirid_val.copy(deep=True)\n",
    "c11_mlval = c11_mlval[['patientid','Hrs before d_time','c11_pred']]\n",
    "c11_mlval = c11_mlval.groupby(['patientid','Hrs before d_time'])['c11_pred'].sum().unstack('Hrs before d_time')\n",
    "c11_mlval = c11_mlval.reset_index()\n",
    "c11_mlval = c11_mlval.rename_axis(None, axis=1)\n",
    "dict = {1 : 'c11_1hr_before', 2 : 'c11_2hrs_before', 3 : 'c11_3hrs_before', 4 : 'c11_4hrs_before', 5 : 'c11_5hrs_before'}\n",
    "c11_mlval.rename(columns=dict,inplace=True)\n",
    "c11_mlval = pd.merge(c11_mlval, pat, how='left', on='patientid')\n",
    "\n",
    "#Reformat mv predictions\n",
    "mv_mlval = hirid_val.copy(deep=True)\n",
    "mv_mlval = mv_mlval[['patientid','Hrs before d_time','mv_pred']]\n",
    "mv_mlval = mv_mlval.groupby(['patientid','Hrs before d_time'])['mv_pred'].sum().unstack('Hrs before d_time')\n",
    "mv_mlval = mv_mlval.reset_index()\n",
    "mv_mlval = mv_mlval.rename_axis(None, axis=1)\n",
    "dict = {1 : 'mv_1hr_before', 2 : 'mv_2hrs_before', 3 : 'mv_3hrs_before', 4 : 'mv_4hrs_before', 5 : 'mv_5hrs_before'}\n",
    "mv_mlval.rename(columns=dict,inplace=True)\n",
    "mv_mlval = pd.merge(mv_mlval, pat, how='left', on='patientid')\n",
    "\n",
    "#Reformat tmv predictions\n",
    "tmv_mlval = hirid_val.copy(deep=True)\n",
    "tmv_mlval = tmv_mlval[['patientid','Hrs before d_time','tmv_pred']]\n",
    "tmv_mlval = tmv_mlval.groupby(['patientid','Hrs before d_time'])['tmv_pred'].sum().unstack('Hrs before d_time')\n",
    "tmv_mlval = tmv_mlval.reset_index()\n",
    "tmv_mlval = tmv_mlval.rename_axis(None, axis=1)\n",
    "dict = {1 : 'tmv_1hr_before', 2 : 'tmv_2hrs_before', 3 : 'tmv_3hrs_before', 4 : 'tmv_4hrs_before', 5 : 'tmv_5hrs_before'}\n",
    "tmv_mlval.rename(columns=dict,inplace=True)\n",
    "tmv_mlval = pd.merge(tmv_mlval, pat, how='left', on='patientid')\n",
    "\n",
    "c1_mlval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DT - C1\n",
    "\n",
    "array = c1_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float)  \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c1_dt = do_cv_learning_dt(X,y)\n",
    "c1_dt['Annotator'] = 'c1'\n",
    "\n",
    "#Opt model\n",
    "c1_opt = model_opt_dt(X,y)\n",
    "c1_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot decision tree - C1\n",
    "\n",
    "pred_cols = ['1hr_before', '2hrs_before','3hrs_before','4hrs_before','5hrs_before']\n",
    "\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "plot_tree(c1_opt, \n",
    "          fontsize=10,\n",
    "          filled=True, \n",
    "          rounded=True, \n",
    "          feature_names=pred_cols);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DT - C2\n",
    "\n",
    "array = c2_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c2_dt = do_cv_learning_dt(X,y)\n",
    "c2_dt['Annotator'] = 'c2'\n",
    "\n",
    "#Opt model\n",
    "c2_opt = model_opt_dt(X,y)\n",
    "\n",
    "c2_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot decision tree - C2\n",
    "\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "plot_tree(c2_opt, \n",
    "          fontsize=10,\n",
    "          filled=True, \n",
    "          rounded=True, \n",
    "          feature_names=pred_cols);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DT - C3\n",
    "\n",
    "array = c3_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c3_dt = do_cv_learning_dt(X,y)\n",
    "c3_dt['Annotator'] = 'c3'\n",
    "\n",
    "#Opt model\n",
    "c3_opt = model_opt_dt(X,y)\n",
    "\n",
    "c3_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot decision tree - C3\n",
    "\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "plot_tree(c3_opt, \n",
    "          fontsize=10,\n",
    "          filled=True, \n",
    "          rounded=True, \n",
    "          feature_names=pred_cols);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DT - C4\n",
    "\n",
    "array = c4_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c4_dt = do_cv_learning_dt(X,y)\n",
    "c4_dt['Annotator'] = 'c4'\n",
    "\n",
    "#Opt model\n",
    "c4_opt = model_opt_dt(X,y)\n",
    "\n",
    "c4_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot decision tree - C4\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(c4_opt, \n",
    "          fontsize=10,\n",
    "          filled=True, \n",
    "          rounded=True, \n",
    "          feature_names=pred_cols);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DT - C5\n",
    "\n",
    "array = c5_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c5_dt = do_cv_learning_dt(X,y)\n",
    "c5_dt['Annotator'] = 'c5'\n",
    "\n",
    "#Opt model\n",
    "c5_opt = model_opt_dt(X,y)\n",
    "\n",
    "c5_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot decision tree - C5\n",
    "\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "plot_tree(c5_opt, \n",
    "          filled=True, \n",
    "          rounded=True, \n",
    "          feature_names=pred_cols);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DT - C6\n",
    "\n",
    "array = c6_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c6_dt = do_cv_learning_dt(X,y)\n",
    "c6_dt['Annotator'] = 'c6'\n",
    "\n",
    "#Opt model\n",
    "c6_opt = model_opt_dt(X,y)\n",
    "\n",
    "c6_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot decision tree- C6\n",
    "\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "plot_tree(c6_opt, \n",
    "          fontsize=10,\n",
    "          filled=True, \n",
    "          rounded=True, \n",
    "          feature_names=pred_cols);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DT - C7\n",
    "\n",
    "array = c7_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c7_dt = do_cv_learning_dt(X,y)\n",
    "c7_dt['Annotator'] = 'c7'\n",
    "\n",
    "#Opt model\n",
    "c7_opt = model_opt_dt(X,y)\n",
    "\n",
    "c7_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot decision tree - C7\n",
    "\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "plot_tree(c7_opt, \n",
    "          filled=True, \n",
    "          rounded=True, \n",
    "          feature_names=pred_cols);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DT - C8\n",
    "\n",
    "array = c8_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c8_dt = do_cv_learning_dt(X,y)\n",
    "c8_dt['Annotator'] = 'c8'\n",
    "\n",
    "#Opt model\n",
    "c8_opt = model_opt_dt(X,y)\n",
    "\n",
    "c8_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot decision tree - C8\n",
    "\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "plot_tree(c8_opt, \n",
    "          fontsize=10,\n",
    "          filled=True, \n",
    "          rounded=True, \n",
    "          feature_names=pred_cols);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DT - C9\n",
    "\n",
    "array = c9_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c9_dt = do_cv_learning_dt(X,y)\n",
    "c9_dt['Annotator'] = 'c9'\n",
    "\n",
    "#Opt model\n",
    "c9_opt = model_opt_dt(X,y)\n",
    "\n",
    "c9_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot decision tree - C9\n",
    "\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "plot_tree(c9_opt, \n",
    "          filled=True, \n",
    "          rounded=True, \n",
    "          feature_names=pred_cols);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DT - C10\n",
    "\n",
    "array = c10_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c10_dt = do_cv_learning_dt(X,y)\n",
    "c10_dt['Annotator'] = 'c10'\n",
    "\n",
    "#Opt model\n",
    "c10_opt = model_opt_dt(X,y)\n",
    "\n",
    "c10_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot decision tree - C10\n",
    "\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "plot_tree(c10_opt, \n",
    "          fontsize=10,\n",
    "          filled=True, \n",
    "          rounded=True, \n",
    "          feature_names=pred_cols);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DT - C11\n",
    "\n",
    "array = c11_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "c11_dt = do_cv_learning_dt(X,y)\n",
    "c11_dt['Annotator'] = 'c11'\n",
    "\n",
    "#Opt model\n",
    "c11_opt = model_opt_dt(X,y)\n",
    "\n",
    "c11_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot decision tree - C11\n",
    "\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "plot_tree(c11_opt, \n",
    "          fontsize=10,\n",
    "          filled=True, \n",
    "          rounded=True, \n",
    "          feature_names=pred_cols);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DT - MV\n",
    "\n",
    "array = mv_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "mv_dt = do_cv_learning_dt(X,y)\n",
    "mv_dt['Annotator'] = 'MV'\n",
    "\n",
    "#Opt model\n",
    "mv_opt = model_opt_dt(X,y)\n",
    "\n",
    "mv_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot decision tree - MV\n",
    "\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "plot_tree(mv_opt, \n",
    "          fontsize=10,\n",
    "          filled=True, \n",
    "          rounded=True, \n",
    "          feature_names=pred_cols);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DT - TMV\n",
    "\n",
    "array = tmv_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#5-fold CV Model Eval\n",
    "tmv_dt = do_cv_learning_dt(X,y)\n",
    "tmv_dt['Annotator'] = 'TMV'\n",
    "\n",
    "#Opt model\n",
    "tmv_opt = model_opt_dt(X,y)\n",
    "\n",
    "tmv_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot decision tree - TMV\n",
    "\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "plot_tree(tmv_opt, \n",
    "          fontsize=10,\n",
    "          filled=True, \n",
    "          rounded=True, \n",
    "          feature_names=pred_cols);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ext Validation (DT) - Performance Summary\n",
    "\n",
    "frames = [c1_dt, c2_dt, c3_dt, c4_dt, c5_dt, c6_dt, c7_dt, c8_dt, c9_dt, c10_dt, c11_dt, mv_dt, tmv_dt]\n",
    "\n",
    "multi_dt = pd.concat(frames)\n",
    "\n",
    "print(multi_dt.shape)\n",
    "multi_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot chart - Static vs Temporal Ext Val\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#Define x and y data\n",
    "x1 = multi_ext_stat['Annotator']\n",
    "y1 = multi_ex2_pred['F1_micro']\n",
    "y2 = multi_neut_pred['F1_micro']\n",
    "y3 = multi_ext_stat['F1_micro']\n",
    "y4 = multi_dt['F1_micro']\n",
    "\n",
    "#Plot chart data\n",
    "plt.figure(figsize=(8,2.5))\n",
    "plt.plot(x1, y1, color='#1F57C8', marker='o', linestyle=\"solid\", label='Temporal WS')\n",
    "plt.plot(x1, y4, color='#9467BD', marker='o', linestyle=\"solid\", label='Temporal ML (DT)')\n",
    "plt.plot(x1, y3, color='#62B463', marker='o', linestyle=\"solid\", label='Static')\n",
    "\n",
    "plt.ylim([0.0,1.1])\n",
    "plt.yticks(np.arange(0.0,1.01, 0.2))\n",
    "\n",
    "#Add title and labels\n",
    "plt.title('HiRID External Validation: Static vs Temporal', fontsize=14)\n",
    "plt.xlabel('Annotator', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylabel('F1_micro', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(bbox_to_anchor=(1, 1), fontsize=10)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot chart - Static vs Temporal Ext Val\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#Define x and y data\n",
    "x1 = multi_ext_stat['Annotator']\n",
    "y1 = multi_ex2_pred['F1_micro']\n",
    "y2 = multi_neut_pred['F1_micro']\n",
    "y3 = multi_ext_stat['F1_micro']\n",
    "y4 = multi_dt['F1_micro']\n",
    "\n",
    "#Plot chart data\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(x1, y1, color='#1F57C8', marker='o', linestyle=\"solid\", label='Temporal WS (Extreme)')\n",
    "plt.plot(x1, y2, color='#DA4802', marker='o', linestyle=\"solid\", label='Temporal WS (Neutral)')\n",
    "plt.plot(x1, y4, color='#9467BD', marker='o', linestyle=\"solid\", label='Temporal DT')\n",
    "plt.plot(x1, y3, color='#62B463', marker='o', linestyle=\"solid\", label='Static')\n",
    "\n",
    "plt.ylim([0.0,1.1])\n",
    "plt.yticks(np.arange(0.0,1.01, 0.2))\n",
    "\n",
    "#Add title and labels\n",
    "plt.title('HiRID External Validation: Static vs Temporal', fontsize=14)\n",
    "plt.xlabel('Annotator', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylabel('F1_micro', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(bbox_to_anchor=(1, 0.5), fontsize=10)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Parameter Grid for hyperparameter optimisation\n",
    "##Create a dictionary with all LR parameter options \n",
    "\n",
    "params_lr = {'penalty': ['l2'],\n",
    "             'C': [100, 10, 1.0, 0.1, 0.01]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Function - LR Model Evaluation via 5-fold CV\n",
    "\n",
    "def do_cv_learning_lr(X, y, verbose=False, do_scale=False, random_state=1):\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    f1s = []\n",
    "\n",
    "    if do_scale:\n",
    "        sc = StandardScaler()\n",
    "        X = sc.fit_transform(X)\n",
    "        \n",
    "    for i, (train,test) in enumerate(cv.split(X,y)):\n",
    "        gcsv = GridSearchCV(LogisticRegression(random_state=1), \n",
    "                            param_grid=params_lr, \n",
    "                            cv=5, \n",
    "                            scoring='f1_micro')\n",
    "        grid_result = gcsv.fit(X[train],y[train])\n",
    "        best_params = grid_result.best_params_\n",
    "        if verbose:\n",
    "            print('fold', i,'best_params', best_params)\n",
    "        clf = grid_result.best_estimator_\n",
    "        f1 = metrics.f1_score(y[test], clf.predict(X[test]), average='micro')\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    ##Performance metrics \n",
    "    dflr_multi_f1data = [['ann', 'multi', 'F1_micro', np.mean(f1s), np.std(f1s)]]\n",
    "\n",
    "    ##print data as DF\n",
    "    dflr_multi_f1data = pd.DataFrame(data=dflr_multi_f1data)\n",
    "    dflr_multi_f1data.columns = ['Annotator','Model','Optimisation','F1_micro','S.D.']\n",
    "    \n",
    "    return dflr_multi_f1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Function - LR Odds Ratios\n",
    "\n",
    "def or_lr(X, y, verbose=False, do_scale=False, random_state=1):\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    ORs = []\n",
    "\n",
    "    if do_scale:\n",
    "        sc = StandardScaler()\n",
    "        X = sc.fit_transform(X)\n",
    "        \n",
    "    for i, (train,test) in enumerate(cv.split(X,y)):\n",
    "        gcsv = GridSearchCV(LogisticRegression(random_state=1), \n",
    "                            param_grid=params_lr, \n",
    "                            cv=5, \n",
    "                            scoring='f1_micro')\n",
    "        grid_result = gcsv.fit(X[train],y[train])\n",
    "        best_params = grid_result.best_params_\n",
    "        if verbose:\n",
    "            print('fold', i,'best_params', best_params)\n",
    "        clf = grid_result.best_estimator_\n",
    "        if hasattr(clf, 'coef_'):\n",
    "            ORs.append([math.exp(c) for c in clf.coef_[0]])\n",
    "    \n",
    "    return ORs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_mlval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR - C1\n",
    "\n",
    "array = c1_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#Model Output\n",
    "c1_lr = do_cv_learning_lr(X,y)\n",
    "c1_lr['Annotator'] = 'C1'\n",
    "\n",
    "#Odds ratios\n",
    "c1_or = or_lr(X,y)\n",
    "\n",
    "c1_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find avg ORs per hour - C1\n",
    "\n",
    "c1_1hr = (c1_or[0][0] + c1_or[1][0] + c1_or[2][0] +  c1_or[3][0] + c1_or[4][0])/5\n",
    "c1_2hr = (c1_or[0][1] + c1_or[1][1] + c1_or[2][1] + c1_or[3][1] + c1_or[4][1])/5\n",
    "c1_3hr = (c1_or[0][2] + c1_or[1][2] + c1_or[2][2] + c1_or[3][2] + c1_or[4][2])/5\n",
    "c1_4hr = (c1_or[0][3] + c1_or[1][3] + c1_or[2][3] + c1_or[3][3] + c1_or[4][3])/5\n",
    "c1_5hr = (c1_or[0][4] + c1_or[1][4] + c1_or[2][4] + c1_or[3][4] + c1_or[4][4])/5\n",
    "\n",
    "c1_or_avg = [c1_1hr, c1_2hr, c1_3hr, c1_4hr, c1_5hr]\n",
    "\n",
    "#Display as DF\n",
    "feature_names = ['c1_1hr_before','c1_2hrs_before','c1_3hrs_before','c1_4hrs_before','c1_5hrs_before']\n",
    "\n",
    "c1_or_table = pd.DataFrame(feature_names, columns = ['Feature'])\n",
    "c1_or_table['C1'] = c1_or_avg\n",
    "c1_or_table['Feature'] = c1_or_table[\"Feature\"].str[3:]\n",
    "c1_or_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR - c2\n",
    "\n",
    "array = c2_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#Model Output\n",
    "c2_lr = do_cv_learning_lr(X,y)\n",
    "c2_lr['Annotator'] = 'C2'\n",
    "\n",
    "#Odds ratios\n",
    "c2_or = or_lr(X,y)\n",
    "\n",
    "c2_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find avg ORs per hour - C2\n",
    "\n",
    "c2_1hr = (c2_or[0][0] + c2_or[1][0] + c2_or[2][0] +  c2_or[3][0] + c2_or[4][0])/5\n",
    "c2_2hr = (c2_or[0][1] + c2_or[1][1] + c2_or[2][1] + c2_or[3][1] + c2_or[4][1])/5\n",
    "c2_3hr = (c2_or[0][2] + c2_or[1][2] + c2_or[2][2] + c2_or[3][2] + c2_or[4][2])/5\n",
    "c2_4hr = (c2_or[0][3] + c2_or[1][3] + c2_or[2][3] + c2_or[3][3] + c2_or[4][3])/5\n",
    "c2_5hr = (c2_or[0][4] + c2_or[1][4] + c2_or[2][4] + c2_or[3][4] + c2_or[4][4])/5\n",
    "\n",
    "c2_or_avg = [c2_1hr, c2_2hr, c2_3hr, c2_4hr, c2_5hr]\n",
    "\n",
    "#Display as DF\n",
    "feature_names = ['c2_1hr_before','c2_2hrs_before','c2_3hrs_before','c2_4hrs_before','c2_5hrs_before']\n",
    "\n",
    "c2_or_table = pd.DataFrame(feature_names, columns = ['Feature'])\n",
    "c2_or_table['C2'] = c2_or_avg\n",
    "c2_or_table['Feature'] = c2_or_table[\"Feature\"].str[3:]\n",
    "c2_or_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR - C3\n",
    "\n",
    "array = c3_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#Model Output\n",
    "c3_lr = do_cv_learning_lr(X,y)\n",
    "c3_lr['Annotator'] = 'C3'\n",
    "\n",
    "#Odds ratios\n",
    "c3_or = or_lr(X,y)\n",
    "\n",
    "c3_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find avg ORs per hour - C3\n",
    "\n",
    "c3_1hr = (c3_or[0][0] + c3_or[1][0] + c3_or[2][0] +  c3_or[3][0] + c3_or[4][0])/5\n",
    "c3_2hr = (c3_or[0][1] + c3_or[1][1] + c3_or[2][1] + c3_or[3][1] + c3_or[4][1])/5\n",
    "c3_3hr = (c3_or[0][2] + c3_or[1][2] + c3_or[2][2] + c3_or[3][2] + c3_or[4][2])/5\n",
    "c3_4hr = (c3_or[0][3] + c3_or[1][3] + c3_or[2][3] + c3_or[3][3] + c3_or[4][3])/5\n",
    "c3_5hr = (c3_or[0][4] + c3_or[1][4] + c3_or[2][4] + c3_or[3][4] + c3_or[4][4])/5\n",
    "\n",
    "c3_or_avg = [c3_1hr, c3_2hr, c3_3hr, c3_4hr, c3_5hr]\n",
    "\n",
    "#Display as DF\n",
    "feature_names = ['c3_1hr_before','c3_2hrs_before','c3_3hrs_before','c3_4hrs_before','c3_5hrs_before']\n",
    "\n",
    "c3_or_table = pd.DataFrame(feature_names, columns = ['Feature'])\n",
    "c3_or_table['C3'] = c3_or_avg\n",
    "c3_or_table['Feature'] = c3_or_table[\"Feature\"].str[3:]\n",
    "c3_or_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR - C4\n",
    "\n",
    "array = c4_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#Model Output\n",
    "c4_lr = do_cv_learning_lr(X,y)\n",
    "c4_lr['Annotator'] = 'C4'\n",
    "\n",
    "#Odds ratios\n",
    "c4_or = or_lr(X,y)\n",
    "\n",
    "c4_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find avg ORs per hour - C4\n",
    "\n",
    "c4_1hr = (c4_or[0][0] + c4_or[1][0] + c4_or[2][0] +  c4_or[3][0] + c4_or[4][0])/5\n",
    "c4_2hr = (c4_or[0][1] + c4_or[1][1] + c4_or[2][1] + c4_or[3][1] + c4_or[4][1])/5\n",
    "c4_3hr = (c4_or[0][2] + c4_or[1][2] + c4_or[2][2] + c4_or[3][2] + c4_or[4][2])/5\n",
    "c4_4hr = (c4_or[0][3] + c4_or[1][3] + c4_or[2][3] + c4_or[3][3] + c4_or[4][3])/5\n",
    "c4_5hr = (c4_or[0][4] + c4_or[1][4] + c4_or[2][4] + c4_or[3][4] + c4_or[4][4])/5\n",
    "\n",
    "c4_or_avg = [c4_1hr, c4_2hr, c4_3hr, c4_4hr, c4_5hr]\n",
    "\n",
    "#Display as DF\n",
    "feature_names = ['c4_1hr_before','c4_2hrs_before','c4_3hrs_before','c4_4hrs_before','c4_5hrs_before']\n",
    "\n",
    "c4_or_table = pd.DataFrame(feature_names, columns = ['Feature'])\n",
    "c4_or_table['C4'] = c4_or_avg\n",
    "c4_or_table['Feature'] = c4_or_table[\"Feature\"].str[3:]\n",
    "c4_or_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR - C5\n",
    "\n",
    "array = c5_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#Model Output\n",
    "c5_lr = do_cv_learning_lr(X,y)\n",
    "c5_lr['Annotator'] = 'C5'\n",
    "\n",
    "#Odds ratios\n",
    "c5_or = or_lr(X,y)\n",
    "\n",
    "c5_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find avg ORs per hour - C5\n",
    "\n",
    "c5_1hr = (c5_or[0][0] + c5_or[1][0] + c5_or[2][0] +  c5_or[3][0] + c5_or[4][0])/5\n",
    "c5_2hr = (c5_or[0][1] + c5_or[1][1] + c5_or[2][1] + c5_or[3][1] + c5_or[4][1])/5\n",
    "c5_3hr = (c5_or[0][2] + c5_or[1][2] + c5_or[2][2] + c5_or[3][2] + c5_or[4][2])/5\n",
    "c5_4hr = (c5_or[0][3] + c5_or[1][3] + c5_or[2][3] + c5_or[3][3] + c5_or[4][3])/5\n",
    "c5_5hr = (c5_or[0][4] + c5_or[1][4] + c5_or[2][4] + c5_or[3][4] + c5_or[4][4])/5\n",
    "\n",
    "c5_or_avg = [c5_1hr, c5_2hr, c5_3hr, c5_4hr, c5_5hr]\n",
    "\n",
    "#Display as DF\n",
    "feature_names = ['c5_1hr_before','c5_2hrs_before','c5_3hrs_before','c5_4hrs_before','c5_5hrs_before']\n",
    "\n",
    "c5_or_table = pd.DataFrame(feature_names, columns = ['Feature'])\n",
    "c5_or_table['C5'] = c5_or_avg\n",
    "c5_or_table['Feature'] = c5_or_table[\"Feature\"].str[3:]\n",
    "c5_or_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR - C6\n",
    "\n",
    "array = c6_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#Model Output\n",
    "c6_lr = do_cv_learning_lr(X,y)\n",
    "c6_lr['Annotator'] = 'C6'\n",
    "\n",
    "#Odds ratios\n",
    "c6_or = or_lr(X,y)\n",
    "\n",
    "c6_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find avg ORs per hour - C6\n",
    "\n",
    "c6_1hr = (c6_or[0][0] + c6_or[1][0] + c6_or[2][0] +  c6_or[3][0] + c6_or[4][0])/5\n",
    "c6_2hr = (c6_or[0][1] + c6_or[1][1] + c6_or[2][1] + c6_or[3][1] + c6_or[4][1])/5\n",
    "c6_3hr = (c6_or[0][2] + c6_or[1][2] + c6_or[2][2] + c6_or[3][2] + c6_or[4][2])/5\n",
    "c6_4hr = (c6_or[0][3] + c6_or[1][3] + c6_or[2][3] + c6_or[3][3] + c6_or[4][3])/5\n",
    "c6_5hr = (c6_or[0][4] + c6_or[1][4] + c6_or[2][4] + c6_or[3][4] + c6_or[4][4])/5\n",
    "\n",
    "c6_or_avg = [c6_1hr, c6_2hr, c6_3hr, c6_4hr, c6_5hr]\n",
    "\n",
    "#Display as DF\n",
    "feature_names = ['c6_1hr_before','c6_2hrs_before','c6_3hrs_before','c6_4hrs_before','c6_5hrs_before']\n",
    "\n",
    "c6_or_table = pd.DataFrame(feature_names, columns = ['Feature'])\n",
    "c6_or_table['C6'] = c6_or_avg\n",
    "c6_or_table['Feature'] = c6_or_table[\"Feature\"].str[3:]\n",
    "c6_or_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR - C7\n",
    "\n",
    "array = c7_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#Model Output\n",
    "c7_lr = do_cv_learning_lr(X,y)\n",
    "c7_lr['Annotator'] = 'C7'\n",
    "\n",
    "#Odds ratios\n",
    "c7_or = or_lr(X,y)\n",
    "\n",
    "c7_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find avg ORs per hour - C7\n",
    "\n",
    "c7_1hr = (c7_or[0][0] + c7_or[1][0] + c7_or[2][0] +  c7_or[3][0] + c7_or[4][0])/5\n",
    "c7_2hr = (c7_or[0][1] + c7_or[1][1] + c7_or[2][1] + c7_or[3][1] + c7_or[4][1])/5\n",
    "c7_3hr = (c7_or[0][2] + c7_or[1][2] + c7_or[2][2] + c7_or[3][2] + c7_or[4][2])/5\n",
    "c7_4hr = (c7_or[0][3] + c7_or[1][3] + c7_or[2][3] + c7_or[3][3] + c7_or[4][3])/5\n",
    "c7_5hr = (c7_or[0][4] + c7_or[1][4] + c7_or[2][4] + c7_or[3][4] + c7_or[4][4])/5\n",
    "\n",
    "c7_or_avg = [c7_1hr, c7_2hr, c7_3hr, c7_4hr, c7_5hr]\n",
    "\n",
    "#Display as DF\n",
    "feature_names = ['c7_1hr_before','c7_2hrs_before','c7_3hrs_before','c7_4hrs_before','c7_5hrs_before']\n",
    "\n",
    "c7_or_table = pd.DataFrame(feature_names, columns = ['Feature'])\n",
    "c7_or_table['C7'] = c7_or_avg\n",
    "c7_or_table['Feature'] = c7_or_table[\"Feature\"].str[3:]\n",
    "c7_or_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR - C8\n",
    "\n",
    "array = c8_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#Model Output\n",
    "c8_lr = do_cv_learning_lr(X,y)\n",
    "c8_lr['Annotator'] = 'C8'\n",
    "\n",
    "#Odds ratios\n",
    "c8_or = or_lr(X,y)\n",
    "\n",
    "c8_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find avg ORs per hour - C8\n",
    "\n",
    "c8_1hr = (c8_or[0][0] + c8_or[1][0] + c8_or[2][0] +  c8_or[3][0] + c8_or[4][0])/5\n",
    "c8_2hr = (c8_or[0][1] + c8_or[1][1] + c8_or[2][1] + c8_or[3][1] + c8_or[4][1])/5\n",
    "c8_3hr = (c8_or[0][2] + c8_or[1][2] + c8_or[2][2] + c8_or[3][2] + c8_or[4][2])/5\n",
    "c8_4hr = (c8_or[0][3] + c8_or[1][3] + c8_or[2][3] + c8_or[3][3] + c8_or[4][3])/5\n",
    "c8_5hr = (c8_or[0][4] + c8_or[1][4] + c8_or[2][4] + c8_or[3][4] + c8_or[4][4])/5\n",
    "\n",
    "c8_or_avg = [c8_1hr, c8_2hr, c8_3hr, c8_4hr, c8_5hr]\n",
    "\n",
    "#Display as DF\n",
    "feature_names = ['c8_1hr_before','c8_2hrs_before','c8_3hrs_before','c8_4hrs_before','c8_5hrs_before']\n",
    "\n",
    "c8_or_table = pd.DataFrame(feature_names, columns = ['Feature'])\n",
    "c8_or_table['C8'] = c8_or_avg\n",
    "c8_or_table['Feature'] = c8_or_table[\"Feature\"].str[3:]\n",
    "c8_or_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR - C9\n",
    "\n",
    "array = c9_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#Model Output\n",
    "c9_lr = do_cv_learning_lr(X,y)\n",
    "c9_lr['Annotator'] = 'C9'\n",
    "\n",
    "#Odds ratios\n",
    "c9_or = or_lr(X,y)\n",
    "\n",
    "c9_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find avg ORs per hour - C9\n",
    "\n",
    "c9_1hr = (c9_or[0][0] + c9_or[1][0] + c9_or[2][0] +  c9_or[3][0] + c9_or[4][0])/5\n",
    "c9_2hr = (c9_or[0][1] + c9_or[1][1] + c9_or[2][1] + c9_or[3][1] + c9_or[4][1])/5\n",
    "c9_3hr = (c9_or[0][2] + c9_or[1][2] + c9_or[2][2] + c9_or[3][2] + c9_or[4][2])/5\n",
    "c9_4hr = (c9_or[0][3] + c9_or[1][3] + c9_or[2][3] + c9_or[3][3] + c9_or[4][3])/5\n",
    "c9_5hr = (c9_or[0][4] + c9_or[1][4] + c9_or[2][4] + c9_or[3][4] + c9_or[4][4])/5\n",
    "\n",
    "c9_or_avg = [c9_1hr, c9_2hr, c9_3hr, c9_4hr, c9_5hr]\n",
    "\n",
    "#Display as DF\n",
    "feature_names = ['c9_1hr_before','c9_2hrs_before','c9_3hrs_before','c9_4hrs_before','c9_5hrs_before']\n",
    "\n",
    "c9_or_table = pd.DataFrame(feature_names, columns = ['Feature'])\n",
    "c9_or_table['C9'] = c9_or_avg\n",
    "c9_or_table['Feature'] = c9_or_table[\"Feature\"].str[3:]\n",
    "c9_or_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR - C10\n",
    "\n",
    "array = c10_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#Model Output\n",
    "c10_lr = do_cv_learning_lr(X,y)\n",
    "c10_lr['Annotator'] = 'C10'\n",
    "\n",
    "#Odds ratios\n",
    "c10_or = or_lr(X,y)\n",
    "\n",
    "c10_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find avg ORs per hour - C10\n",
    "\n",
    "c10_1hr = (c10_or[0][0] + c10_or[1][0] + c10_or[2][0] +  c10_or[3][0] + c10_or[4][0])/5\n",
    "c10_2hr = (c10_or[0][1] + c10_or[1][1] + c10_or[2][1] + c10_or[3][1] + c10_or[4][1])/5\n",
    "c10_3hr = (c10_or[0][2] + c10_or[1][2] + c10_or[2][2] + c10_or[3][2] + c10_or[4][2])/5\n",
    "c10_4hr = (c10_or[0][3] + c10_or[1][3] + c10_or[2][3] + c10_or[3][3] + c10_or[4][3])/5\n",
    "c10_5hr = (c10_or[0][4] + c10_or[1][4] + c10_or[2][4] + c10_or[3][4] + c10_or[4][4])/5\n",
    "\n",
    "c10_or_avg = [c10_1hr, c10_2hr, c10_3hr, c10_4hr, c10_5hr]\n",
    "\n",
    "#Display as DF\n",
    "feature_names = ['c10_1hr_before','c10_2hrs_before','c10_3hrs_before','c10_4hrs_before','c10_5hrs_before']\n",
    "\n",
    "c10_or_table = pd.DataFrame(feature_names, columns = ['Feature'])\n",
    "c10_or_table['C10'] = c10_or_avg\n",
    "c10_or_table['Feature'] = c10_or_table[\"Feature\"].str[4:]\n",
    "c10_or_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR - C11\n",
    "\n",
    "array = c11_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#Model Output\n",
    "c11_lr = do_cv_learning_lr(X,y)\n",
    "c11_lr['Annotator'] = 'C11'\n",
    "\n",
    "#Odds ratios\n",
    "c11_or = or_lr(X,y)\n",
    "\n",
    "c11_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find avg ORs per hour - C11\n",
    "\n",
    "c11_1hr = (c11_or[0][0] + c11_or[1][0] + c11_or[2][0] +  c11_or[3][0] + c11_or[4][0])/5\n",
    "c11_2hr = (c11_or[0][1] + c11_or[1][1] + c11_or[2][1] + c11_or[3][1] + c11_or[4][1])/5\n",
    "c11_3hr = (c11_or[0][2] + c11_or[1][2] + c11_or[2][2] + c11_or[3][2] + c11_or[4][2])/5\n",
    "c11_4hr = (c11_or[0][3] + c11_or[1][3] + c11_or[2][3] + c11_or[3][3] + c11_or[4][3])/5\n",
    "c11_5hr = (c11_or[0][4] + c11_or[1][4] + c11_or[2][4] + c11_or[3][4] + c11_or[4][4])/5\n",
    "\n",
    "c11_or_avg = [c11_1hr, c11_2hr, c11_3hr, c11_4hr, c11_5hr]\n",
    "\n",
    "#Display as DF\n",
    "feature_names = ['c11_1hr_before','c11_2hrs_before','c11_3hrs_before','c11_4hrs_before','c11_5hrs_before']\n",
    "\n",
    "c11_or_table = pd.DataFrame(feature_names, columns = ['Feature'])\n",
    "c11_or_table['C11'] = c11_or_avg\n",
    "c11_or_table['Feature'] = c11_or_table[\"Feature\"].str[4:]\n",
    "c11_or_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR - MV\n",
    "\n",
    "array = mv_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#Model Output\n",
    "mv_lr = do_cv_learning_lr(X,y)\n",
    "mv_lr['Annotator'] = 'MV'\n",
    "\n",
    "#Odds ratios\n",
    "mv_or = or_lr(X,y)\n",
    "\n",
    "mv_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find avg ORs per hour - MV\n",
    "\n",
    "mv_1hr = (mv_or[0][0] + mv_or[1][0] + mv_or[2][0] +  mv_or[3][0] + mv_or[4][0])/5\n",
    "mv_2hr = (mv_or[0][1] + mv_or[1][1] + mv_or[2][1] + mv_or[3][1] + mv_or[4][1])/5\n",
    "mv_3hr = (mv_or[0][2] + mv_or[1][2] + mv_or[2][2] + mv_or[3][2] + mv_or[4][2])/5\n",
    "mv_4hr = (mv_or[0][3] + mv_or[1][3] + mv_or[2][3] + mv_or[3][3] + mv_or[4][3])/5\n",
    "mv_5hr = (mv_or[0][4] + mv_or[1][4] + mv_or[2][4] + mv_or[3][4] + mv_or[4][4])/5\n",
    "\n",
    "mv_or_avg = [mv_1hr, mv_2hr, mv_3hr, mv_4hr, mv_5hr]\n",
    "\n",
    "#Display as DF\n",
    "feature_names = ['mv_1hr_before','mv_2hrs_before','mv_3hrs_before','mv_4hrs_before','mv_5hrs_before']\n",
    "\n",
    "mv_or_table = pd.DataFrame(feature_names, columns = ['Feature'])\n",
    "mv_or_table['MV'] = mv_or_avg\n",
    "mv_or_table['Feature'] = mv_or_table[\"Feature\"].str[3:]\n",
    "mv_or_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR - TMV\n",
    "\n",
    "array = tmv_mlval.to_numpy()\n",
    "X = array[:,1:6]  \n",
    "y = array[:,6]\n",
    "\n",
    "X = X.astype(float) \n",
    "y = y.astype(int) \n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#Model Output\n",
    "tmv_lr = do_cv_learning_lr(X,y)\n",
    "tmv_lr['Annotator'] = 'TMV'\n",
    "\n",
    "#Odds ratios\n",
    "tmv_or = or_lr(X,y)\n",
    "\n",
    "tmv_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find avg ORs per hour - TMV\n",
    "\n",
    "tmv_1hr = (tmv_or[0][0] + tmv_or[1][0] + tmv_or[2][0] +  tmv_or[3][0] + tmv_or[4][0])/5\n",
    "tmv_2hr = (tmv_or[0][1] + tmv_or[1][1] + tmv_or[2][1] + tmv_or[3][1] + tmv_or[4][1])/5\n",
    "tmv_3hr = (tmv_or[0][2] + tmv_or[1][2] + tmv_or[2][2] + tmv_or[3][2] + tmv_or[4][2])/5\n",
    "tmv_4hr = (tmv_or[0][3] + tmv_or[1][3] + tmv_or[2][3] + tmv_or[3][3] + tmv_or[4][3])/5\n",
    "tmv_5hr = (tmv_or[0][4] + tmv_or[1][4] + tmv_or[2][4] + tmv_or[3][4] + tmv_or[4][4])/5\n",
    "\n",
    "tmv_or_avg = [tmv_1hr, tmv_2hr, tmv_3hr, tmv_4hr, tmv_5hr]\n",
    "\n",
    "#Display as DF\n",
    "feature_names = ['tmv_1hr_before','tmv_2hrs_before','tmv_3hrs_before','tmv_4hrs_before','tmv_5hrs_before']\n",
    "\n",
    "tmv_or_table = pd.DataFrame(feature_names, columns = ['Feature'])\n",
    "tmv_or_table['TMV'] = tmv_or_avg\n",
    "tmv_or_table['Feature'] = tmv_or_table[\"Feature\"].str[4:]\n",
    "tmv_or_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ext Validation: LR - Performance Summary\n",
    "\n",
    "frames = [c1_lr, c2_lr, c3_lr, c4_lr, c5_lr, c6_lr, c7_lr, c8_lr, c9_lr, c10_lr, c11_lr, mv_lr, tmv_lr]\n",
    "\n",
    "multi_lr = pd.concat(frames)\n",
    "print(multi_lr.shape)\n",
    "multi_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#External Validation: LR Odds Ratios - Summary\n",
    "\n",
    "ann_ORs = c1_or_table.merge(c2_or_table,on=['Feature']).merge(c3_or_table,on=['Feature']).merge(c4_or_table,on=['Feature']).merge(c5_or_table,on=['Feature']).merge(c6_or_table,on=['Feature']).merge(c7_or_table,on=['Feature']).merge(c8_or_table,on=['Feature']).merge(c9_or_table,on=['Feature']).merge(c10_or_table,on=['Feature']).merge(c11_or_table,on=['Feature']).merge(mv_or_table,on=['Feature']).merge(tmv_or_table,on=['Feature'])\n",
    "ann_ORs = ann_ORs.transpose().reset_index()\n",
    "ann_ORs = ann_ORs.rename(columns=ann_ORs.iloc[0])\n",
    "ann_ORs = ann_ORs.iloc[1: , :]\n",
    "ann_ORs.rename(columns = {'Feature':'Annotator'}, inplace = True)\n",
    "ann_ORs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot chart - Logistic Regresison: Static VS Temporal\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#Define x and y data\n",
    "x1 = multi_ext_stat['Annotator']\n",
    "y1 = multi_ex2_pred['F1_micro']\n",
    "y2 = multi_neut_pred['F1_micro']\n",
    "y3 = multi_ext_stat['F1_micro']\n",
    "\n",
    "#Plot chart data\n",
    "plt.figure(figsize=(10,3.5))\n",
    "plt.plot(x1, y1, color='#1F57C8', marker='o', linestyle=\"solid\", label='Temporal WS (Extreme)')\n",
    "plt.plot(x1, y2, color='#DA4802', marker='o', linestyle=\"solid\", label='Temporal WS (Neutral)')\n",
    "plt.plot(x1, y3, color='#62B463', marker='o', linestyle=\"solid\", label='Static')\n",
    "\n",
    "plt.yticks(np.arange(0.0,1.01, 0.2))\n",
    "\n",
    "#Add title and labels\n",
    "plt.title('HiRID External Validation: Static vs Temporal', fontsize=14)\n",
    "plt.xlabel('Annotator', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylabel('F1 micro', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(bbox_to_anchor=(1, 1), fontsize=11)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot chart - Model Calibration (DT & LR)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#Define x and y data\n",
    "x1 = multi_ext_stat['Annotator']\n",
    "y4 = multi_dt['F1_micro']\n",
    "y5 = multi_lr['F1_micro']\n",
    "\n",
    "#Plot chart data\n",
    "plt.figure(figsize=(10,3.5))\n",
    "plt.plot(x1, y4, color='#9467BD', marker='o', linestyle=\"solid\", label='Decision Tree')\n",
    "plt.plot(x1, y5, color='#ffa500', marker='o', linestyle=\"solid\", label='Logistic Regression')\n",
    "\n",
    "plt.yticks(np.arange(0.0,1.01, 0.2))\n",
    "\n",
    "#Add title and labels\n",
    "plt.title('HiRID Futher Analysis: Model Calibration', fontsize=16)\n",
    "plt.xlabel('Annotator', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylabel('F1 micro', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=13, loc=4)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_ORs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Feature Importance\n",
    "\n",
    "##create dataframe to plot\n",
    "index = ann_ORs['Annotator'].to_list()\n",
    "\n",
    "a1hr_before = ann_ORs['1hr_before'].to_list()\n",
    "a2hrs_before = ann_ORs['2hrs_before'].to_list()\n",
    "a3hrs_before = ann_ORs['3hrs_before'].to_list()\n",
    "a4hrs_before = ann_ORs['4hrs_before'].to_list()\n",
    "a5hrs_before = ann_ORs['5hrs_before'].to_list()\n",
    "\n",
    "df = pd.DataFrame({'1hr before discharge/death': a1hr_before,\n",
    "                  '2hrs before discharge/death': a2hrs_before,\n",
    "                  '3hrs before discharge/death': a3hrs_before,\n",
    "                  '4hrs before discharge/death': a4hrs_before,\n",
    "                  '5hrs before discharge/death': a5hrs_before}, index=index)\n",
    "\n",
    "##plot grouped bar chart\n",
    "ax = df.plot.bar(rot=0, color={'1hr before discharge/death': '#FFA319FF', '2hrs before discharge/death': '#CD534CFF', \n",
    "                               '3hrs before discharge/death': '#1f77b4', '4hrs before discharge/death': '#62B463',\n",
    "                               '5hrs before discharge/death': '#9467BD'}, width=0.7)\n",
    "\n",
    "##set Figure size\n",
    "fig = ax.get_figure()\n",
    "fig.set_size_inches(12, 3.5)\n",
    "\n",
    "##set title and axis labels\n",
    "ax.set_title(\"Odds Ratio distributions for HiRID predicted labels 1-5hrs before discharge/death\", fontsize=15)\n",
    "ax.set_xlabel(\"Annotator\", fontsize=14)\n",
    "ax.set_xticklabels(index,fontsize=14)\n",
    "ax.set_ylabel(\"Odds Ratio\", fontsize=14)\n",
    "ax.axhline(y=1, color='#000000', linestyle='-')\n",
    "plt.legend(loc=(1.02,0), fontsize=12)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. External Validation - IAA Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_discharge = hirid_stat.copy(deep=True)\n",
    "true_discharge = true_discharge[['patientid','discharge_status']]\n",
    "true_discharge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Extreme 1 cut-off - Discharged Alive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_ex1_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only Discharged alive - from ex1 cut-off\n",
    "\n",
    "true_alive_ex1 = ann_ex1_pred.copy(deep=True)\n",
    "true_alive_ex1 = pd.merge(true_alive_ex1, true_discharge, on='patientid')\n",
    "true_alive_ex1 = true_alive_ex1[true_alive_ex1['discharge_status']=='alive']\n",
    "true_alive_ex1.columns = ['patientid', 'C1', 'C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','MV','TMV','discharge_status']\n",
    "true_alive_ex1 = true_alive_ex1.drop(['patientid','MV','TMV','discharge_status'], axis = 1)\n",
    "true_alive_ex1 = true_alive_ex1.applymap(str)\n",
    "true_alive_ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate pairwise Cohen's kappa (Discharged Alive only)\n",
    "\n",
    "c1_pred = true_alive_ex1.iloc[:,0]\n",
    "c2_pred = true_alive_ex1.iloc[:,1]\n",
    "c3_pred = true_alive_ex1.iloc[:,2]\n",
    "c4_pred = true_alive_ex1.iloc[:,3]\n",
    "c5_pred = true_alive_ex1.iloc[:,4]\n",
    "c6_pred = true_alive_ex1.iloc[:,5]\n",
    "c7_pred = true_alive_ex1.iloc[:,6]\n",
    "c8_pred = true_alive_ex1.iloc[:,7]\n",
    "c9_pred = true_alive_ex1.iloc[:,8]\n",
    "c10_pred = true_alive_ex1.iloc[:,9]\n",
    "c11_pred = true_alive_ex1.iloc[:,10]\n",
    "\n",
    "c1_c2 = round(cohen_kappa_score(c1_pred, c2_pred),2)\n",
    "c1_c3 = round(cohen_kappa_score(c1_pred, c3_pred),2)\n",
    "c1_c4 = round(cohen_kappa_score(c1_pred, c4_pred),2)\n",
    "c1_c5 = round(cohen_kappa_score(c1_pred, c5_pred),2)\n",
    "c1_c6 = round(cohen_kappa_score(c1_pred, c6_pred),2)\n",
    "c1_c7 = round(cohen_kappa_score(c1_pred, c7_pred),2)\n",
    "c1_c8 = round(cohen_kappa_score(c1_pred, c8_pred),2)\n",
    "c1_c9 = round(cohen_kappa_score(c1_pred, c9_pred),2)\n",
    "c1_c10 = round(cohen_kappa_score(c1_pred, c10_pred),2)\n",
    "c1_c11 = round(cohen_kappa_score(c1_pred, c11_pred),2)\n",
    "\n",
    "c2_c3 = round(cohen_kappa_score(c2_pred, c3_pred),2)\n",
    "c2_c4 = round(cohen_kappa_score(c2_pred, c4_pred),2)\n",
    "c2_c5 = round(cohen_kappa_score(c2_pred, c5_pred),2)\n",
    "c2_c6 = round(cohen_kappa_score(c2_pred, c6_pred),2)\n",
    "c2_c7 = round(cohen_kappa_score(c2_pred, c7_pred),2)\n",
    "c2_c8 = round(cohen_kappa_score(c2_pred, c8_pred),2)\n",
    "c2_c9 = round(cohen_kappa_score(c2_pred, c9_pred),2)\n",
    "c2_c10 = round(cohen_kappa_score(c2_pred, c10_pred),2)\n",
    "c2_c11 = round(cohen_kappa_score(c2_pred, c11_pred),2)\n",
    "\n",
    "c3_c4 = round(cohen_kappa_score(c3_pred, c4_pred),2)\n",
    "c3_c5 = round(cohen_kappa_score(c3_pred, c5_pred),2)\n",
    "c3_c6 = round(cohen_kappa_score(c3_pred, c6_pred),2)\n",
    "c3_c7 = round(cohen_kappa_score(c3_pred, c7_pred),2)\n",
    "c3_c8 = round(cohen_kappa_score(c3_pred, c8_pred),2)\n",
    "c3_c9 = round(cohen_kappa_score(c3_pred, c9_pred),2)\n",
    "c3_c10 = round(cohen_kappa_score(c3_pred, c10_pred),2)\n",
    "c3_c11 = round(cohen_kappa_score(c3_pred, c11_pred),2)\n",
    "\n",
    "c4_c5 = round(cohen_kappa_score(c4_pred, c5_pred),2)\n",
    "c4_c6 = round(cohen_kappa_score(c4_pred, c6_pred),2)\n",
    "c4_c7 = round(cohen_kappa_score(c4_pred, c7_pred),2)\n",
    "c4_c8 = round(cohen_kappa_score(c4_pred, c8_pred),2)\n",
    "c4_c9 = round(cohen_kappa_score(c4_pred, c9_pred),2)\n",
    "c4_c10 = round(cohen_kappa_score(c4_pred, c10_pred),2)\n",
    "c4_c11 = round(cohen_kappa_score(c4_pred, c11_pred),2)\n",
    "\n",
    "c5_c6 = round(cohen_kappa_score(c5_pred, c6_pred),2)\n",
    "c5_c7 = round(cohen_kappa_score(c5_pred, c7_pred),2)\n",
    "c5_c8 = round(cohen_kappa_score(c5_pred, c8_pred),2)\n",
    "c5_c9 = round(cohen_kappa_score(c5_pred, c9_pred),2)\n",
    "c5_c10 = round(cohen_kappa_score(c5_pred, c10_pred),2)\n",
    "c5_c11 = round(cohen_kappa_score(c5_pred, c11_pred),2)\n",
    "\n",
    "c6_c7 = round(cohen_kappa_score(c6_pred, c7_pred),2)\n",
    "c6_c8 = round(cohen_kappa_score(c6_pred, c8_pred),2)\n",
    "c6_c9 = round(cohen_kappa_score(c6_pred, c9_pred),2)\n",
    "c6_c10 = round(cohen_kappa_score(c6_pred, c10_pred),2)\n",
    "c6_c11 = round(cohen_kappa_score(c6_pred, c11_pred),2)\n",
    "\n",
    "c7_c8 = round(cohen_kappa_score(c7_pred, c8_pred),2)\n",
    "c7_c9 = round(cohen_kappa_score(c7_pred, c9_pred),2)\n",
    "c7_c10 = round(cohen_kappa_score(c7_pred, c10_pred),2)\n",
    "c7_c11 = round(cohen_kappa_score(c7_pred, c11_pred),2)\n",
    "\n",
    "c8_c9 = round(cohen_kappa_score(c8_pred, c9_pred),2)\n",
    "c8_c10 = round(cohen_kappa_score(c8_pred, c10_pred),2)\n",
    "c8_c11 = round(cohen_kappa_score(c8_pred, c11_pred),2)\n",
    "\n",
    "c9_c10 = round(cohen_kappa_score(c9_pred, c10_pred),2)\n",
    "c9_c11 = round(cohen_kappa_score(c9_pred, c11_pred),2)\n",
    "\n",
    "c10_c11 = round(cohen_kappa_score(c10_pred, c11_pred),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pairwise Cohen's kappa (Discharged Alive only)\n",
    "\n",
    "C0 = [\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\"]\n",
    "C1 = [1.00, c1_c2, c1_c3, c1_c4, c1_c5, c1_c6, c1_c7, c1_c8, c1_c9, c1_c10, c1_c11]\n",
    "C2 = [\"\", 1.00, c2_c3, c2_c4, c2_c5, c2_c6, c2_c7, c2_c8, c2_c9, c2_c10, c2_c11]\n",
    "C3 = [\"\", \"\", 1.00, c3_c4, c3_c5, c3_c6, c3_c7, c3_c8, c3_c9, c3_c10, c3_c11]\n",
    "C4 = [\"\", \"\", \"\", 1.00, c4_c5, c4_c6, c4_c7, c4_c8, c4_c9, c4_c10, c4_c11]\n",
    "C5 = [\"\", \"\", \"\", \"\", 1.00, c5_c6, c5_c7, c5_c8, c5_c9, c5_c10, c5_c11]\n",
    "C6 = [\"\", \"\", \"\", \"\", \"\", 1.00, c6_c7, c6_c8, c6_c9, c6_c10, c6_c11]\n",
    "C7 = [\"\", \"\", \"\", \"\", \"\", \"\", 1.00, c7_c8, c7_c9, c7_c10, c7_c11]\n",
    "C8 = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", 1.00, c8_c9, c8_c10, c8_c11]\n",
    "C9 = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", 1.00, c9_c10, c9_c11]\n",
    "C10 = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", 1.00, c10_c11]\n",
    "C11 = [\"\", \"\", \"\", \"\", \"\", \"\" , \"\", \"\", \"\", \"\", 1.00]\n",
    "\n",
    "C0 = pd.DataFrame(data=C0)\n",
    "C1 = pd.DataFrame(data=C1)\n",
    "C2 = pd.DataFrame(data=C2)\n",
    "C3 = pd.DataFrame(data=C3)\n",
    "C4 = pd.DataFrame(data=C4)\n",
    "C5 = pd.DataFrame(data=C5)\n",
    "C6 = pd.DataFrame(data=C6)\n",
    "C7 = pd.DataFrame(data=C7)\n",
    "C8 = pd.DataFrame(data=C8)\n",
    "C9 = pd.DataFrame(data=C9)\n",
    "C10 = pd.DataFrame(data=C10)\n",
    "C11 = pd.DataFrame(data=C11)\n",
    "\n",
    "C0.columns = [\"\"]\n",
    "C1.columns = ['C1']\n",
    "C2.columns = ['C2']\n",
    "C3.columns = ['C3']\n",
    "C4.columns = ['C4']\n",
    "C5.columns = ['C5']\n",
    "C6.columns = ['C6']\n",
    "C7.columns = ['C7']\n",
    "C8.columns = ['C8']\n",
    "C9.columns = ['C9']\n",
    "C10.columns = ['C10']\n",
    "C11.columns = ['C11']\n",
    "\n",
    "frames = [C0,C1,C2,C3,C4,C5,C6,C7,C8,C9,C10,C11]\n",
    "\n",
    "cohen_k_ex1_alive = pd.concat(frames, axis=1)\n",
    "cohen_k_ex1_alive = cohen_k_ex1_alive.set_index(\"\")\n",
    "\n",
    "cohen_k_ex1_alive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot pairwise Cohen's kappa (Discharged Alive only)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "cols = [\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\"]\n",
    "cohen_k_ex1_alive[cols] = cohen_k_ex1_alive[cols].apply(pd.to_numeric)\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(8, 5), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "res = sns.heatmap(cohen_k_ex1_alive, annot=True, vmin=0, vmax=1, \n",
    "                  fmt='.2f', cmap=\"YlGnBu\", annot_kws={\"fontsize\":15})\n",
    "\n",
    "res.set_xticklabels(res.get_xmajorticklabels(), fontsize = 15)\n",
    "res.set_yticklabels(res.get_ymajorticklabels(), fontsize = 15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#range cohen's k: -0.00 to 0.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average pairwise cohen's kappa values (Discharge alived)\n",
    " \n",
    "#data\n",
    "sample = [c1_c2, c1_c3, c1_c4, c1_c5, c1_c6, c1_c7, c1_c8, c1_c9, c1_c10, c1_c11,\n",
    "          c2_c3, c2_c4, c2_c5, c2_c6, c2_c7, c2_c8, c2_c9, c2_c10, c2_c11,\n",
    "          c3_c4, c3_c5, c3_c6, c3_c7, c3_c8, c3_c9, c3_c10, c3_c11, \n",
    "          c4_c5, c4_c6, c4_c7, c4_c8, c4_c9, c4_c10, c4_c11, \n",
    "          c5_c6, c5_c7, c5_c8, c5_c9, c5_c10, c5_c11,  \n",
    "          c6_c7, c6_c8, c6_c9, c6_c10, c6_c11, \n",
    "          c7_c8, c7_c9, c7_c10, c7_c11,  \n",
    "          c8_c9, c8_c10, c8_c11,  \n",
    "          c9_c10, c9_c11,  \n",
    "          c10_c11]\n",
    "\n",
    "avg = round(mean(sample),3)\n",
    "sd = round(statistics.stdev(sample),3)\n",
    " \n",
    "# Prints average & standard deviation\n",
    "print(\"Average:\", avg)\n",
    "print(\"Standard Deviation:\", sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Fleiss' kappa for Discharged Alive\n",
    "\n",
    "all_alive_ex1 = true_alive_ex1.copy(deep=True)\n",
    "all_alive_ex1['count_alive']  = all_alive_ex1.eq('0').sum(axis=1)\n",
    "all_alive_ex1['count_dead']  = all_alive_ex1.eq('1').sum(axis=1)\n",
    "all_alive_ex1['count_other']  = all_alive_ex1.eq('3').sum(axis=1)\n",
    "\n",
    "##drop unncessary cols\n",
    "cols = [ 'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11']\n",
    "\n",
    "all_alive_ex1 = all_alive_ex1.drop(cols, axis = 1)\n",
    "\n",
    "all_alive_ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Fleiss' kappa - Discharged Alive\n",
    "\n",
    "fleiss_k_ex1_alive = round(fleiss_kappa(all_alive_ex1, method='fleiss'),3)\n",
    "\n",
    "print(\"Fleiss' kappa: {:.3f}\".format(fleiss_k_ex1_alive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Extreme 2 cut-off - Died"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only Discharged died - from ex1 cut-off\n",
    "\n",
    "true_died_ex1 = ann_ex1_pred.copy(deep=True)\n",
    "true_died_ex1 = pd.merge(true_died_ex1, true_discharge, on='patientid')\n",
    "true_died_ex1 = true_died_ex1[true_died_ex1['discharge_status']=='dead']\n",
    "true_died_ex1.columns = ['patientid', 'C1', 'C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','MV','TMV','discharge_status']\n",
    "true_died_ex1 = true_died_ex1.drop(['patientid','MV','TMV','discharge_status'], axis = 1)\n",
    "true_died_ex1 = true_died_ex1.applymap(str)\n",
    "true_died_ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate pairwise Cohen's kappa (Discharged died only)\n",
    "\n",
    "c1_pred = true_died_ex1.iloc[:,0]\n",
    "c2_pred = true_died_ex1.iloc[:,1]\n",
    "c3_pred = true_died_ex1.iloc[:,2]\n",
    "c4_pred = true_died_ex1.iloc[:,3]\n",
    "c5_pred = true_died_ex1.iloc[:,4]\n",
    "c6_pred = true_died_ex1.iloc[:,5]\n",
    "c7_pred = true_died_ex1.iloc[:,6]\n",
    "c8_pred = true_died_ex1.iloc[:,7]\n",
    "c9_pred = true_died_ex1.iloc[:,8]\n",
    "c10_pred = true_died_ex1.iloc[:,9]\n",
    "c11_pred = true_died_ex1.iloc[:,10]\n",
    "\n",
    "c1_c2 = round(cohen_kappa_score(c1_pred, c2_pred),2)\n",
    "c1_c3 = round(cohen_kappa_score(c1_pred, c3_pred),2)\n",
    "c1_c4 = round(cohen_kappa_score(c1_pred, c4_pred),2)\n",
    "c1_c5 = round(cohen_kappa_score(c1_pred, c5_pred),2)\n",
    "c1_c6 = round(cohen_kappa_score(c1_pred, c6_pred),2)\n",
    "c1_c7 = round(cohen_kappa_score(c1_pred, c7_pred),2)\n",
    "c1_c8 = round(cohen_kappa_score(c1_pred, c8_pred),2)\n",
    "c1_c9 = round(cohen_kappa_score(c1_pred, c9_pred),2)\n",
    "c1_c10 = round(cohen_kappa_score(c1_pred, c10_pred),2)\n",
    "c1_c11 = round(cohen_kappa_score(c1_pred, c11_pred),2)\n",
    "\n",
    "c2_c3 = round(cohen_kappa_score(c2_pred, c3_pred),2)\n",
    "c2_c4 = round(cohen_kappa_score(c2_pred, c4_pred),2)\n",
    "c2_c5 = round(cohen_kappa_score(c2_pred, c5_pred),2)\n",
    "c2_c6 = round(cohen_kappa_score(c2_pred, c6_pred),2)\n",
    "c2_c7 = round(cohen_kappa_score(c2_pred, c7_pred),2)\n",
    "c2_c8 = round(cohen_kappa_score(c2_pred, c8_pred),2)\n",
    "c2_c9 = round(cohen_kappa_score(c2_pred, c9_pred),2)\n",
    "c2_c10 = round(cohen_kappa_score(c2_pred, c10_pred),2)\n",
    "c2_c11 = round(cohen_kappa_score(c2_pred, c11_pred),2)\n",
    "\n",
    "c3_c4 = round(cohen_kappa_score(c3_pred, c4_pred),2)\n",
    "c3_c5 = round(cohen_kappa_score(c3_pred, c5_pred),2)\n",
    "c3_c6 = round(cohen_kappa_score(c3_pred, c6_pred),2)\n",
    "c3_c7 = round(cohen_kappa_score(c3_pred, c7_pred),2)\n",
    "c3_c8 = round(cohen_kappa_score(c3_pred, c8_pred),2)\n",
    "c3_c9 = round(cohen_kappa_score(c3_pred, c9_pred),2)\n",
    "c3_c10 = round(cohen_kappa_score(c3_pred, c10_pred),2)\n",
    "c3_c11 = round(cohen_kappa_score(c3_pred, c11_pred),2)\n",
    "\n",
    "c4_c5 = round(cohen_kappa_score(c4_pred, c5_pred),2)\n",
    "c4_c6 = round(cohen_kappa_score(c4_pred, c6_pred),2)\n",
    "c4_c7 = round(cohen_kappa_score(c4_pred, c7_pred),2)\n",
    "c4_c8 = round(cohen_kappa_score(c4_pred, c8_pred),2)\n",
    "c4_c9 = round(cohen_kappa_score(c4_pred, c9_pred),2)\n",
    "c4_c10 = round(cohen_kappa_score(c4_pred, c10_pred),2)\n",
    "c4_c11 = round(cohen_kappa_score(c4_pred, c11_pred),2)\n",
    "\n",
    "c5_c6 = round(cohen_kappa_score(c5_pred, c6_pred),2)\n",
    "c5_c7 = round(cohen_kappa_score(c5_pred, c7_pred),2)\n",
    "c5_c8 = round(cohen_kappa_score(c5_pred, c8_pred),2)\n",
    "c5_c9 = round(cohen_kappa_score(c5_pred, c9_pred),2)\n",
    "c5_c10 = round(cohen_kappa_score(c5_pred, c10_pred),2)\n",
    "c5_c11 = round(cohen_kappa_score(c5_pred, c11_pred),2)\n",
    "\n",
    "c6_c7 = round(cohen_kappa_score(c6_pred, c7_pred),2)\n",
    "c6_c8 = round(cohen_kappa_score(c6_pred, c8_pred),2)\n",
    "c6_c9 = round(cohen_kappa_score(c6_pred, c9_pred),2)\n",
    "c6_c10 = round(cohen_kappa_score(c6_pred, c10_pred),2)\n",
    "c6_c11 = round(cohen_kappa_score(c6_pred, c11_pred),2)\n",
    "\n",
    "c7_c8 = round(cohen_kappa_score(c7_pred, c8_pred),2)\n",
    "c7_c9 = round(cohen_kappa_score(c7_pred, c9_pred),2)\n",
    "c7_c10 = round(cohen_kappa_score(c7_pred, c10_pred),2)\n",
    "c7_c11 = round(cohen_kappa_score(c7_pred, c11_pred),2)\n",
    "\n",
    "c8_c9 = round(cohen_kappa_score(c8_pred, c9_pred),2)\n",
    "c8_c10 = round(cohen_kappa_score(c8_pred, c10_pred),2)\n",
    "c8_c11 = round(cohen_kappa_score(c8_pred, c11_pred),2)\n",
    "\n",
    "c9_c10 = round(cohen_kappa_score(c9_pred, c10_pred),2)\n",
    "c9_c11 = round(cohen_kappa_score(c9_pred, c11_pred),2)\n",
    "\n",
    "c10_c11 = round(cohen_kappa_score(c10_pred, c11_pred),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pairwise Cohen's kappa (Discharged died only)\n",
    "\n",
    "C0 = [\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\"]\n",
    "C1 = [1.00, c1_c2, c1_c3, c1_c4, c1_c5, c1_c6, c1_c7, c1_c8, c1_c9, c1_c10, c1_c11]\n",
    "C2 = [\"\", 1.00, c2_c3, c2_c4, c2_c5, c2_c6, c2_c7, c2_c8, c2_c9, c2_c10, c2_c11]\n",
    "C3 = [\"\", \"\", 1.00, c3_c4, c3_c5, c3_c6, c3_c7, c3_c8, c3_c9, c3_c10, c3_c11]\n",
    "C4 = [\"\", \"\", \"\", 1.00, c4_c5, c4_c6, c4_c7, c4_c8, c4_c9, c4_c10, c4_c11]\n",
    "C5 = [\"\", \"\", \"\", \"\", 1.00, c5_c6, c5_c7, c5_c8, c5_c9, c5_c10, c5_c11]\n",
    "C6 = [\"\", \"\", \"\", \"\", \"\", 1.00, c6_c7, c6_c8, c6_c9, c6_c10, c6_c11]\n",
    "C7 = [\"\", \"\", \"\", \"\", \"\", \"\", 1.00, c7_c8, c7_c9, c7_c10, c7_c11]\n",
    "C8 = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", 1.00, c8_c9, c8_c10, c8_c11]\n",
    "C9 = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", 1.00, c9_c10, c9_c11]\n",
    "C10 = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", 1.00, c10_c11]\n",
    "C11 = [\"\", \"\", \"\", \"\", \"\", \"\" , \"\", \"\", \"\", \"\", 1.00]\n",
    "\n",
    "C0 = pd.DataFrame(data=C0)\n",
    "C1 = pd.DataFrame(data=C1)\n",
    "C2 = pd.DataFrame(data=C2)\n",
    "C3 = pd.DataFrame(data=C3)\n",
    "C4 = pd.DataFrame(data=C4)\n",
    "C5 = pd.DataFrame(data=C5)\n",
    "C6 = pd.DataFrame(data=C6)\n",
    "C7 = pd.DataFrame(data=C7)\n",
    "C8 = pd.DataFrame(data=C8)\n",
    "C9 = pd.DataFrame(data=C9)\n",
    "C10 = pd.DataFrame(data=C10)\n",
    "C11 = pd.DataFrame(data=C11)\n",
    "\n",
    "C0.columns = [\"\"]\n",
    "C1.columns = ['C1']\n",
    "C2.columns = ['C2']\n",
    "C3.columns = ['C3']\n",
    "C4.columns = ['C4']\n",
    "C5.columns = ['C5']\n",
    "C6.columns = ['C6']\n",
    "C7.columns = ['C7']\n",
    "C8.columns = ['C8']\n",
    "C9.columns = ['C9']\n",
    "C10.columns = ['C10']\n",
    "C11.columns = ['C11']\n",
    "\n",
    "frames = [C0,C1,C2,C3,C4,C5,C6,C7,C8,C9,C10,C11]\n",
    "\n",
    "cohen_k_ex1_died = pd.concat(frames, axis=1)\n",
    "cohen_k_ex1_died = cohen_k_ex1_died.set_index(\"\")\n",
    "\n",
    "cohen_k_ex1_died"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot pairwise Cohen's kappa (Discharged Died only)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "cols = [\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\"]\n",
    "cohen_k_ex1_died[cols] = cohen_k_ex1_died[cols].apply(pd.to_numeric)\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(8, 5), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "res = sns.heatmap(cohen_k_ex1_died, annot=True, vmin=0, vmax=1, \n",
    "                  fmt='.2f', cmap=\"YlGnBu\", annot_kws={\"fontsize\":15})\n",
    "\n",
    "res.set_xticklabels(res.get_xmajorticklabels(), fontsize = 15)\n",
    "res.set_yticklabels(res.get_ymajorticklabels(), fontsize = 15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#range cohen's k: -0.01 to 0.79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average pairwise cohen's kappa values (Discharge alived)\n",
    " \n",
    "#data\n",
    "sample = [c1_c2, c1_c3, c1_c4, c1_c5, c1_c6, c1_c7, c1_c8, c1_c9, c1_c10, c1_c11,\n",
    "          c2_c3, c2_c4, c2_c5, c2_c6, c2_c7, c2_c8, c2_c9, c2_c10, c2_c11,\n",
    "          c3_c4, c3_c5, c3_c6, c3_c7, c3_c8, c3_c9, c3_c10, c3_c11, \n",
    "          c4_c5, c4_c6, c4_c7, c4_c8, c4_c9, c4_c10, c4_c11, \n",
    "          c5_c6, c5_c7, c5_c8, c5_c9, c5_c10, c5_c11,  \n",
    "          c6_c7, c6_c8, c6_c9, c6_c10, c6_c11, \n",
    "          c7_c8, c7_c9, c7_c10, c7_c11,  \n",
    "          c8_c9, c8_c10, c8_c11,  \n",
    "          c9_c10, c9_c11,  \n",
    "          c10_c11]\n",
    "\n",
    "avg = round(mean(sample),3)\n",
    "sd = round(statistics.stdev(sample),3)\n",
    " \n",
    "# Prints average & standard deviation\n",
    "print(\"Average:\", avg)\n",
    "print(\"Standard Deviation:\", sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Fleiss' kappa for Discharged died\n",
    "\n",
    "all_died_ex1 = true_died_ex1.copy(deep=True)\n",
    "all_died_ex1['count_died']  = all_died_ex1.eq('0').sum(axis=1)\n",
    "all_died_ex1['count_dead']  = all_died_ex1.eq('1').sum(axis=1)\n",
    "all_died_ex1['count_other']  = all_died_ex1.eq('3').sum(axis=1)\n",
    "\n",
    "##drop unncessary cols\n",
    "cols = [ 'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11']\n",
    "\n",
    "all_died_ex1 = all_died_ex1.drop(cols, axis = 1)\n",
    "\n",
    "all_died_ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Fleiss' kappa - Discharged Alive\n",
    "\n",
    "fleiss_k_ex1_died = round(fleiss_kappa(all_died_ex1, method='fleiss'),3)\n",
    "\n",
    "print(\"Fleiss' kappa: {:.3f}\".format(fleiss_k_ex1_died))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Extreme 2 cut-off - Discharged Alive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_ex2_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only Discharged alive - from Ex2 cut-off\n",
    "\n",
    "true_alive_ex2 = ann_ex2_pred.copy(deep=True)\n",
    "true_alive_ex2 = pd.merge(true_alive_ex2, true_discharge, on='patientid')\n",
    "true_alive_ex2 = true_alive_ex2[true_alive_ex2['discharge_status']=='alive']\n",
    "true_alive_ex2.columns = ['patientid', 'C1', 'C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','MV','TMV','discharge_status']\n",
    "true_alive_ex2 = true_alive_ex2.drop(['patientid','MV','TMV','discharge_status'], axis = 1)\n",
    "true_alive_ex2 = true_alive_ex2.applymap(str)\n",
    "true_alive_ex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate pairwise Cohen's kappa (Discharged Alive only)\n",
    "\n",
    "c1_pred = true_alive_ex2.iloc[:,0]\n",
    "c2_pred = true_alive_ex2.iloc[:,1]\n",
    "c3_pred = true_alive_ex2.iloc[:,2]\n",
    "c4_pred = true_alive_ex2.iloc[:,3]\n",
    "c5_pred = true_alive_ex2.iloc[:,4]\n",
    "c6_pred = true_alive_ex2.iloc[:,5]\n",
    "c7_pred = true_alive_ex2.iloc[:,6]\n",
    "c8_pred = true_alive_ex2.iloc[:,7]\n",
    "c9_pred = true_alive_ex2.iloc[:,8]\n",
    "c10_pred = true_alive_ex2.iloc[:,9]\n",
    "c11_pred = true_alive_ex2.iloc[:,10]\n",
    "\n",
    "c1_c2 = round(cohen_kappa_score(c1_pred, c2_pred),2)\n",
    "c1_c3 = round(cohen_kappa_score(c1_pred, c3_pred),2)\n",
    "c1_c4 = round(cohen_kappa_score(c1_pred, c4_pred),2)\n",
    "c1_c5 = round(cohen_kappa_score(c1_pred, c5_pred),2)\n",
    "c1_c6 = round(cohen_kappa_score(c1_pred, c6_pred),2)\n",
    "c1_c7 = round(cohen_kappa_score(c1_pred, c7_pred),2)\n",
    "c1_c8 = round(cohen_kappa_score(c1_pred, c8_pred),2)\n",
    "c1_c9 = round(cohen_kappa_score(c1_pred, c9_pred),2)\n",
    "c1_c10 = round(cohen_kappa_score(c1_pred, c10_pred),2)\n",
    "c1_c11 = round(cohen_kappa_score(c1_pred, c11_pred),2)\n",
    "\n",
    "c2_c3 = round(cohen_kappa_score(c2_pred, c3_pred),2)\n",
    "c2_c4 = round(cohen_kappa_score(c2_pred, c4_pred),2)\n",
    "c2_c5 = round(cohen_kappa_score(c2_pred, c5_pred),2)\n",
    "c2_c6 = round(cohen_kappa_score(c2_pred, c6_pred),2)\n",
    "c2_c7 = round(cohen_kappa_score(c2_pred, c7_pred),2)\n",
    "c2_c8 = round(cohen_kappa_score(c2_pred, c8_pred),2)\n",
    "c2_c9 = round(cohen_kappa_score(c2_pred, c9_pred),2)\n",
    "c2_c10 = round(cohen_kappa_score(c2_pred, c10_pred),2)\n",
    "c2_c11 = round(cohen_kappa_score(c2_pred, c11_pred),2)\n",
    "\n",
    "c3_c4 = round(cohen_kappa_score(c3_pred, c4_pred),2)\n",
    "c3_c5 = round(cohen_kappa_score(c3_pred, c5_pred),2)\n",
    "c3_c6 = round(cohen_kappa_score(c3_pred, c6_pred),2)\n",
    "c3_c7 = round(cohen_kappa_score(c3_pred, c7_pred),2)\n",
    "c3_c8 = round(cohen_kappa_score(c3_pred, c8_pred),2)\n",
    "c3_c9 = round(cohen_kappa_score(c3_pred, c9_pred),2)\n",
    "c3_c10 = round(cohen_kappa_score(c3_pred, c10_pred),2)\n",
    "c3_c11 = round(cohen_kappa_score(c3_pred, c11_pred),2)\n",
    "\n",
    "c4_c5 = round(cohen_kappa_score(c4_pred, c5_pred),2)\n",
    "c4_c6 = round(cohen_kappa_score(c4_pred, c6_pred),2)\n",
    "c4_c7 = round(cohen_kappa_score(c4_pred, c7_pred),2)\n",
    "c4_c8 = round(cohen_kappa_score(c4_pred, c8_pred),2)\n",
    "c4_c9 = round(cohen_kappa_score(c4_pred, c9_pred),2)\n",
    "c4_c10 = round(cohen_kappa_score(c4_pred, c10_pred),2)\n",
    "c4_c11 = round(cohen_kappa_score(c4_pred, c11_pred),2)\n",
    "\n",
    "c5_c6 = round(cohen_kappa_score(c5_pred, c6_pred),2)\n",
    "c5_c7 = round(cohen_kappa_score(c5_pred, c7_pred),2)\n",
    "c5_c8 = round(cohen_kappa_score(c5_pred, c8_pred),2)\n",
    "c5_c9 = round(cohen_kappa_score(c5_pred, c9_pred),2)\n",
    "c5_c10 = round(cohen_kappa_score(c5_pred, c10_pred),2)\n",
    "c5_c11 = round(cohen_kappa_score(c5_pred, c11_pred),2)\n",
    "\n",
    "c6_c7 = round(cohen_kappa_score(c6_pred, c7_pred),2)\n",
    "c6_c8 = round(cohen_kappa_score(c6_pred, c8_pred),2)\n",
    "c6_c9 = round(cohen_kappa_score(c6_pred, c9_pred),2)\n",
    "c6_c10 = round(cohen_kappa_score(c6_pred, c10_pred),2)\n",
    "c6_c11 = round(cohen_kappa_score(c6_pred, c11_pred),2)\n",
    "\n",
    "c7_c8 = round(cohen_kappa_score(c7_pred, c8_pred),2)\n",
    "c7_c9 = round(cohen_kappa_score(c7_pred, c9_pred),2)\n",
    "c7_c10 = round(cohen_kappa_score(c7_pred, c10_pred),2)\n",
    "c7_c11 = round(cohen_kappa_score(c7_pred, c11_pred),2)\n",
    "\n",
    "c8_c9 = round(cohen_kappa_score(c8_pred, c9_pred),2)\n",
    "c8_c10 = round(cohen_kappa_score(c8_pred, c10_pred),2)\n",
    "c8_c11 = round(cohen_kappa_score(c8_pred, c11_pred),2)\n",
    "\n",
    "c9_c10 = round(cohen_kappa_score(c9_pred, c10_pred),2)\n",
    "c9_c11 = round(cohen_kappa_score(c9_pred, c11_pred),2)\n",
    "\n",
    "c10_c11 = round(cohen_kappa_score(c10_pred, c11_pred),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pairwise Cohen's kappa (Discharged Alive only)\n",
    "\n",
    "C0 = [\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\"]\n",
    "C1 = [1.00, c1_c2, c1_c3, c1_c4, c1_c5, c1_c6, c1_c7, c1_c8, c1_c9, c1_c10, c1_c11]\n",
    "C2 = [\"\", 1.00, c2_c3, c2_c4, c2_c5, c2_c6, c2_c7, c2_c8, c2_c9, c2_c10, c2_c11]\n",
    "C3 = [\"\", \"\", 1.00, c3_c4, c3_c5, c3_c6, c3_c7, c3_c8, c3_c9, c3_c10, c3_c11]\n",
    "C4 = [\"\", \"\", \"\", 1.00, c4_c5, c4_c6, c4_c7, c4_c8, c4_c9, c4_c10, c4_c11]\n",
    "C5 = [\"\", \"\", \"\", \"\", 1.00, c5_c6, c5_c7, c5_c8, c5_c9, c5_c10, c5_c11]\n",
    "C6 = [\"\", \"\", \"\", \"\", \"\", 1.00, c6_c7, c6_c8, c6_c9, c6_c10, c6_c11]\n",
    "C7 = [\"\", \"\", \"\", \"\", \"\", \"\", 1.00, c7_c8, c7_c9, c7_c10, c7_c11]\n",
    "C8 = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", 1.00, c8_c9, c8_c10, c8_c11]\n",
    "C9 = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", 1.00, c9_c10, c9_c11]\n",
    "C10 = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", 1.00, c10_c11]\n",
    "C11 = [\"\", \"\", \"\", \"\", \"\", \"\" , \"\", \"\", \"\", \"\", 1.00]\n",
    "\n",
    "C0 = pd.DataFrame(data=C0)\n",
    "C1 = pd.DataFrame(data=C1)\n",
    "C2 = pd.DataFrame(data=C2)\n",
    "C3 = pd.DataFrame(data=C3)\n",
    "C4 = pd.DataFrame(data=C4)\n",
    "C5 = pd.DataFrame(data=C5)\n",
    "C6 = pd.DataFrame(data=C6)\n",
    "C7 = pd.DataFrame(data=C7)\n",
    "C8 = pd.DataFrame(data=C8)\n",
    "C9 = pd.DataFrame(data=C9)\n",
    "C10 = pd.DataFrame(data=C10)\n",
    "C11 = pd.DataFrame(data=C11)\n",
    "\n",
    "C0.columns = [\"\"]\n",
    "C1.columns = ['C1']\n",
    "C2.columns = ['C2']\n",
    "C3.columns = ['C3']\n",
    "C4.columns = ['C4']\n",
    "C5.columns = ['C5']\n",
    "C6.columns = ['C6']\n",
    "C7.columns = ['C7']\n",
    "C8.columns = ['C8']\n",
    "C9.columns = ['C9']\n",
    "C10.columns = ['C10']\n",
    "C11.columns = ['C11']\n",
    "\n",
    "frames = [C0,C1,C2,C3,C4,C5,C6,C7,C8,C9,C10,C11]\n",
    "\n",
    "cohen_k_ex2_alive = pd.concat(frames, axis=1)\n",
    "cohen_k_ex2_alive = cohen_k_ex2_alive.set_index(\"\")\n",
    "\n",
    "cohen_k_ex2_alive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot pairwise Cohen's kappa (Discharged Alive only)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "cols = [\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\"]\n",
    "cohen_k_ex2_alive[cols] = cohen_k_ex2_alive[cols].apply(pd.to_numeric)\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(8, 5), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "res = sns.heatmap(cohen_k_ex2_alive, annot=True, vmin=0, vmax=1, \n",
    "                  fmt='.2f', cmap=\"YlGnBu\", annot_kws={\"fontsize\":15})\n",
    "\n",
    "res.set_xticklabels(res.get_xmajorticklabels(), fontsize = 15)\n",
    "res.set_yticklabels(res.get_ymajorticklabels(), fontsize = 15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#range cohen's k: 0.06 to 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average pairwise cohen's kappa values (Discharge alived)\n",
    " \n",
    "#data\n",
    "sample = [c1_c2, c1_c3, c1_c4, c1_c5, c1_c6, c1_c7, c1_c8, c1_c9, c1_c10, c1_c11,\n",
    "          c2_c3, c2_c4, c2_c5, c2_c6, c2_c7, c2_c8, c2_c9, c2_c10, c2_c11,\n",
    "          c3_c4, c3_c5, c3_c6, c3_c7, c3_c8, c3_c9, c3_c10, c3_c11, \n",
    "          c4_c5, c4_c6, c4_c7, c4_c8, c4_c9, c4_c10, c4_c11, \n",
    "          c5_c6, c5_c7, c5_c8, c5_c9, c5_c10, c5_c11,  \n",
    "          c6_c7, c6_c8, c6_c9, c6_c10, c6_c11, \n",
    "          c7_c8, c7_c9, c7_c10, c7_c11,  \n",
    "          c8_c9, c8_c10, c8_c11,  \n",
    "          c9_c10, c9_c11,  \n",
    "          c10_c11]\n",
    "\n",
    "avg = round(mean(sample),3)\n",
    "sd = round(statistics.stdev(sample),3)\n",
    " \n",
    "# Prints average & standard deviation\n",
    "print(\"Average:\", avg)\n",
    "print(\"Standard Deviation:\", sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Fleiss' kappa for Discharged Alive\n",
    "\n",
    "all_alive_ex2 = true_alive_ex2.copy(deep=True)\n",
    "all_alive_ex2['count_alive']  = all_alive_ex2.eq('0').sum(axis=1)\n",
    "all_alive_ex2['count_dead']  = all_alive_ex2.eq('1').sum(axis=1)\n",
    "all_alive_ex2['count_other']  = all_alive_ex2.eq('3').sum(axis=1)\n",
    "\n",
    "##drop unncessary cols\n",
    "cols = [ 'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11']\n",
    "\n",
    "all_alive_ex2 = all_alive_ex2.drop(cols, axis = 1)\n",
    "\n",
    "all_alive_ex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Fleiss' kappa - Discharged Alive\n",
    "\n",
    "fleiss_k_ex2_alive = round(fleiss_kappa(all_alive_ex2, method='fleiss'),3)\n",
    "\n",
    "print(\"Fleiss' kappa: {:.3f}\".format(fleiss_k_ex2_alive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Extreme 2 cut-off - Died"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only Discharged died - from Ex2 cut-off\n",
    "\n",
    "true_died_ex2 = ann_ex2_pred.copy(deep=True)\n",
    "true_died_ex2 = pd.merge(true_died_ex2, true_discharge, on='patientid')\n",
    "true_died_ex2 = true_died_ex2[true_died_ex2['discharge_status']=='dead']\n",
    "true_died_ex2.columns = ['patientid', 'C1', 'C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','MV','TMV','discharge_status']\n",
    "true_died_ex2 = true_died_ex2.drop(['patientid','MV','TMV','discharge_status'], axis = 1)\n",
    "true_died_ex2 = true_died_ex2.applymap(str)\n",
    "true_died_ex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate pairwise Cohen's kappa (Died only)\n",
    "\n",
    "c1_pred = true_died_ex2.iloc[:,0]\n",
    "c2_pred = true_died_ex2.iloc[:,1]\n",
    "c3_pred = true_died_ex2.iloc[:,2]\n",
    "c4_pred = true_died_ex2.iloc[:,3]\n",
    "c5_pred = true_died_ex2.iloc[:,4]\n",
    "c6_pred = true_died_ex2.iloc[:,5]\n",
    "c7_pred = true_died_ex2.iloc[:,6]\n",
    "c8_pred = true_died_ex2.iloc[:,7]\n",
    "c9_pred = true_died_ex2.iloc[:,8]\n",
    "c10_pred = true_died_ex2.iloc[:,9]\n",
    "c11_pred = true_died_ex2.iloc[:,10]\n",
    "\n",
    "c1_c2 = round(cohen_kappa_score(c1_pred, c2_pred),2)\n",
    "c1_c3 = round(cohen_kappa_score(c1_pred, c3_pred),2)\n",
    "c1_c4 = round(cohen_kappa_score(c1_pred, c4_pred),2)\n",
    "c1_c5 = round(cohen_kappa_score(c1_pred, c5_pred),2)\n",
    "c1_c6 = round(cohen_kappa_score(c1_pred, c6_pred),2)\n",
    "c1_c7 = round(cohen_kappa_score(c1_pred, c7_pred),2)\n",
    "c1_c8 = round(cohen_kappa_score(c1_pred, c8_pred),2)\n",
    "c1_c9 = round(cohen_kappa_score(c1_pred, c9_pred),2)\n",
    "c1_c10 = round(cohen_kappa_score(c1_pred, c10_pred),2)\n",
    "c1_c11 = round(cohen_kappa_score(c1_pred, c11_pred),2)\n",
    "\n",
    "c2_c3 = round(cohen_kappa_score(c2_pred, c3_pred),2)\n",
    "c2_c4 = round(cohen_kappa_score(c2_pred, c4_pred),2)\n",
    "c2_c5 = round(cohen_kappa_score(c2_pred, c5_pred),2)\n",
    "c2_c6 = round(cohen_kappa_score(c2_pred, c6_pred),2)\n",
    "c2_c7 = round(cohen_kappa_score(c2_pred, c7_pred),2)\n",
    "c2_c8 = round(cohen_kappa_score(c2_pred, c8_pred),2)\n",
    "c2_c9 = round(cohen_kappa_score(c2_pred, c9_pred),2)\n",
    "c2_c10 = round(cohen_kappa_score(c2_pred, c10_pred),2)\n",
    "c2_c11 = round(cohen_kappa_score(c2_pred, c11_pred),2)\n",
    "\n",
    "c3_c4 = round(cohen_kappa_score(c3_pred, c4_pred),2)\n",
    "c3_c5 = round(cohen_kappa_score(c3_pred, c5_pred),2)\n",
    "c3_c6 = round(cohen_kappa_score(c3_pred, c6_pred),2)\n",
    "c3_c7 = round(cohen_kappa_score(c3_pred, c7_pred),2)\n",
    "c3_c8 = round(cohen_kappa_score(c3_pred, c8_pred),2)\n",
    "c3_c9 = round(cohen_kappa_score(c3_pred, c9_pred),2)\n",
    "c3_c10 = round(cohen_kappa_score(c3_pred, c10_pred),2)\n",
    "c3_c11 = round(cohen_kappa_score(c3_pred, c11_pred),2)\n",
    "\n",
    "c4_c5 = round(cohen_kappa_score(c4_pred, c5_pred),2)\n",
    "c4_c6 = round(cohen_kappa_score(c4_pred, c6_pred),2)\n",
    "c4_c7 = round(cohen_kappa_score(c4_pred, c7_pred),2)\n",
    "c4_c8 = round(cohen_kappa_score(c4_pred, c8_pred),2)\n",
    "c4_c9 = round(cohen_kappa_score(c4_pred, c9_pred),2)\n",
    "c4_c10 = round(cohen_kappa_score(c4_pred, c10_pred),2)\n",
    "c4_c11 = round(cohen_kappa_score(c4_pred, c11_pred),2)\n",
    "\n",
    "c5_c6 = round(cohen_kappa_score(c5_pred, c6_pred),2)\n",
    "c5_c7 = round(cohen_kappa_score(c5_pred, c7_pred),2)\n",
    "c5_c8 = round(cohen_kappa_score(c5_pred, c8_pred),2)\n",
    "c5_c9 = round(cohen_kappa_score(c5_pred, c9_pred),2)\n",
    "c5_c10 = round(cohen_kappa_score(c5_pred, c10_pred),2)\n",
    "c5_c11 = round(cohen_kappa_score(c5_pred, c11_pred),2)\n",
    "\n",
    "c6_c7 = round(cohen_kappa_score(c6_pred, c7_pred),2)\n",
    "c6_c8 = round(cohen_kappa_score(c6_pred, c8_pred),2)\n",
    "c6_c9 = round(cohen_kappa_score(c6_pred, c9_pred),2)\n",
    "c6_c10 = round(cohen_kappa_score(c6_pred, c10_pred),2)\n",
    "c6_c11 = round(cohen_kappa_score(c6_pred, c11_pred),2)\n",
    "\n",
    "c7_c8 = round(cohen_kappa_score(c7_pred, c8_pred),2)\n",
    "c7_c9 = round(cohen_kappa_score(c7_pred, c9_pred),2)\n",
    "c7_c10 = round(cohen_kappa_score(c7_pred, c10_pred),2)\n",
    "c7_c11 = round(cohen_kappa_score(c7_pred, c11_pred),2)\n",
    "\n",
    "c8_c9 = round(cohen_kappa_score(c8_pred, c9_pred),2)\n",
    "c8_c10 = round(cohen_kappa_score(c8_pred, c10_pred),2)\n",
    "c8_c11 = round(cohen_kappa_score(c8_pred, c11_pred),2)\n",
    "\n",
    "c9_c10 = round(cohen_kappa_score(c9_pred, c10_pred),2)\n",
    "c9_c11 = round(cohen_kappa_score(c9_pred, c11_pred),2)\n",
    "\n",
    "c10_c11 = round(cohen_kappa_score(c10_pred, c11_pred),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pairwise Cohen's kappa (Died only)\n",
    "\n",
    "C0 = [\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\"]\n",
    "C1 = [1.00, c1_c2, c1_c3, c1_c4, c1_c5, c1_c6, c1_c7, c1_c8, c1_c9, c1_c10, c1_c11]\n",
    "C2 = [\"\", 1.00, c2_c3, c2_c4, c2_c5, c2_c6, c2_c7, c2_c8, c2_c9, c2_c10, c2_c11]\n",
    "C3 = [\"\", \"\", 1.00, c3_c4, c3_c5, c3_c6, c3_c7, c3_c8, c3_c9, c3_c10, c3_c11]\n",
    "C4 = [\"\", \"\", \"\", 1.00, c4_c5, c4_c6, c4_c7, c4_c8, c4_c9, c4_c10, c4_c11]\n",
    "C5 = [\"\", \"\", \"\", \"\", 1.00, c5_c6, c5_c7, c5_c8, c5_c9, c5_c10, c5_c11]\n",
    "C6 = [\"\", \"\", \"\", \"\", \"\", 1.00, c6_c7, c6_c8, c6_c9, c6_c10, c6_c11]\n",
    "C7 = [\"\", \"\", \"\", \"\", \"\", \"\", 1.00, c7_c8, c7_c9, c7_c10, c7_c11]\n",
    "C8 = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", 1.00, c8_c9, c8_c10, c8_c11]\n",
    "C9 = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", 1.00, c9_c10, c9_c11]\n",
    "C10 = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", 1.00, c10_c11]\n",
    "C11 = [\"\", \"\", \"\", \"\", \"\", \"\" , \"\", \"\", \"\", \"\", 1.00]\n",
    "\n",
    "C0 = pd.DataFrame(data=C0)\n",
    "C1 = pd.DataFrame(data=C1)\n",
    "C2 = pd.DataFrame(data=C2)\n",
    "C3 = pd.DataFrame(data=C3)\n",
    "C4 = pd.DataFrame(data=C4)\n",
    "C5 = pd.DataFrame(data=C5)\n",
    "C6 = pd.DataFrame(data=C6)\n",
    "C7 = pd.DataFrame(data=C7)\n",
    "C8 = pd.DataFrame(data=C8)\n",
    "C9 = pd.DataFrame(data=C9)\n",
    "C10 = pd.DataFrame(data=C10)\n",
    "C11 = pd.DataFrame(data=C11)\n",
    "\n",
    "C0.columns = [\"\"]\n",
    "C1.columns = ['C1']\n",
    "C2.columns = ['C2']\n",
    "C3.columns = ['C3']\n",
    "C4.columns = ['C4']\n",
    "C5.columns = ['C5']\n",
    "C6.columns = ['C6']\n",
    "C7.columns = ['C7']\n",
    "C8.columns = ['C8']\n",
    "C9.columns = ['C9']\n",
    "C10.columns = ['C10']\n",
    "C11.columns = ['C11']\n",
    "\n",
    "frames = [C0,C1,C2,C3,C4,C5,C6,C7,C8,C9,C10,C11]\n",
    "\n",
    "cohen_k_ex2_died = pd.concat(frames, axis=1)\n",
    "cohen_k_ex2_died = cohen_k_ex2_died.set_index(\"\")\n",
    "\n",
    "cohen_k_ex2_died"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot pairwise Cohen's kappa (Died only)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "cols = [\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\"]\n",
    "cohen_k_ex2_died[cols] = cohen_k_ex2_died[cols].apply(pd.to_numeric)\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(8, 5), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "res = sns.heatmap(cohen_k_ex2_died, annot=True, vmin=0, vmax=1, \n",
    "                  fmt='.2f', cmap=\"YlGnBu\", annot_kws={\"fontsize\":15})\n",
    "\n",
    "res.set_xticklabels(res.get_xmajorticklabels(), fontsize = 15)\n",
    "res.set_yticklabels(res.get_ymajorticklabels(), fontsize = 15)\n",
    "\n",
    "#plt.title(\"Experiment 1 (RF) - Pairwise Cohen's Kappa\", fontsize=19, fontweight='bold')\n",
    "\n",
    "#plt.legend(fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#range cohen's k:0.09 to 0.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average pairwise cohen's kappa values (Died)\n",
    " \n",
    "#data\n",
    "sample = [c1_c2, c1_c3, c1_c4, c1_c5, c1_c6, c1_c7, c1_c8, c1_c9, c1_c10, c1_c11,\n",
    "          c2_c3, c2_c4, c2_c5, c2_c6, c2_c7, c2_c8, c2_c9, c2_c10, c2_c11,\n",
    "          c3_c4, c3_c5, c3_c6, c3_c7, c3_c8, c3_c9, c3_c10, c3_c11, \n",
    "          c4_c5, c4_c6, c4_c7, c4_c8, c4_c9, c4_c10, c4_c11, \n",
    "          c5_c6, c5_c7, c5_c8, c5_c9, c5_c10, c5_c11,  \n",
    "          c6_c7, c6_c8, c6_c9, c6_c10, c6_c11, \n",
    "          c7_c8, c7_c9, c7_c10, c7_c11,  \n",
    "          c8_c9, c8_c10, c8_c11,  \n",
    "          c9_c10, c9_c11,  \n",
    "          c10_c11]\n",
    "\n",
    "avg = round(mean(sample),3)\n",
    "sd = round(statistics.stdev(sample),3)\n",
    " \n",
    "# Prints average & standard deviation\n",
    "print(\"Average:\", avg)\n",
    "print(\"Standard Deviation:\", sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Fleiss' kappa for Discharged died\n",
    "\n",
    "all_died_ex2 = true_died_ex2.copy(deep=True)\n",
    "all_died_ex2['count_died']  = all_died_ex2.eq('0').sum(axis=1)\n",
    "all_died_ex2['count_dead']  = all_died_ex2.eq('1').sum(axis=1)\n",
    "all_died_ex2['count_other']  = all_died_ex2.eq('3').sum(axis=1)\n",
    "\n",
    "##drop unncessary cols\n",
    "cols = [ 'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11']\n",
    "\n",
    "all_died_ex2 = all_died_ex2.drop(cols, axis = 1)\n",
    "\n",
    "all_died_ex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Fleiss' kappa - Discharged Died\n",
    "\n",
    "fleiss_k_ex2_died = round(fleiss_kappa(all_died_ex2, method='fleiss'),3)\n",
    "\n",
    "print(\"Fleiss' kappa: {:.3f}\".format(fleiss_k_ex2_died))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Neutral cut-off - Discharged Alive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_neut_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only Discharged alive - from neut cut-off\n",
    "\n",
    "true_alive_neut = ann_neut_pred.copy(deep=True)\n",
    "true_alive_neut = pd.merge(true_alive_neut, true_discharge, on='patientid')\n",
    "true_alive_neut = true_alive_neut[true_alive_neut['discharge_status']=='alive']\n",
    "true_alive_neut.columns = ['patientid', 'C1', 'C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','MV','TMV','discharge_status']\n",
    "true_alive_neut = true_alive_neut.drop(['patientid','MV','TMV','discharge_status'], axis = 1)\n",
    "true_alive_neut = true_alive_neut.applymap(str)\n",
    "true_alive_neut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate pairwise Cohen's kappa (Discharged Alive only)\n",
    "\n",
    "c1_pred = true_alive_neut.iloc[:,0]\n",
    "c2_pred = true_alive_neut.iloc[:,1]\n",
    "c3_pred = true_alive_neut.iloc[:,2]\n",
    "c4_pred = true_alive_neut.iloc[:,3]\n",
    "c5_pred = true_alive_neut.iloc[:,4]\n",
    "c6_pred = true_alive_neut.iloc[:,5]\n",
    "c7_pred = true_alive_neut.iloc[:,6]\n",
    "c8_pred = true_alive_neut.iloc[:,7]\n",
    "c9_pred = true_alive_neut.iloc[:,8]\n",
    "c10_pred = true_alive_neut.iloc[:,9]\n",
    "c11_pred = true_alive_neut.iloc[:,10]\n",
    "\n",
    "c1_c2 = round(cohen_kappa_score(c1_pred, c2_pred),2)\n",
    "c1_c3 = round(cohen_kappa_score(c1_pred, c3_pred),2)\n",
    "c1_c4 = round(cohen_kappa_score(c1_pred, c4_pred),2)\n",
    "c1_c5 = round(cohen_kappa_score(c1_pred, c5_pred),2)\n",
    "c1_c6 = round(cohen_kappa_score(c1_pred, c6_pred),2)\n",
    "c1_c7 = round(cohen_kappa_score(c1_pred, c7_pred),2)\n",
    "c1_c8 = round(cohen_kappa_score(c1_pred, c8_pred),2)\n",
    "c1_c9 = round(cohen_kappa_score(c1_pred, c9_pred),2)\n",
    "c1_c10 = round(cohen_kappa_score(c1_pred, c10_pred),2)\n",
    "c1_c11 = round(cohen_kappa_score(c1_pred, c11_pred),2)\n",
    "\n",
    "c2_c3 = round(cohen_kappa_score(c2_pred, c3_pred),2)\n",
    "c2_c4 = round(cohen_kappa_score(c2_pred, c4_pred),2)\n",
    "c2_c5 = round(cohen_kappa_score(c2_pred, c5_pred),2)\n",
    "c2_c6 = round(cohen_kappa_score(c2_pred, c6_pred),2)\n",
    "c2_c7 = round(cohen_kappa_score(c2_pred, c7_pred),2)\n",
    "c2_c8 = round(cohen_kappa_score(c2_pred, c8_pred),2)\n",
    "c2_c9 = round(cohen_kappa_score(c2_pred, c9_pred),2)\n",
    "c2_c10 = round(cohen_kappa_score(c2_pred, c10_pred),2)\n",
    "c2_c11 = round(cohen_kappa_score(c2_pred, c11_pred),2)\n",
    "\n",
    "c3_c4 = round(cohen_kappa_score(c3_pred, c4_pred),2)\n",
    "c3_c5 = round(cohen_kappa_score(c3_pred, c5_pred),2)\n",
    "c3_c6 = round(cohen_kappa_score(c3_pred, c6_pred),2)\n",
    "c3_c7 = round(cohen_kappa_score(c3_pred, c7_pred),2)\n",
    "c3_c8 = round(cohen_kappa_score(c3_pred, c8_pred),2)\n",
    "c3_c9 = round(cohen_kappa_score(c3_pred, c9_pred),2)\n",
    "c3_c10 = round(cohen_kappa_score(c3_pred, c10_pred),2)\n",
    "c3_c11 = round(cohen_kappa_score(c3_pred, c11_pred),2)\n",
    "\n",
    "c4_c5 = round(cohen_kappa_score(c4_pred, c5_pred),2)\n",
    "c4_c6 = round(cohen_kappa_score(c4_pred, c6_pred),2)\n",
    "c4_c7 = round(cohen_kappa_score(c4_pred, c7_pred),2)\n",
    "c4_c8 = round(cohen_kappa_score(c4_pred, c8_pred),2)\n",
    "c4_c9 = round(cohen_kappa_score(c4_pred, c9_pred),2)\n",
    "c4_c10 = round(cohen_kappa_score(c4_pred, c10_pred),2)\n",
    "c4_c11 = round(cohen_kappa_score(c4_pred, c11_pred),2)\n",
    "\n",
    "c5_c6 = round(cohen_kappa_score(c5_pred, c6_pred),2)\n",
    "c5_c7 = round(cohen_kappa_score(c5_pred, c7_pred),2)\n",
    "c5_c8 = round(cohen_kappa_score(c5_pred, c8_pred),2)\n",
    "c5_c9 = round(cohen_kappa_score(c5_pred, c9_pred),2)\n",
    "c5_c10 = round(cohen_kappa_score(c5_pred, c10_pred),2)\n",
    "c5_c11 = round(cohen_kappa_score(c5_pred, c11_pred),2)\n",
    "\n",
    "c6_c7 = round(cohen_kappa_score(c6_pred, c7_pred),2)\n",
    "c6_c8 = round(cohen_kappa_score(c6_pred, c8_pred),2)\n",
    "c6_c9 = round(cohen_kappa_score(c6_pred, c9_pred),2)\n",
    "c6_c10 = round(cohen_kappa_score(c6_pred, c10_pred),2)\n",
    "c6_c11 = round(cohen_kappa_score(c6_pred, c11_pred),2)\n",
    "\n",
    "c7_c8 = round(cohen_kappa_score(c7_pred, c8_pred),2)\n",
    "c7_c9 = round(cohen_kappa_score(c7_pred, c9_pred),2)\n",
    "c7_c10 = round(cohen_kappa_score(c7_pred, c10_pred),2)\n",
    "c7_c11 = round(cohen_kappa_score(c7_pred, c11_pred),2)\n",
    "\n",
    "c8_c9 = round(cohen_kappa_score(c8_pred, c9_pred),2)\n",
    "c8_c10 = round(cohen_kappa_score(c8_pred, c10_pred),2)\n",
    "c8_c11 = round(cohen_kappa_score(c8_pred, c11_pred),2)\n",
    "\n",
    "c9_c10 = round(cohen_kappa_score(c9_pred, c10_pred),2)\n",
    "c9_c11 = round(cohen_kappa_score(c9_pred, c11_pred),2)\n",
    "\n",
    "c10_c11 = round(cohen_kappa_score(c10_pred, c11_pred),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pairwise Cohen's kappa (Discharged Alive only)\n",
    "\n",
    "C0 = [\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\"]\n",
    "C1 = [1.00, c1_c2, c1_c3, c1_c4, c1_c5, c1_c6, c1_c7, c1_c8, c1_c9, c1_c10, c1_c11]\n",
    "C2 = [\"\", 1.00, c2_c3, c2_c4, c2_c5, c2_c6, c2_c7, c2_c8, c2_c9, c2_c10, c2_c11]\n",
    "C3 = [\"\", \"\", 1.00, c3_c4, c3_c5, c3_c6, c3_c7, c3_c8, c3_c9, c3_c10, c3_c11]\n",
    "C4 = [\"\", \"\", \"\", 1.00, c4_c5, c4_c6, c4_c7, c4_c8, c4_c9, c4_c10, c4_c11]\n",
    "C5 = [\"\", \"\", \"\", \"\", 1.00, c5_c6, c5_c7, c5_c8, c5_c9, c5_c10, c5_c11]\n",
    "C6 = [\"\", \"\", \"\", \"\", \"\", 1.00, c6_c7, c6_c8, c6_c9, c6_c10, c6_c11]\n",
    "C7 = [\"\", \"\", \"\", \"\", \"\", \"\", 1.00, c7_c8, c7_c9, c7_c10, c7_c11]\n",
    "C8 = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", 1.00, c8_c9, c8_c10, c8_c11]\n",
    "C9 = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", 1.00, c9_c10, c9_c11]\n",
    "C10 = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", 1.00, c10_c11]\n",
    "C11 = [\"\", \"\", \"\", \"\", \"\", \"\" , \"\", \"\", \"\", \"\", 1.00]\n",
    "\n",
    "C0 = pd.DataFrame(data=C0)\n",
    "C1 = pd.DataFrame(data=C1)\n",
    "C2 = pd.DataFrame(data=C2)\n",
    "C3 = pd.DataFrame(data=C3)\n",
    "C4 = pd.DataFrame(data=C4)\n",
    "C5 = pd.DataFrame(data=C5)\n",
    "C6 = pd.DataFrame(data=C6)\n",
    "C7 = pd.DataFrame(data=C7)\n",
    "C8 = pd.DataFrame(data=C8)\n",
    "C9 = pd.DataFrame(data=C9)\n",
    "C10 = pd.DataFrame(data=C10)\n",
    "C11 = pd.DataFrame(data=C11)\n",
    "\n",
    "C0.columns = [\"\"]\n",
    "C1.columns = ['C1']\n",
    "C2.columns = ['C2']\n",
    "C3.columns = ['C3']\n",
    "C4.columns = ['C4']\n",
    "C5.columns = ['C5']\n",
    "C6.columns = ['C6']\n",
    "C7.columns = ['C7']\n",
    "C8.columns = ['C8']\n",
    "C9.columns = ['C9']\n",
    "C10.columns = ['C10']\n",
    "C11.columns = ['C11']\n",
    "\n",
    "frames = [C0,C1,C2,C3,C4,C5,C6,C7,C8,C9,C10,C11]\n",
    "\n",
    "cohen_k_neut_alive = pd.concat(frames, axis=1)\n",
    "cohen_k_neut_alive = cohen_k_neut_alive.set_index(\"\")\n",
    "\n",
    "cohen_k_neut_alive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot pairwise Cohen's kappa (Discharged Alive only)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "cols = [\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\"]\n",
    "cohen_k_neut_alive[cols] = cohen_k_neut_alive[cols].apply(pd.to_numeric)\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(8, 5), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "res = sns.heatmap(cohen_k_neut_alive, annot=True, vmin=0, vmax=1, \n",
    "                  fmt='.2f', cmap=\"YlGnBu\", annot_kws={\"fontsize\":15})\n",
    "\n",
    "res.set_xticklabels(res.get_xmajorticklabels(), fontsize = 15)\n",
    "res.set_yticklabels(res.get_ymajorticklabels(), fontsize = 15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#range cohen's k: 0.05 to 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average pairwise cohen's kappa values (Discharge alived)\n",
    " \n",
    "#data\n",
    "sample = [c1_c2, c1_c3, c1_c4, c1_c5, c1_c6, c1_c7, c1_c8, c1_c9, c1_c10, c1_c11,\n",
    "          c2_c3, c2_c4, c2_c5, c2_c6, c2_c7, c2_c8, c2_c9, c2_c10, c2_c11,\n",
    "          c3_c4, c3_c5, c3_c6, c3_c7, c3_c8, c3_c9, c3_c10, c3_c11, \n",
    "          c4_c5, c4_c6, c4_c7, c4_c8, c4_c9, c4_c10, c4_c11, \n",
    "          c5_c6, c5_c7, c5_c8, c5_c9, c5_c10, c5_c11,  \n",
    "          c6_c7, c6_c8, c6_c9, c6_c10, c6_c11, \n",
    "          c7_c8, c7_c9, c7_c10, c7_c11,  \n",
    "          c8_c9, c8_c10, c8_c11,  \n",
    "          c9_c10, c9_c11,  \n",
    "          c10_c11]\n",
    "\n",
    "avg = round(mean(sample),3)\n",
    "sd = round(statistics.stdev(sample),3)\n",
    " \n",
    "# Prints average & standard deviation\n",
    "print(\"Average:\", avg)\n",
    "print(\"Standard Deviation:\", sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Fleiss' kappa for Discharged Alive\n",
    "\n",
    "all_alive_neut = true_alive_neut.copy(deep=True)\n",
    "all_alive_neut['count_alive']  = all_alive_neut.eq('0').sum(axis=1)\n",
    "all_alive_neut['count_dead']  = all_alive_neut.eq('1').sum(axis=1)\n",
    "all_alive_neut['count_other']  = all_alive_neut.eq('3').sum(axis=1)\n",
    "\n",
    "##drop unncessary cols\n",
    "cols = [ 'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11']\n",
    "\n",
    "all_alive_neut = all_alive_neut.drop(cols, axis = 1)\n",
    "\n",
    "all_alive_neut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Fleiss' kappa - Discharged Died\n",
    "\n",
    "fleiss_k_neut_alive = round(fleiss_kappa(all_alive_neut, method='fleiss'),3)\n",
    "\n",
    "print(\"Fleiss' kappa: {:.3f}\".format(fleiss_k_neut_alive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Neutral cut-off - Died"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only Discharged died - from neut cut-off\n",
    "\n",
    "true_died_neut = ann_neut_pred.copy(deep=True)\n",
    "true_died_neut = pd.merge(true_died_neut, true_discharge, on='patientid')\n",
    "true_died_neut = true_died_neut[true_died_neut['discharge_status']=='dead']\n",
    "true_died_neut.columns = ['patientid', 'C1', 'C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','MV','TMV','discharge_status']\n",
    "true_died_neut = true_died_neut.drop(['patientid','MV','TMV','discharge_status'], axis = 1)\n",
    "true_died_neut = true_died_neut.applymap(str)\n",
    "true_died_neut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate pairwise Cohen's kappa (Discharged died only)\n",
    "\n",
    "c1_pred = true_died_neut.iloc[:,0]\n",
    "c2_pred = true_died_neut.iloc[:,1]\n",
    "c3_pred = true_died_neut.iloc[:,2]\n",
    "c4_pred = true_died_neut.iloc[:,3]\n",
    "c5_pred = true_died_neut.iloc[:,4]\n",
    "c6_pred = true_died_neut.iloc[:,5]\n",
    "c7_pred = true_died_neut.iloc[:,6]\n",
    "c8_pred = true_died_neut.iloc[:,7]\n",
    "c9_pred = true_died_neut.iloc[:,8]\n",
    "c10_pred = true_died_neut.iloc[:,9]\n",
    "c11_pred = true_died_neut.iloc[:,10]\n",
    "\n",
    "c1_c2 = round(cohen_kappa_score(c1_pred, c2_pred),2)\n",
    "c1_c3 = round(cohen_kappa_score(c1_pred, c3_pred),2)\n",
    "c1_c4 = round(cohen_kappa_score(c1_pred, c4_pred),2)\n",
    "c1_c5 = round(cohen_kappa_score(c1_pred, c5_pred),2)\n",
    "c1_c6 = round(cohen_kappa_score(c1_pred, c6_pred),2)\n",
    "c1_c7 = round(cohen_kappa_score(c1_pred, c7_pred),2)\n",
    "c1_c8 = round(cohen_kappa_score(c1_pred, c8_pred),2)\n",
    "c1_c9 = round(cohen_kappa_score(c1_pred, c9_pred),2)\n",
    "c1_c10 = round(cohen_kappa_score(c1_pred, c10_pred),2)\n",
    "c1_c11 = round(cohen_kappa_score(c1_pred, c11_pred),2)\n",
    "\n",
    "c2_c3 = round(cohen_kappa_score(c2_pred, c3_pred),2)\n",
    "c2_c4 = round(cohen_kappa_score(c2_pred, c4_pred),2)\n",
    "c2_c5 = round(cohen_kappa_score(c2_pred, c5_pred),2)\n",
    "c2_c6 = round(cohen_kappa_score(c2_pred, c6_pred),2)\n",
    "c2_c7 = round(cohen_kappa_score(c2_pred, c7_pred),2)\n",
    "c2_c8 = round(cohen_kappa_score(c2_pred, c8_pred),2)\n",
    "c2_c9 = round(cohen_kappa_score(c2_pred, c9_pred),2)\n",
    "c2_c10 = round(cohen_kappa_score(c2_pred, c10_pred),2)\n",
    "c2_c11 = round(cohen_kappa_score(c2_pred, c11_pred),2)\n",
    "\n",
    "c3_c4 = round(cohen_kappa_score(c3_pred, c4_pred),2)\n",
    "c3_c5 = round(cohen_kappa_score(c3_pred, c5_pred),2)\n",
    "c3_c6 = round(cohen_kappa_score(c3_pred, c6_pred),2)\n",
    "c3_c7 = round(cohen_kappa_score(c3_pred, c7_pred),2)\n",
    "c3_c8 = round(cohen_kappa_score(c3_pred, c8_pred),2)\n",
    "c3_c9 = round(cohen_kappa_score(c3_pred, c9_pred),2)\n",
    "c3_c10 = round(cohen_kappa_score(c3_pred, c10_pred),2)\n",
    "c3_c11 = round(cohen_kappa_score(c3_pred, c11_pred),2)\n",
    "\n",
    "c4_c5 = round(cohen_kappa_score(c4_pred, c5_pred),2)\n",
    "c4_c6 = round(cohen_kappa_score(c4_pred, c6_pred),2)\n",
    "c4_c7 = round(cohen_kappa_score(c4_pred, c7_pred),2)\n",
    "c4_c8 = round(cohen_kappa_score(c4_pred, c8_pred),2)\n",
    "c4_c9 = round(cohen_kappa_score(c4_pred, c9_pred),2)\n",
    "c4_c10 = round(cohen_kappa_score(c4_pred, c10_pred),2)\n",
    "c4_c11 = round(cohen_kappa_score(c4_pred, c11_pred),2)\n",
    "\n",
    "c5_c6 = round(cohen_kappa_score(c5_pred, c6_pred),2)\n",
    "c5_c7 = round(cohen_kappa_score(c5_pred, c7_pred),2)\n",
    "c5_c8 = round(cohen_kappa_score(c5_pred, c8_pred),2)\n",
    "c5_c9 = round(cohen_kappa_score(c5_pred, c9_pred),2)\n",
    "c5_c10 = round(cohen_kappa_score(c5_pred, c10_pred),2)\n",
    "c5_c11 = round(cohen_kappa_score(c5_pred, c11_pred),2)\n",
    "\n",
    "c6_c7 = round(cohen_kappa_score(c6_pred, c7_pred),2)\n",
    "c6_c8 = round(cohen_kappa_score(c6_pred, c8_pred),2)\n",
    "c6_c9 = round(cohen_kappa_score(c6_pred, c9_pred),2)\n",
    "c6_c10 = round(cohen_kappa_score(c6_pred, c10_pred),2)\n",
    "c6_c11 = round(cohen_kappa_score(c6_pred, c11_pred),2)\n",
    "\n",
    "c7_c8 = round(cohen_kappa_score(c7_pred, c8_pred),2)\n",
    "c7_c9 = round(cohen_kappa_score(c7_pred, c9_pred),2)\n",
    "c7_c10 = round(cohen_kappa_score(c7_pred, c10_pred),2)\n",
    "c7_c11 = round(cohen_kappa_score(c7_pred, c11_pred),2)\n",
    "\n",
    "c8_c9 = round(cohen_kappa_score(c8_pred, c9_pred),2)\n",
    "c8_c10 = round(cohen_kappa_score(c8_pred, c10_pred),2)\n",
    "c8_c11 = round(cohen_kappa_score(c8_pred, c11_pred),2)\n",
    "\n",
    "c9_c10 = round(cohen_kappa_score(c9_pred, c10_pred),2)\n",
    "c9_c11 = round(cohen_kappa_score(c9_pred, c11_pred),2)\n",
    "\n",
    "c10_c11 = round(cohen_kappa_score(c10_pred, c11_pred),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pairwise Cohen's kappa (Discharged died only)\n",
    "\n",
    "C0 = [\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\"]\n",
    "C1 = [1.00, c1_c2, c1_c3, c1_c4, c1_c5, c1_c6, c1_c7, c1_c8, c1_c9, c1_c10, c1_c11]\n",
    "C2 = [\"\", 1.00, c2_c3, c2_c4, c2_c5, c2_c6, c2_c7, c2_c8, c2_c9, c2_c10, c2_c11]\n",
    "C3 = [\"\", \"\", 1.00, c3_c4, c3_c5, c3_c6, c3_c7, c3_c8, c3_c9, c3_c10, c3_c11]\n",
    "C4 = [\"\", \"\", \"\", 1.00, c4_c5, c4_c6, c4_c7, c4_c8, c4_c9, c4_c10, c4_c11]\n",
    "C5 = [\"\", \"\", \"\", \"\", 1.00, c5_c6, c5_c7, c5_c8, c5_c9, c5_c10, c5_c11]\n",
    "C6 = [\"\", \"\", \"\", \"\", \"\", 1.00, c6_c7, c6_c8, c6_c9, c6_c10, c6_c11]\n",
    "C7 = [\"\", \"\", \"\", \"\", \"\", \"\", 1.00, c7_c8, c7_c9, c7_c10, c7_c11]\n",
    "C8 = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", 1.00, c8_c9, c8_c10, c8_c11]\n",
    "C9 = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", 1.00, c9_c10, c9_c11]\n",
    "C10 = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", 1.00, c10_c11]\n",
    "C11 = [\"\", \"\", \"\", \"\", \"\", \"\" , \"\", \"\", \"\", \"\", 1.00]\n",
    "\n",
    "C0 = pd.DataFrame(data=C0)\n",
    "C1 = pd.DataFrame(data=C1)\n",
    "C2 = pd.DataFrame(data=C2)\n",
    "C3 = pd.DataFrame(data=C3)\n",
    "C4 = pd.DataFrame(data=C4)\n",
    "C5 = pd.DataFrame(data=C5)\n",
    "C6 = pd.DataFrame(data=C6)\n",
    "C7 = pd.DataFrame(data=C7)\n",
    "C8 = pd.DataFrame(data=C8)\n",
    "C9 = pd.DataFrame(data=C9)\n",
    "C10 = pd.DataFrame(data=C10)\n",
    "C11 = pd.DataFrame(data=C11)\n",
    "\n",
    "C0.columns = [\"\"]\n",
    "C1.columns = ['C1']\n",
    "C2.columns = ['C2']\n",
    "C3.columns = ['C3']\n",
    "C4.columns = ['C4']\n",
    "C5.columns = ['C5']\n",
    "C6.columns = ['C6']\n",
    "C7.columns = ['C7']\n",
    "C8.columns = ['C8']\n",
    "C9.columns = ['C9']\n",
    "C10.columns = ['C10']\n",
    "C11.columns = ['C11']\n",
    "\n",
    "frames = [C0,C1,C2,C3,C4,C5,C6,C7,C8,C9,C10,C11]\n",
    "\n",
    "cohen_k_neut_died = pd.concat(frames, axis=1)\n",
    "cohen_k_neut_died = cohen_k_neut_died.set_index(\"\")\n",
    "\n",
    "cohen_k_neut_died"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot pairwise Cohen's kappa (Discharged Alive only)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "cols = [\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\",\"C9\",\"C10\",\"C11\"]\n",
    "cohen_k_neut_died[cols] = cohen_k_neut_died[cols].apply(pd.to_numeric)\n",
    "\n",
    "fig = plt.figure(num=None, figsize=(8, 5), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "res = sns.heatmap(cohen_k_neut_died, annot=True, vmin=0, vmax=1, \n",
    "                  fmt='.2f', cmap=\"YlGnBu\", annot_kws={\"fontsize\":15})\n",
    "\n",
    "res.set_xticklabels(res.get_xmajorticklabels(), fontsize = 15)\n",
    "res.set_yticklabels(res.get_ymajorticklabels(), fontsize = 15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#range cohen's k: 0.30 to 0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average pairwise cohen's kappa values (Discharge alived)\n",
    " \n",
    "#data\n",
    "sample = [c1_c2, c1_c3, c1_c4, c1_c5, c1_c6, c1_c7, c1_c8, c1_c9, c1_c10, c1_c11,\n",
    "          c2_c3, c2_c4, c2_c5, c2_c6, c2_c7, c2_c8, c2_c9, c2_c10, c2_c11,\n",
    "          c3_c4, c3_c5, c3_c6, c3_c7, c3_c8, c3_c9, c3_c10, c3_c11, \n",
    "          c4_c5, c4_c6, c4_c7, c4_c8, c4_c9, c4_c10, c4_c11, \n",
    "          c5_c6, c5_c7, c5_c8, c5_c9, c5_c10, c5_c11,  \n",
    "          c6_c7, c6_c8, c6_c9, c6_c10, c6_c11, \n",
    "          c7_c8, c7_c9, c7_c10, c7_c11,  \n",
    "          c8_c9, c8_c10, c8_c11,  \n",
    "          c9_c10, c9_c11,  \n",
    "          c10_c11]\n",
    "\n",
    "avg = round(mean(sample),3)\n",
    "sd = round(statistics.stdev(sample),3)\n",
    " \n",
    "# Prints average & standard deviation\n",
    "print(\"Average:\", avg)\n",
    "print(\"Standard Deviation:\", sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Fleiss' kappa for Discharged died\n",
    "\n",
    "all_died_neut = true_died_neut.copy(deep=True)\n",
    "all_died_neut['count_died']  = all_died_neut.eq('0').sum(axis=1)\n",
    "all_died_neut['count_dead']  = all_died_neut.eq('1').sum(axis=1)\n",
    "all_died_neut['count_other']  = all_died_neut.eq('3').sum(axis=1)\n",
    "\n",
    "##drop unncessary cols\n",
    "cols = [ 'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11']\n",
    "\n",
    "all_died_neut = all_died_neut.drop(cols, axis = 1)\n",
    "\n",
    "all_died_neut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Fleiss' kappa - Discharged Died\n",
    "\n",
    "fleiss_k_neut_died = round(fleiss_kappa(all_died_neut, method='fleiss'),3)\n",
    "\n",
    "print(\"Fleiss' kappa: {:.3f}\".format(fleiss_k_neut_died))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
